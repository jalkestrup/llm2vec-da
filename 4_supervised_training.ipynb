{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised training\n",
    "Refer to \n",
    "https://github.com/jalkestrup/llm2vec-dtu/blob/main/experiments/run_supervised.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['query', 'positive', 'negative', 'language', 'task', 'instruction', 'prompt', 'response'],\n",
      "    num_rows: 968249\n",
      "})\n",
      "968249\n"
     ]
    }
   ],
   "source": [
    "#Load in the dataset from the folder /teamspace/studios/this_studio/synthetic-supervised-dataset-synthetic-from-retrieval-tasks-danish\n",
    "from datasets import load_from_disk\n",
    "ds_transformed = load_from_disk(\"/teamspace/studios/this_studio/synthetic-supervised-dataset-2\")\n",
    "#Print the first example\n",
    "print(ds_transformed)\n",
    "#Num of samples\n",
    "print(len(ds_transformed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "#from accelerate import Accelerator, DistributedDataParallelKwargs\n",
    "\n",
    "from huggingface_hub import HfApi\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "api = HfApi(token=os.getenv(\"HF_TOKEN\"))\n",
    "\n",
    "#from huggingface_hub import notebook_login\n",
    "#notebook_login()\n",
    "\n",
    "# Handle lighting AI studio\n",
    "if '/teamspace' in os.getcwd():\n",
    "    os.chdir('/teamspace/studios/this_studio/llm2vec-da')\n",
    "    # Hmm lighting AI studio changed to the below ..?\n",
    "    #os.chdir('/home/zeus/content/llm2vec-da')\n",
    "    print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since DDSC/nordic-embedding-training-data couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /home/jeal/.cache/huggingface/datasets/DDSC___nordic-embedding-training-data/default/0.0.0/fba903a3f0369fa1a239aab9993c735b0f3d6e12 (last modified on Sun Apr 20 10:57:26 2025).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'Hvad var de langsigtede konsekvenser for dansk økonomi af den danske forfatningslov af 1849?', 'positive': 'Den danske forfatningslov af 1849 markerede et paradigmeskift i dansk politisk og økonomisk struktur.  Loven etableret et konstitutionelt monarki, begrænsede kongelig magt og indførte en parlamentarisk form for styre. Dette havde vidtrækkende konsekvenser for den danske økonomi.  Den nye forfatning lagde grunden for en mere liberalistisk økonomisk orden, med fokus på fri handel, privat ejendomsret og entreprenørskab.  Den øgede politiske stabilitet og forudsigelighed tiltrak udenlandsk kapital og investeringer, der bidrog til økonomisk vækst.  Samtidig reducerede loven den statslige indblanding i økonomien, hvilket gav plads til privat initiativ og markedskræfter.  Introduktionen af en national valuta og en centralbank styrkede den økonomiske integration med andre europæiske lande.  Mens forfatningsloven af 1849 ikke direkte førte til økonomisk mirakel, lagde den grunden for en periode med vækst og modernisering af den danske økonomi.', 'negative': 'Den danske forfatningslov af 1849 havde stor betydning for udviklingen af dansk national identitet.  Loven vedtog princippet om folkevælde og lige rettigheder for alle borgere, hvilket styrkede følelsen af fælleskab og national tilhørsforhold.  Den nye forfatning gav også anledning til en livlig debat om Danmarks fremtid og rolle i Europa.  Mange grundede politiske partier og foreninger, som arbejdede for at fremme deres visioner for det nye Danmark.  På kulturelt plan inspirerede forfatningsloven til en blomstringsperiode for kunst og litteratur, der reflekterede den nye tid og de nye ideer.', 'language': 'danish', 'task': 'retrieval', 'instruction': 'Locate historical documents mentioning a specific event.', 'prompt': [{'content': 'You have been assigned a retrieval task: Locate historical documents mentioning a specific event.\\n    Your mission is to write one text retrieval example for this task in JSON format. The JSON object must contain the following\\n    keys:\\n    - \"user_query\": a string, a random user search query specified by the retrieval task.\\n    - \"positive_document\": a string, a relevant document for the user query.\\n    - \"hard_negative_document\": a string, a hard negative document that only appears relevant to the query.\\n    Please adhere to the following guidelines:\\n    - The \"user_query\" should be long-tail, 5 to 15 words, clear, and diverse in topic.\\n    - All documents must be created independent of the query. Avoid copying the query verbatim. It’s acceptable if some parts of\\n    the \"positive_document\" are not topically related to the query.\\n    - All documents should be at least 200 words long.\\n    - The \"hard_negative_document\" contains some useful information, but it should be less useful or comprehensive compared\\n    to the \"positive_document\".\\n    - Both the query and documents should be in DANISH.\\n    - Do not provide any explanation in any document on why it is relevant or not relevant to the query.\\n    - Both the query and documents require PhD level education to understand.\\n    Your output must always be a JSON object only, do not explain yourself or output anything else. Be creative!', 'role': 'user'}], 'response': '```json\\n{\\n  \"user_query\": \"Hvad var de langsigtede konsekvenser for dansk økonomi af den danske forfatningslov af 1849?\",\\n  \"positive_document\": \"Den danske forfatningslov af 1849 markerede et paradigmeskift i dansk politisk og økonomisk struktur.  Loven etableret et konstitutionelt monarki, begrænsede kongelig magt og indførte en parlamentarisk form for styre. Dette havde vidtrækkende konsekvenser for den danske økonomi.  Den nye forfatning lagde grunden for en mere liberalistisk økonomisk orden, med fokus på fri handel, privat ejendomsret og entreprenørskab.  Den øgede politiske stabilitet og forudsigelighed tiltrak udenlandsk kapital og investeringer, der bidrog til økonomisk vækst.  Samtidig reducerede loven den statslige indblanding i økonomien, hvilket gav plads til privat initiativ og markedskræfter.  Introduktionen af en national valuta og en centralbank styrkede den økonomiske integration med andre europæiske lande.  Mens forfatningsloven af 1849 ikke direkte førte til økonomisk mirakel, lagde den grunden for en periode med vækst og modernisering af den danske økonomi.\",\\n  \"hard_negative_document\": \"Den danske forfatningslov af 1849 havde stor betydning for udviklingen af dansk national identitet.  Loven vedtog princippet om folkevælde og lige rettigheder for alle borgere, hvilket styrkede følelsen af fælleskab og national tilhørsforhold.  Den nye forfatning gav også anledning til en livlig debat om Danmarks fremtid og rolle i Europa.  Mange grundede politiske partier og foreninger, som arbejdede for at fremme deres visioner for det nye Danmark.  På kulturelt plan inspirerede forfatningsloven til en blomstringsperiode for kunst og litteratur, der reflekterede den nye tid og de nye ideer.\"\\n}\\n```\\n'}\n"
     ]
    }
   ],
   "source": [
    "# load https://huggingface.co/datasets/DDSC/nordic-embedding-training-data with hugginface data loader\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"DDSC/nordic-embedding-training-data\", split=\"train[:10%]\")\n",
    "\n",
    "# print the first example\n",
    "print(dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0bfdc8ad80e48ab972437f6ac4a8853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/6 shards):   0%|          | 0/968249 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save to local file\n",
    "dataset.save_to_disk(\"nordic-embedding-training-data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'peft'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HfArgumentParser, TrainingArguments\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllm2vec_da\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marguments\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EmbeddingModelArguments, DataTrainingArguments, CustomArguments\n\u001b[1;32m      4\u001b[0m simcse_parser \u001b[38;5;241m=\u001b[39m HfArgumentParser(\n\u001b[1;32m      5\u001b[0m         (EmbeddingModelArguments, DataTrainingArguments, TrainingArguments, CustomArguments)\n\u001b[1;32m      6\u001b[0m     )\n\u001b[1;32m      8\u001b[0m model_args, data_args, training_args, custom_args \u001b[38;5;241m=\u001b[39m simcse_parser\u001b[38;5;241m.\u001b[39mparse_json_file(\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfigs/supervised/MetaLlama3-sheared.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m     )\n",
      "File \u001b[0;32m~/src/llm2vec-da/llm2vec_da/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_modifications\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllm2vec_class\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LLM2Vec\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m initialize_peft, get_model_class\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataCollatorForLanguageModelingWithFullMasking\n",
      "File \u001b[0;32m~/src/llm2vec-da/llm2vec_da/model_modifications/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbidirectional_llama\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LlamaBiModel, LlamaBiForMNTP\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllm2vec_class\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LLM2Vec\n",
      "File \u001b[0;32m~/src/llm2vec-da/llm2vec_da/model_modifications/bidirectional_llama.py:21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_attn_mask_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AttentionMaskConverter\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_transformers_attn_greater_or_equal_4_43_1\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PeftModel\n\u001b[1;32m     23\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mModifiedLlamaAttention\u001b[39;00m(LlamaAttention):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'peft'"
     ]
    }
   ],
   "source": [
    "from transformers import HfArgumentParser, TrainingArguments\n",
    "from llm2vec_da.arguments import EmbeddingModelArguments, DataTrainingArguments, CustomArguments\n",
    "\n",
    "simcse_parser = HfArgumentParser(\n",
    "        (EmbeddingModelArguments, DataTrainingArguments, TrainingArguments, CustomArguments)\n",
    "    )\n",
    "\n",
    "model_args, data_args, training_args, custom_args = simcse_parser.parse_json_file(\n",
    "        \"configs/supervised/MetaLlama3-sheared.json\"\n",
    "    )\n",
    "\n",
    "# if training_args.ddp_find_unused_parameters:\n",
    "#     kwargs = [\n",
    "#         DistributedDataParallelKwargs(\n",
    "#             dim=0,\n",
    "#             broadcast_buffers=True,\n",
    "#             bucket_cap_mb=25,\n",
    "#             find_unused_parameters=True,\n",
    "#             check_reduction=False,\n",
    "#             gradient_as_bucket_view=False,\n",
    "#         )\n",
    "#     ]\n",
    "# else:\n",
    "#     kwargs = []\n",
    "\n",
    "# accelerator = Accelerator(kwargs_handlers=kwargs)\n",
    "transformers.set_seed(training_args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Union\n",
    "import random\n",
    "import logging\n",
    "import torch\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Abstract class for datasets\n",
    "    \"\"\"\n",
    "    \n",
    "    def load_data(self, file_path: str = None):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def __len__(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "@dataclass\n",
    "class DataSample:\n",
    "    id_: int\n",
    "    query: str\n",
    "    positive: str\n",
    "    negative: str = None\n",
    "    task_name: str = None\n",
    "\n",
    "\n",
    "class TrainSample:\n",
    "    \"\"\"\n",
    "    Structure for one input example with texts, the label and a unique id\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, guid: str = \"\", texts: List[str] = None, label: Union[int, float] = 0\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Creates one TrainSample with the given texts, guid and label\n",
    "\n",
    "\n",
    "        :param guid\n",
    "            id for the example\n",
    "        :param texts\n",
    "            the texts for the example.\n",
    "        :param label\n",
    "            the label for the example\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.texts = texts\n",
    "        self.label = label\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<TrainSample> label: {}, texts: {}\".format(\n",
    "            str(self.label), \"; \".join(self.texts)\n",
    "        )\n",
    "\n",
    "\n",
    "class NordicE5Data(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_name: str = \"DDSC/nordic-embedding-training-data\",\n",
    "        split: str = \"train\",\n",
    "        effective_batch_size: int = 32,\n",
    "        separator: str = \"!@#$%^&*()\", #Note default of LLM2Vec is !@#$%^&*() , changing this would also have to be changed in the llm2vec lib when encoding/decoding\n",
    "    ):\n",
    "        self.dataset_name = dataset_name\n",
    "        self.split = split\n",
    "        self.effective_batch_size = effective_batch_size\n",
    "        self.separator = separator\n",
    "\n",
    "        self.data = []\n",
    "        self.load_data()  # you might or might not need extra args here\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def load_data(self):\n",
    "        logger.info(\"Loading dataset from {}\".format(self.dataset_name))\n",
    "\n",
    "        # 1) Load the HF dataset (pick the split you actually need)\n",
    "        dataset = load_dataset(\n",
    "            self.dataset_name,\n",
    "            split=self.split,\n",
    "        )\n",
    "\n",
    "        # 2) Convert it to a list of DataSamples\n",
    "        all_samples = []\n",
    "        # Add tqdm progress bar add hint of 'Loading dataset'\n",
    "        for idx, row in tqdm(enumerate(dataset), total=len(dataset), desc='Loading dataset'):\n",
    "            # The HF dataset has columns: query, positive, negative, instruction\n",
    "            instruction = row[\"instruction\"]\n",
    "            query =  f\"{instruction}; {self.separator}{row['query']}\"\n",
    "            pos   =  f\"{self.separator}{row['positive']}\"\n",
    "            neg_raw = row[\"negative\"]\n",
    "            if neg_raw is None or neg_raw.strip().lower() in {\"\", \"none\", \"null\"}:\n",
    "                neg = None\n",
    "            else:\n",
    "                neg   =  f\"{self.separator}{row['negative']}\"\n",
    "\n",
    "            task  =  row[\"task\"]\n",
    "\n",
    "            all_samples.append(\n",
    "                DataSample(\n",
    "                    id_=idx,\n",
    "                    query=query,\n",
    "                    positive=pos,\n",
    "                    negative=neg,\n",
    "                    task_name=task\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # 3) Shuffle or batch your data if you want\n",
    "        random.shuffle(all_samples)\n",
    "\n",
    "        # 4) Optionally chunk into batches\n",
    "        logger.info(f\"Batching data for effective batch size = {self.effective_batch_size} ...\")\n",
    "        batched_idx = []\n",
    "        final_idx_order = []\n",
    "\n",
    "        # We'll walk in steps of self.effective_batch_size\n",
    "        # and discard the last partial batch\n",
    "        # Add tqdm progress bar add hint of 'Batching data'\n",
    "        for i in tqdm(range(0, len(all_samples), self.effective_batch_size), total=len(all_samples)//self.effective_batch_size, desc='Batching data'):\n",
    "            chunk = all_samples[i : i + self.effective_batch_size]\n",
    "            if len(chunk) == self.effective_batch_size:\n",
    "                batched_idx.append(chunk)\n",
    "            else:\n",
    "                logger.info(\"Skipping partial batch of size %d\", len(chunk))\n",
    "\n",
    "        # Shuffle the chunk order\n",
    "        random.shuffle(batched_idx)\n",
    "\n",
    "        # Flatten the chunked list back into a single list\n",
    "        final_data = []\n",
    "        for chunk in batched_idx:\n",
    "            final_data.extend(chunk)\n",
    "\n",
    "        self.data = final_data\n",
    "        logger.info(f\"Loaded and batched {len(self.data)} samples.\")\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.data[index]\n",
    "        texts = [sample.query, sample.positive]\n",
    "        if sample.negative is not None:          \n",
    "            texts.append(sample.negative)\n",
    "        return TrainSample(texts=texts, label=1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NordicE5Data(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hf_dataset,\n",
    "        instruction_column = 'instruction',\n",
    "        query_column = 'query',\n",
    "        pos_column = 'positive',\n",
    "        neg_column = 'negative',\n",
    "        task_column = 'task',\n",
    "        split: str = \"train\",\n",
    "        effective_batch_size: int = 32,\n",
    "        separator: str = \"!@#$%^&*()\", #Note default of LLM2Vec is !@#$%^&*() , changing this would also have to be changed in the llm2vec lib when encoding/decoding\n",
    "    ):\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.instruction_column = instruction_column\n",
    "        self.query_column = query_column\n",
    "        self.pos_column = pos_column\n",
    "        self.neg_column = neg_column\n",
    "        self.task_column = task_column\n",
    "        self.split = split\n",
    "        self.effective_batch_size = effective_batch_size\n",
    "        self.separator = separator\n",
    "\n",
    "        self.data = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def load_data(self):\n",
    "        # 1) Convert the hf dataset to a list of DataSamples\n",
    "        all_samples = []\n",
    "        # Add tqdm progress bar add hint of 'Loading dataset'\n",
    "        for idx, row in tqdm(enumerate(self.hf_dataset), total=len(self.hf_dataset), desc='Loading dataset'):\n",
    "            \n",
    "            instruction = row[self.instruction_column]\n",
    "            query =  f\"{instruction}; {self.separator}{row[self.query_column]}\"\n",
    "            pos   =  f\"{self.separator}{row[self.pos_column]}\"\n",
    "\n",
    "            neg_raw = row[self.neg_column]\n",
    "            if neg_raw is None or neg_raw.strip().lower() in {\"\", \"none\", \"null\"}:\n",
    "                neg = None\n",
    "            else:\n",
    "                neg   =  f\"{self.separator}{row[self.neg_column]}\"\n",
    "\n",
    "            if row[self.task_column]:\n",
    "                task  =  row[self.task_column]\n",
    "            else:\n",
    "                task = None\n",
    "\n",
    "            all_samples.append(\n",
    "                DataSample(\n",
    "                    id_=idx,\n",
    "                    query=query,\n",
    "                    positive=pos,\n",
    "                    negative=neg,\n",
    "                    task_name=task\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # 3) Shuffle or batch your data if you want\n",
    "        random.shuffle(all_samples)\n",
    "\n",
    "        # 4) Optionally chunk into batches\n",
    "        logger.info(f\"Batching data for effective batch size = {self.effective_batch_size} ...\")\n",
    "        batched_idx = []\n",
    "        final_idx_order = []\n",
    "\n",
    "        # We'll walk in steps of self.effective_batch_size\n",
    "        # and discard the last partial batch\n",
    "        # Add tqdm progress bar add hint of 'Batching data'\n",
    "        for i in tqdm(range(0, len(all_samples), self.effective_batch_size), total=len(all_samples)//self.effective_batch_size, desc='Batching data'):\n",
    "            chunk = all_samples[i : i + self.effective_batch_size]\n",
    "            if len(chunk) == self.effective_batch_size:\n",
    "                batched_idx.append(chunk)\n",
    "            else:\n",
    "                logger.info(\"Skipping partial batch of size %d\", len(chunk))\n",
    "\n",
    "        # Shuffle the chunk order\n",
    "        random.shuffle(batched_idx)\n",
    "\n",
    "        # Flatten the chunked list back into a single list\n",
    "        final_data = []\n",
    "        for chunk in batched_idx:\n",
    "            final_data.extend(chunk)\n",
    "\n",
    "        self.data = final_data\n",
    "        logger.info(f\"Loaded and batched {len(self.data)} samples.\")\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.data[index]\n",
    "        texts = [sample.query, sample.positive]\n",
    "        if sample.negative is not None:          \n",
    "            texts.append(sample.negative)\n",
    "        return TrainSample(texts=texts, label=1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'Hvad var de langsigtede konsekvenser for dansk økonomi af den danske forfatningslov af 1849?', 'positive': 'Den danske forfatningslov af 1849 markerede et paradigmeskift i dansk politisk og økonomisk struktur.  Loven etableret et konstitutionelt monarki, begrænsede kongelig magt og indførte en parlamentarisk form for styre. Dette havde vidtrækkende konsekvenser for den danske økonomi.  Den nye forfatning lagde grunden for en mere liberalistisk økonomisk orden, med fokus på fri handel, privat ejendomsret og entreprenørskab.  Den øgede politiske stabilitet og forudsigelighed tiltrak udenlandsk kapital og investeringer, der bidrog til økonomisk vækst.  Samtidig reducerede loven den statslige indblanding i økonomien, hvilket gav plads til privat initiativ og markedskræfter.  Introduktionen af en national valuta og en centralbank styrkede den økonomiske integration med andre europæiske lande.  Mens forfatningsloven af 1849 ikke direkte førte til økonomisk mirakel, lagde den grunden for en periode med vækst og modernisering af den danske økonomi.', 'negative': 'Den danske forfatningslov af 1849 havde stor betydning for udviklingen af dansk national identitet.  Loven vedtog princippet om folkevælde og lige rettigheder for alle borgere, hvilket styrkede følelsen af fælleskab og national tilhørsforhold.  Den nye forfatning gav også anledning til en livlig debat om Danmarks fremtid og rolle i Europa.  Mange grundede politiske partier og foreninger, som arbejdede for at fremme deres visioner for det nye Danmark.  På kulturelt plan inspirerede forfatningsloven til en blomstringsperiode for kunst og litteratur, der reflekterede den nye tid og de nye ideer.', 'language': 'danish', 'task': 'retrieval', 'instruction': 'Locate historical documents mentioning a specific event.', 'prompt': [{'content': 'You have been assigned a retrieval task: Locate historical documents mentioning a specific event.\\n    Your mission is to write one text retrieval example for this task in JSON format. The JSON object must contain the following\\n    keys:\\n    - \"user_query\": a string, a random user search query specified by the retrieval task.\\n    - \"positive_document\": a string, a relevant document for the user query.\\n    - \"hard_negative_document\": a string, a hard negative document that only appears relevant to the query.\\n    Please adhere to the following guidelines:\\n    - The \"user_query\" should be long-tail, 5 to 15 words, clear, and diverse in topic.\\n    - All documents must be created independent of the query. Avoid copying the query verbatim. It’s acceptable if some parts of\\n    the \"positive_document\" are not topically related to the query.\\n    - All documents should be at least 200 words long.\\n    - The \"hard_negative_document\" contains some useful information, but it should be less useful or comprehensive compared\\n    to the \"positive_document\".\\n    - Both the query and documents should be in DANISH.\\n    - Do not provide any explanation in any document on why it is relevant or not relevant to the query.\\n    - Both the query and documents require PhD level education to understand.\\n    Your output must always be a JSON object only, do not explain yourself or output anything else. Be creative!', 'role': 'user'}], 'response': '```json\\n{\\n  \"user_query\": \"Hvad var de langsigtede konsekvenser for dansk økonomi af den danske forfatningslov af 1849?\",\\n  \"positive_document\": \"Den danske forfatningslov af 1849 markerede et paradigmeskift i dansk politisk og økonomisk struktur.  Loven etableret et konstitutionelt monarki, begrænsede kongelig magt og indførte en parlamentarisk form for styre. Dette havde vidtrækkende konsekvenser for den danske økonomi.  Den nye forfatning lagde grunden for en mere liberalistisk økonomisk orden, med fokus på fri handel, privat ejendomsret og entreprenørskab.  Den øgede politiske stabilitet og forudsigelighed tiltrak udenlandsk kapital og investeringer, der bidrog til økonomisk vækst.  Samtidig reducerede loven den statslige indblanding i økonomien, hvilket gav plads til privat initiativ og markedskræfter.  Introduktionen af en national valuta og en centralbank styrkede den økonomiske integration med andre europæiske lande.  Mens forfatningsloven af 1849 ikke direkte førte til økonomisk mirakel, lagde den grunden for en periode med vækst og modernisering af den danske økonomi.\",\\n  \"hard_negative_document\": \"Den danske forfatningslov af 1849 havde stor betydning for udviklingen af dansk national identitet.  Loven vedtog princippet om folkevælde og lige rettigheder for alle borgere, hvilket styrkede følelsen af fælleskab og national tilhørsforhold.  Den nye forfatning gav også anledning til en livlig debat om Danmarks fremtid og rolle i Europa.  Mange grundede politiske partier og foreninger, som arbejdede for at fremme deres visioner for det nye Danmark.  På kulturelt plan inspirerede forfatningsloven til en blomstringsperiode for kunst og litteratur, der reflekterede den nye tid og de nye ideer.\"\\n}\\n```\\n'}\n"
     ]
    }
   ],
   "source": [
    "for i in dataset:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 96825/96825 [00:16<00:00, 5864.82it/s]\n",
      "Batching data: 3026it [00:00, 532693.86it/s]           \n"
     ]
    }
   ],
   "source": [
    "nordic_e5_dataset = NordicE5Data(dataset)\n",
    "nordic_e5_dataset.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Locate travel blogs about backpacking through Southeast Asia.; !@#$%^&*()Jeg planlægger en lang rejse til Sydøstasien og vil gerne backpacke gennem flere lande. Jeg er interesseret i at opleve den lokale kultur, prøvesmage eksotisk mad, vandre i regnskoven og måske endda tage et dykkerkursus. Jeg leder efter blogs fra erfarne backpackere, der kan give mig tips og råd om ruter, overnatningssteder, budget og kulturelle normer.',\n",
       " '!@#$%^&*()Min syv måneders rejse gennem Sydøstasien var en sand transformation. Fra de pulserende gader i Bangkok til de fredlige templer i Angkor Wat, hvert land gav mig en unik indsigt i regionens rige kultur og natur. Jeg backpackede gennem Thailand, Laos, Vietnam, Kambodsja og Malaysia, og jeg boede på alt fra billige guesthouses til luksuriøse bungalower ved stranden. Jeg lærte at forhandle priser på lokale markeder, at spise med hænderne og at kommunikerer med et smil, selv når sproget var en barriere. Højdepunktet var et dykkerkursus på de Malaysiske øer, hvor jeg svømmede side om side med havskildpadder og koralrev.',\n",
       " '!@#$%^&*()Sydøstasien er et paradis for backpackere takket være det lave leveomkostninger, det varierede landskab og den venlige befolkning. Jeg har selv backpacket i Thailand og Vietnam, og jeg kan varmt anbefale at prøve streetfood, besøgefloating markets og at tage en tur med longtail båden. Husk at forhandle priserne, og hold øje med dine ejendele i folkemængder.']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nordic_e5_dataset[0].texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\n",
    "    \"DDSC/nordic-embedding-training-data\",\n",
    "    split=\"train\",\n",
    "    columns=['query', 'positive', 'negative', 'instruction', 'task']\n",
    ")\n",
    "\n",
    "# check if the negative attribute is not none\n",
    "if dataset[100000]['negative'] is not None:\n",
    "    print(dataset[100000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 968249/968249 [02:12<00:00, 7323.16it/s]\n",
      "Batching data: 30258it [00:00, 230415.93it/s]                           \n"
     ]
    }
   ],
   "source": [
    "nordic_e5_data = NordicE5Data(\n",
    "    dataset_name=\"DDSC/nordic-embedding-training-data\",\n",
    "    split=\"train\",\n",
    "    #effective_batch_size=training_args.per_device_train_batch_size * accelerator.num_processes,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nordic_e5_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnordic_e5_data\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtexts\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nordic_e5_data' is not defined"
     ]
    }
   ],
   "source": [
    "nordic_e5_data[0].texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm2vec_da import LLM2Vec\n",
    "\n",
    "torch_dtype = (\n",
    "    model_args.torch_dtype\n",
    "    if model_args.torch_dtype in [\"auto\", None]\n",
    "    else getattr(torch, model_args.torch_dtype)\n",
    ")\n",
    "\n",
    "model = LLM2Vec.from_pretrained(\n",
    "    base_model_name_or_path=model_args.model_name_or_path,\n",
    "    enable_bidirectional=model_args.bidirectional,\n",
    "    peft_model_name_or_path=model_args.peft_model_name_or_path,\n",
    "    merge_peft=True,\n",
    "    pooling_mode=model_args.pooling_mode,\n",
    "    max_length=data_args.max_seq_length,\n",
    "    torch_dtype=torch_dtype,\n",
    "    attn_implementation=model_args.attn_implementation,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    LlamaConfig,\n",
    "    MistralConfig,\n",
    "    GemmaConfig,\n",
    "    Qwen2Config,\n",
    ")\n",
    "\n",
    "\n",
    "def prepare_for_tokenization(model, text, pooling_mode=\"mean\"):\n",
    "    if model.config._name_or_path == \"meta-llama/Meta-Llama-3-8B-Instruct\":\n",
    "        text = (\n",
    "            \"<|start_header_id|>user<|end_header_id|>\\n\\n\" + text.strip() + \"<|eot_id|>\"\n",
    "        )\n",
    "        return text\n",
    "    if model.config._name_or_path in [\n",
    "        \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "        \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    ]:\n",
    "        text = \"[INST] \" + text.strip() + \" [/INST]\"\n",
    "    if model.config._name_or_path in [\n",
    "        \"google/gemma-2-9b-it\",\n",
    "    ]:\n",
    "        text = \"<bos><start_of_turn>user\\n\" + text.strip() + \"<end_of_turn>\"\n",
    "    if model.config._name_or_path in [\n",
    "        \"Qwen/Qwen2-1.5B-Instruct\",\n",
    "        \"Qwen/Qwen2-7B-Instruct\",\n",
    "    ]:\n",
    "        text = \"<|im_start|>user\\n\" + text.strip() + \"<|im_end|>\"\n",
    "    if pooling_mode == \"eos_token\":\n",
    "        if model.config._name_or_path == \"meta-llama/Meta-Llama-3-8B\":\n",
    "            text = text.strip() + \"<|end_of_text|>\"\n",
    "        elif isinstance(model.config, LlamaConfig) or isinstance(\n",
    "            model.config, MistralConfig\n",
    "        ):\n",
    "            text = text.strip() + \" </s>\"\n",
    "        elif isinstance(model.config, GemmaConfig):\n",
    "            text = text.strip() + \"<eos>\"\n",
    "        elif isinstance(model.config, Qwen2Config):\n",
    "            text = text.strip() + \"<|endoftext|>\"\n",
    "    return text\n",
    "\n",
    "class MixedNegCollator:\n",
    "    #def __init__(self, model: LLM2Vec):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def _prep(self, txt):\n",
    "        return prepare_for_tokenization(self.model, txt,\n",
    "                                        pooling_mode=self.model.pooling_mode)\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        q_texts, p_texts, n_texts, labels = [], [], [], []\n",
    "\n",
    "        for ex in batch:\n",
    "            q_texts.append(self._prep(ex.texts[0]))\n",
    "            p_texts.append(self._prep(ex.texts[1]))\n",
    "\n",
    "            if len(ex.texts) > 2 and ex.texts[2]:\n",
    "                n_texts.append(self._prep(ex.texts[2]))\n",
    "\n",
    "            labels.append(ex.label)\n",
    "\n",
    "        sent_feat_q = self.model.tokenize(q_texts)          # size B\n",
    "        sent_feat_p = self.model.tokenize(p_texts)          # size B\n",
    "        sent_feat_n = (\n",
    "            self.model.tokenize(n_texts) if n_texts else None\n",
    "        )                                                   # size ≤ B or None\n",
    "\n",
    "        return (sent_feat_q, sent_feat_p, sent_feat_n), torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import LlamaConfig\n",
    "\n",
    "class TinyLLM2Vec(nn.Module):\n",
    "    \"\"\"\n",
    "    Drop‑in replacement for LLM2Vec that is tiny but respects the API:\n",
    "      - .tokenize(list[str]) -> dict[str, Tensor] batch encoding\n",
    "      - .encode(features)    -> Tensor (batch, D)\n",
    "      - .pooling_mode attr   -> str\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name=\"prajjwal1/bert-tiny\", pooling_mode=\"cls\"):\n",
    "        super().__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model     = AutoModel.from_pretrained(model_name)\n",
    "        #self.config = self.model.config          # forward attr used by prep‑fn\n",
    "        self.config = LlamaConfig()\n",
    "        self.config._name_or_path = \"meta-llama/Meta-Llama-3-8B\"\n",
    "        \n",
    "        self.pooling_mode = pooling_mode   # value read by prepare_for_tokenization\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def tokenize(self, texts):\n",
    "        return self.tokenizer(\n",
    "            texts, padding=True, truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode(self, features):\n",
    "        out = self.model(**features).last_hidden_state   # (B, L, H)\n",
    "        if self.pooling_mode == \"cls\":\n",
    "            return out[:, 0]                             # (B, H)\n",
    "        elif self.pooling_mode == \"mean\":\n",
    "            mask = features[\"attention_mask\"].unsqueeze(-1)\n",
    "            return (out * mask).sum(1) / mask.sum(1)     # (B, H)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown pooling mode\")\n",
    "\n",
    "model = TinyLLM2Vec(pooling_mode=\"mean\")      # instead of Llama‑8B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test of prepare_for_tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input query: Locate travel blogs about backpacking through Southeast Asia.; !@#$%^&*()Jeg planlægger en lang rejse til Sydøstasien og vil gerne backpacke gennem flere lande. Jeg er interesseret i at opleve den lokale kultur, prøvesmage eksotisk mad, vandre i regnskoven og måske endda tage et dykkerkursus. Jeg leder efter blogs fra erfarne backpackere, der kan give mig tips og råd om ruter, overnatningssteder, budget og kulturelle normer.\n",
      "Output query: Locate travel blogs about backpacking through Southeast Asia.; !@#$%^&*()Jeg planlægger en lang rejse til Sydøstasien og vil gerne backpacke gennem flere lande. Jeg er interesseret i at opleve den lokale kultur, prøvesmage eksotisk mad, vandre i regnskoven og måske endda tage et dykkerkursus. Jeg leder efter blogs fra erfarne backpackere, der kan give mig tips og råd om ruter, overnatningssteder, budget og kulturelle normer.<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "# Set _name_or_path to define tokenizer model behavior\n",
    "model.config._name_or_path =  \"meta-llama/Meta-Llama-3-8B\"\n",
    "\n",
    "# Inspect the input query and the output query before and after\n",
    "print(f'Input query: {nordic_e5_dataset[0].texts[0]}')\n",
    "print(f'Output query: {prepare_for_tokenization(model, nordic_e5_dataset[0].texts[0], pooling_mode=\"eos_token\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from llm2vec_da.loss import HardNegativeNLLLoss\n",
    "#import dataloader torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "collator = MixedNegCollator(model)           # the new collator\n",
    "\n",
    "loader   = DataLoader(\n",
    "               dataset=nordic_e5_dataset,\n",
    "               batch_size=8,                 # any small number\n",
    "               shuffle=True,\n",
    "               collate_fn=collator\n",
    "           )\n",
    "\n",
    "#loss_fn  = HardNegativeNLLLoss(scale=20.0)   # unchanged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of batch: 2\n",
      "# of q_feat: torch.Size([8, 91])\n",
      "# of p_feat: torch.Size([8, 422])\n",
      "# of n_feat: torch.Size([8, 328])\n",
      "# of labels: 8\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(loader))\n",
    "(q_feat, p_feat, n_feat), _ = batch\n",
    "print(f'Length of batch: {len(batch)}\\n# of q_feat: {q_feat[\"input_ids\"].shape}\\n# of p_feat: {p_feat[\"input_ids\"].shape}\\n# of n_feat: {n_feat[\"input_ids\"].shape if n_feat else None}\\n# of labels: {len(_)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward OK, loss = 1.78125\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(loader))\n",
    "(q_feat, p_feat, n_feat), _ = batch\n",
    "\n",
    "q_reps = model.encode(q_feat)                # (B, D)\n",
    "p_reps = model.encode(p_feat)                # (B, D)\n",
    "n_reps = model.encode(n_feat) if n_feat else None\n",
    "\n",
    "loss = loss_fn(q_reps, p_reps, n_reps)\n",
    "print(\"forward OK, loss =\", loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm2vec_da import LLM2Vec\n",
    "\n",
    "torch_dtype = (\n",
    "    model_args.torch_dtype\n",
    "    if model_args.torch_dtype in [\"auto\", None]\n",
    "    else getattr(torch, model_args.torch_dtype)\n",
    ")\n",
    "\n",
    "model = LLM2Vec.from_pretrained(\n",
    "    base_model_name_or_path=model_args.model_name_or_path,\n",
    "    enable_bidirectional=model_args.bidirectional,\n",
    "    peft_model_name_or_path=model_args.peft_model_name_or_path,\n",
    "    merge_peft=True,\n",
    "    pooling_mode=model_args.pooling_mode,\n",
    "    max_length=model_args.max_seq_length,\n",
    "    torch_dtype=torch_dtype,\n",
    "    attn_implementation=model_args.attn_implementation,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up PEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm2vec_da.model import initialize_peft\n",
    "\n",
    "\n",
    "peft_model = initialize_peft(\n",
    "    model.model,\n",
    "    lora_r=custom_args.lora_r,\n",
    "    lora_alpha=2 * custom_args.lora_r,\n",
    "    lora_dropout=custom_args.lora_dropout,\n",
    ")\n",
    "\n",
    "# model organization is LLM2VecModel.model -> HF Model, we have to apply PEFT to the inner model\n",
    "model.model = peft_model.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'custom_args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllm2vec\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_loss\n\u001b[0;32m----> 2\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m load_loss(\u001b[43mcustom_args\u001b[49m\u001b[38;5;241m.\u001b[39mloss_class, scale\u001b[38;5;241m=\u001b[39mcustom_args\u001b[38;5;241m.\u001b[39mloss_scale)\n\u001b[1;32m      3\u001b[0m train_loss\n",
      "\u001b[0;31mNameError\u001b[0m: name 'custom_args' is not defined"
     ]
    }
   ],
   "source": [
    "from llm2vec.loss.utils import load_loss\n",
    "train_loss = load_loss(custom_args.loss_class, scale=custom_args.loss_scale)\n",
    "train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm2vec_da.training import SupervisedDefaultCollator\n",
    "\n",
    "tokenizer = model.tokenizer\n",
    "data_collator = SupervisedDefaultCollator(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm2vec_da.training import SupervisedTrainer\n",
    "trainer = SupervisedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_examples,\n",
    "    eval_dataset=validation_examples,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=model.tokenizer,\n",
    "    loss_function=train_loss,\n",
    ")\n",
    "\n",
    "if custom_args.stop_after_n_steps is not None:\n",
    "    trainer.add_callback(StopTrainingCallback(custom_args.stop_after_n_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dtudeep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
