{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised training\n",
    "Refer to \n",
    "https://github.com/jalkestrup/llm2vec-dtu/blob/main/experiments/run_supervised.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from accelerate import Accelerator, DistributedDataParallelKwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "api = HfApi(token=os.getenv(\"HF_TOKEN\"))\n",
    "\n",
    "# Alternatively, login with huggingface_hub GUI\n",
    "#notebook_login()\n",
    "\n",
    "# Handle lighting AI studio path\n",
    "if '/teamspace' in os.getcwd():\n",
    "    os.chdir('/teamspace/studios/this_studio/llm2vec-da')\n",
    "    # Hmm lighting AI studio changed to the below ..?\n",
    "    #os.chdir('/home/zeus/content/llm2vec-da')\n",
    "    print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import HfArgumentParser, TrainingArguments\n",
    "from llm2vec_da.arguments import EmbeddingModelArguments, DataTrainingArguments, CustomArguments\n",
    "\n",
    "supervised_parser = HfArgumentParser(\n",
    "        (EmbeddingModelArguments, DataTrainingArguments, TrainingArguments, CustomArguments)\n",
    "    )\n",
    "\n",
    "model_args, data_args, training_args, custom_args = supervised_parser.parse_json_file(\n",
    "        \"configs/supervised/MetaLlama3-sheared.json\"\n",
    "    )\n",
    "\n",
    "if training_args.ddp_find_unused_parameters:\n",
    "    kwargs = [\n",
    "        DistributedDataParallelKwargs(\n",
    "            dim=0,\n",
    "            broadcast_buffers=True,\n",
    "            bucket_cap_mb=25,\n",
    "            find_unused_parameters=True,\n",
    "            check_reduction=False,\n",
    "            gradient_as_bucket_view=False,\n",
    "        )\n",
    "    ]\n",
    "else:\n",
    "    kwargs = []\n",
    "\n",
    "accelerator = Accelerator(kwargs_handlers=kwargs)\n",
    "transformers.set_seed(training_args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since DDSC/nordic-embedding-training-data couldn't be found on the Hugging Face Hub (offline mode is enabled).\n",
      "Found the latest cached dataset configuration 'default' at /home/jeal/.cache/huggingface/datasets/DDSC___nordic-embedding-training-data/default/0.0.0/fba903a3f0369fa1a239aab9993c735b0f3d6e12 (last modified on Sun Apr 20 10:57:26 2025).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'Hvad var de langsigtede konsekvenser for dansk økonomi af den danske forfatningslov af 1849?', 'positive': 'Den danske forfatningslov af 1849 markerede et paradigmeskift i dansk politisk og økonomisk struktur.  Loven etableret et konstitutionelt monarki, begrænsede kongelig magt og indførte en parlamentarisk form for styre. Dette havde vidtrækkende konsekvenser for den danske økonomi.  Den nye forfatning lagde grunden for en mere liberalistisk økonomisk orden, med fokus på fri handel, privat ejendomsret og entreprenørskab.  Den øgede politiske stabilitet og forudsigelighed tiltrak udenlandsk kapital og investeringer, der bidrog til økonomisk vækst.  Samtidig reducerede loven den statslige indblanding i økonomien, hvilket gav plads til privat initiativ og markedskræfter.  Introduktionen af en national valuta og en centralbank styrkede den økonomiske integration med andre europæiske lande.  Mens forfatningsloven af 1849 ikke direkte førte til økonomisk mirakel, lagde den grunden for en periode med vækst og modernisering af den danske økonomi.', 'negative': 'Den danske forfatningslov af 1849 havde stor betydning for udviklingen af dansk national identitet.  Loven vedtog princippet om folkevælde og lige rettigheder for alle borgere, hvilket styrkede følelsen af fælleskab og national tilhørsforhold.  Den nye forfatning gav også anledning til en livlig debat om Danmarks fremtid og rolle i Europa.  Mange grundede politiske partier og foreninger, som arbejdede for at fremme deres visioner for det nye Danmark.  På kulturelt plan inspirerede forfatningsloven til en blomstringsperiode for kunst og litteratur, der reflekterede den nye tid og de nye ideer.', 'language': 'danish', 'task': 'retrieval', 'instruction': 'Locate historical documents mentioning a specific event.', 'prompt': [{'content': 'You have been assigned a retrieval task: Locate historical documents mentioning a specific event.\\n    Your mission is to write one text retrieval example for this task in JSON format. The JSON object must contain the following\\n    keys:\\n    - \"user_query\": a string, a random user search query specified by the retrieval task.\\n    - \"positive_document\": a string, a relevant document for the user query.\\n    - \"hard_negative_document\": a string, a hard negative document that only appears relevant to the query.\\n    Please adhere to the following guidelines:\\n    - The \"user_query\" should be long-tail, 5 to 15 words, clear, and diverse in topic.\\n    - All documents must be created independent of the query. Avoid copying the query verbatim. It’s acceptable if some parts of\\n    the \"positive_document\" are not topically related to the query.\\n    - All documents should be at least 200 words long.\\n    - The \"hard_negative_document\" contains some useful information, but it should be less useful or comprehensive compared\\n    to the \"positive_document\".\\n    - Both the query and documents should be in DANISH.\\n    - Do not provide any explanation in any document on why it is relevant or not relevant to the query.\\n    - Both the query and documents require PhD level education to understand.\\n    Your output must always be a JSON object only, do not explain yourself or output anything else. Be creative!', 'role': 'user'}], 'response': '```json\\n{\\n  \"user_query\": \"Hvad var de langsigtede konsekvenser for dansk økonomi af den danske forfatningslov af 1849?\",\\n  \"positive_document\": \"Den danske forfatningslov af 1849 markerede et paradigmeskift i dansk politisk og økonomisk struktur.  Loven etableret et konstitutionelt monarki, begrænsede kongelig magt og indførte en parlamentarisk form for styre. Dette havde vidtrækkende konsekvenser for den danske økonomi.  Den nye forfatning lagde grunden for en mere liberalistisk økonomisk orden, med fokus på fri handel, privat ejendomsret og entreprenørskab.  Den øgede politiske stabilitet og forudsigelighed tiltrak udenlandsk kapital og investeringer, der bidrog til økonomisk vækst.  Samtidig reducerede loven den statslige indblanding i økonomien, hvilket gav plads til privat initiativ og markedskræfter.  Introduktionen af en national valuta og en centralbank styrkede den økonomiske integration med andre europæiske lande.  Mens forfatningsloven af 1849 ikke direkte førte til økonomisk mirakel, lagde den grunden for en periode med vækst og modernisering af den danske økonomi.\",\\n  \"hard_negative_document\": \"Den danske forfatningslov af 1849 havde stor betydning for udviklingen af dansk national identitet.  Loven vedtog princippet om folkevælde og lige rettigheder for alle borgere, hvilket styrkede følelsen af fælleskab og national tilhørsforhold.  Den nye forfatning gav også anledning til en livlig debat om Danmarks fremtid og rolle i Europa.  Mange grundede politiske partier og foreninger, som arbejdede for at fremme deres visioner for det nye Danmark.  På kulturelt plan inspirerede forfatningsloven til en blomstringsperiode for kunst og litteratur, der reflekterede den nye tid og de nye ideer.\"\\n}\\n```\\n'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(data_args.dataset_name, split=\"train[:10%]\")\n",
    "#dataset = load_dataset(\"DDSC/nordic-embedding-training-data\", split=\"train[:10%]\")\n",
    "\n",
    "# Optionally, save to local file\n",
    "#dataset.save_to_disk(\"nordic-embedding-training-data\")\n",
    "\n",
    "# Optionally, load from local file\n",
    "#from datasets import load_from_disk\n",
    "#ds_transformed = load_from_disk(\"/teamspace/studios/this_studio/synthetic-supervised-dataset-2\")\n",
    "\n",
    "print(dataset[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Union\n",
    "import random\n",
    "import logging\n",
    "import torch\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Abstract class for datasets\n",
    "    \"\"\"\n",
    "    \n",
    "    def load_data(self, file_path: str = None):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def __len__(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "@dataclass\n",
    "class DataSample:\n",
    "    id_: int\n",
    "    query: str\n",
    "    positive: str\n",
    "    negative: str = None\n",
    "    task_name: str = None\n",
    "\n",
    "\n",
    "class TrainSample:\n",
    "    \"\"\"\n",
    "    Structure for one input example with texts, the label and a unique id\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, guid: str = \"\", texts: List[str] = None, label: Union[int, float] = 0\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Creates one TrainSample with the given texts, guid and label\n",
    "\n",
    "\n",
    "        :param guid\n",
    "            id for the example\n",
    "        :param texts\n",
    "            the texts for the example.\n",
    "        :param label\n",
    "            the label for the example\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.texts = texts\n",
    "        self.label = label\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<TrainSample> label: {}, texts: {}\".format(\n",
    "            str(self.label), \"; \".join(self.texts)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class NordicE5Data(Dataset):\n",
    "    \"\"\"\n",
    "    A dataset class for loading and processing data from a Hugging Face dataset to a datasample following E5 datastructure.\n",
    "    \n",
    "    This class handles loading instruction-based samples with queries, positive examples,\n",
    "    and optional negative examples. It processes the data into batches suitable for training.\n",
    "\n",
    "    Args:\n",
    "        hf_dataset: The dataset to load from (local or remote)\n",
    "        [Optional] instruction_column (str): Column name for instructions. Defaults to 'instruction'. Prepends the instruction to the query.\n",
    "        query_column (str): Column name for queries. Defaults to 'query'\n",
    "        pos_column (str): Column name for positive examples. Defaults to 'positive'\n",
    "        [Optional] neg_column (str): Column name for negative examples. Defaults to 'negative'\n",
    "        [Optional] task_column (str): Column name for task labels. Task is used to group the data by task during batching.\n",
    "        split (str): Dataset split to use. Defaults to \"train\"\n",
    "        effective_batch_size (int): Size of batches to create, accounting for parallel processes.\n",
    "        separator (str): Separator string between text segments. Defaults to \"!@#$%^&*()\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hf_dataset,\n",
    "        instruction_column = 'instruction',\n",
    "        query_column = 'query',\n",
    "        pos_column = 'positive',\n",
    "        neg_column = 'negative',\n",
    "        task_column = 'task',\n",
    "        split: str = \"train\",\n",
    "        effective_batch_size: int = 32,\n",
    "        separator: str = \"!@#$%^&*()\", #Note default of LLM2Vec is !@#$%^&*() , changing this would also have to be changed in the llm2vec lib when encoding/decoding\n",
    "    ):\n",
    "        self.instruction_column = instruction_column\n",
    "        self.query_column = query_column\n",
    "        self.pos_column = pos_column\n",
    "        self.neg_column = neg_column\n",
    "        self.task_column = task_column\n",
    "        self.split = split\n",
    "        self.effective_batch_size = effective_batch_size\n",
    "        self.separator = separator\n",
    "        self.data = []\n",
    "        self.load_data(hf_dataset)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def load_data(self, hf_dataset):\n",
    "        # 1) Convert the hf dataset to a list of DataSamples\n",
    "        all_samples = []\n",
    "        for idx, row in tqdm(enumerate(hf_dataset), total=len(hf_dataset), desc='Loading dataset'):\n",
    "            \n",
    "            # If no query and positive example, skip the example\n",
    "            if self.query_column not in row or self.pos_column not in row:\n",
    "                logger.warning(f\"No query or positive example found for example {idx}, skipping\")\n",
    "                continue\n",
    "\n",
    "            # If instruction column is provided, prepend the instruction to the query\n",
    "            if self.instruction_column:\n",
    "                instruction = row[self.instruction_column]\n",
    "                query =  f\"{instruction}; {self.separator}{row[self.query_column]}\"\n",
    "            else:\n",
    "                query =  f\"{row[self.query_column]}\"\n",
    "        \n",
    "            pos   =  f\"{self.separator}{row[self.pos_column]}\"\n",
    "\n",
    "            # If negative column is provided include negative example\n",
    "            neg_raw = row[self.neg_column]\n",
    "            if neg_raw is None or neg_raw.strip().lower() in {\"\", \"none\", \"null\"}:\n",
    "                neg = None\n",
    "            else:\n",
    "                neg   =  f\"{self.separator}{row[self.neg_column]}\"\n",
    "\n",
    "            # If task column is provided include task name as to group batches per task\n",
    "            if row[self.task_column]:\n",
    "                task  =  row[self.task_column]\n",
    "            else:\n",
    "                task = None\n",
    "\n",
    "            all_samples.append(\n",
    "                DataSample(\n",
    "                    id_=idx,\n",
    "                    query=query,\n",
    "                    positive=pos,\n",
    "                    negative=neg,\n",
    "                    task_name=task\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # First, group samples by task\n",
    "        task_samples = {}\n",
    "        for idx, sample in tqdm(enumerate(all_samples), total=len(all_samples), desc='Grouping data by task'):\n",
    "            task = sample.task_name\n",
    "            if task not in task_samples:\n",
    "                task_samples[task] = []\n",
    "            task_samples[task].append(sample)\n",
    "\n",
    "        logger.info(f\"Batching data for effective batch size = {self.effective_batch_size} ...\")\n",
    "        batched_data = []\n",
    "\n",
    "        # Create full batches for each task\n",
    "        for task, samples in tqdm(task_samples.items(), total=len(task_samples), desc='Batching data'):\n",
    "            task_batches = []\n",
    "            for i in range(0, len(samples), self.effective_batch_size):\n",
    "                batch = samples[i : i + self.effective_batch_size]\n",
    "                if len(batch) == self.effective_batch_size:\n",
    "                    task_batches.append(batch)\n",
    "                else:\n",
    "                    logger.info(f\"Skipping partial batch of {len(batch)} samples for task {task}\")\n",
    "            \n",
    "            if task_batches:  # If we got any full batches for this task\n",
    "                batched_data.extend(task_batches)\n",
    "\n",
    "        # Shuffle the batches to mix tasks during training\n",
    "        random.shuffle(batched_data)\n",
    "\n",
    "        # Flatten while maintaining batch boundaries\n",
    "        self.data = [sample for batch in batched_data for sample in batch]\n",
    "        logger.info(f\"Loaded and batched {len(self.data)} samples from {len(task_samples)} tasks\")\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.data[index]\n",
    "        texts = [sample.query, sample.positive]\n",
    "        if sample.negative is not None:          \n",
    "            texts.append(sample.negative)\n",
    "        return TrainSample(texts=texts, label=1.0)\n",
    "        \n",
    "def custom_dataset(hf_dataset,\n",
    "                      effective_batch_size):\n",
    "    \n",
    "    dataset_map = {\n",
    "        \"nordic-embedding-training-data\": NordicE5Data\n",
    "    }\n",
    "\n",
    "    if hf_dataset.info.dataset_name in dataset_map:\n",
    "        return dataset_map[hf_dataset.info.dataset_name](hf_dataset,\n",
    "                                                        effective_batch_size=effective_batch_size)\n",
    "    else:\n",
    "        raise ValueError(f\"Dataset {hf_dataset.info.dataset_name} not found in dataset_map\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 96825/96825 [00:16<00:00, 5890.64it/s]\n",
      "Grouping data by task: 100%|██████████| 96825/96825 [00:00<00:00, 4120218.79it/s]\n",
      "INFO:__main__:Batching data for effective batch size = 32 ...\n",
      "Batching data:   0%|          | 0/2 [00:00<?, ?it/s]INFO:__main__:Skipping partial batch of 15 samples for task retrieval\n",
      "INFO:__main__:Skipping partial batch of 10 samples for task classification\n",
      "Batching data: 100%|██████████| 2/2 [00:00<00:00, 327.62it/s]\n",
      "INFO:__main__:Loaded and batched 96800 samples from 2 tasks\n"
     ]
    }
   ],
   "source": [
    "# dataset = load_dataset(\n",
    "#     \"DDSC/nordic-embedding-training-data\",\n",
    "#     split=\"train\",\n",
    "#     columns=['query', 'positive', 'negative', 'instruction', 'task']\n",
    "# )\n",
    "\n",
    "train_dataset = custom_dataset(dataset, \n",
    "                                 effective_batch_size=training_args.per_device_train_batch_size* accelerator.num_processes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Locate news articles reporting on political developments in a specific region.; !@#$%^&*()Hvilke indflydelsesrige faktorer bestemmer fremtidens finansielle regulering i Sydøstasien i lyset af voksende middelklasse og stigende digitalisering?',\n",
       " '!@#$%^&*()Den globale økonomiske vækst har kørt ind i en periode med usikkerhed. Inflation, stigende renter og geopolitisk spænding skaber udfordringer for centralbanker og regeringer verden over. Men Sydøstasien, med sin dynamiske befolkning og voksende middelklasse, udviser stadig robust vækst. Digitaliseringen er et afgørende kendetegn ved denne vækst, der fører til nye muligheder og komplekse udfordringer for regionens finansielle sektor. Regeringssamarbejdet er afgørende for at navigere i dette komplekse landskab og skabe et stabilt og inkluderende finansiel fremtid.',\n",
       " '!@#$%^&*()Sydøstasien er et hotspot for teknologisk innovation, især inden for fintech. Nye betalingsløsninger og digitale bankplattforme bryder traditionel banksektor og øger finansiel inklusion. Men udviklingen af et robust cybersikkerhedsrammeværk og forbrugerbeskyttelse er afgørende for at sikre bæredygtig vækst og forhindre misbrug.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0].texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    LlamaConfig,\n",
    "    MistralConfig,\n",
    "    GemmaConfig,\n",
    "    Qwen2Config,\n",
    ")\n",
    "\n",
    "def prepare_for_tokenization(model, text, pooling_mode=\"mean\"):\n",
    "    if model.config._name_or_path == \"meta-llama/Meta-Llama-3-8B-Instruct\":\n",
    "        text = (\n",
    "            \"<|start_header_id|>user<|end_header_id|>\\n\\n\" + text.strip() + \"<|eot_id|>\"\n",
    "        )\n",
    "        return text\n",
    "    if model.config._name_or_path in [\n",
    "        \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "        \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    ]:\n",
    "        text = \"[INST] \" + text.strip() + \" [/INST]\"\n",
    "    if model.config._name_or_path in [\n",
    "        \"google/gemma-2-9b-it\",\n",
    "    ]:\n",
    "        text = \"<bos><start_of_turn>user\\n\" + text.strip() + \"<end_of_turn>\"\n",
    "    if model.config._name_or_path in [\n",
    "        \"Qwen/Qwen2-1.5B-Instruct\",\n",
    "        \"Qwen/Qwen2-7B-Instruct\",\n",
    "    ]:\n",
    "        text = \"<|im_start|>user\\n\" + text.strip() + \"<|im_end|>\"\n",
    "    if pooling_mode == \"eos_token\":\n",
    "        if model.config._name_or_path == \"meta-llama/Meta-Llama-3-8B\":\n",
    "            text = text.strip() + \"<|end_of_text|>\"\n",
    "        elif isinstance(model.config, LlamaConfig) or isinstance(\n",
    "            model.config, MistralConfig\n",
    "        ):\n",
    "            text = text.strip() + \" </s>\"\n",
    "        elif isinstance(model.config, GemmaConfig):\n",
    "            text = text.strip() + \"<eos>\"\n",
    "        elif isinstance(model.config, Qwen2Config):\n",
    "            text = text.strip() + \"<|endoftext|>\"\n",
    "    return text\n",
    "\n",
    "class MixedNegCollator:\n",
    "    #def __init__(self, model: LLM2Vec):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def _prep(self, txt):\n",
    "        return prepare_for_tokenization(self.model, txt,\n",
    "                                        pooling_mode=self.model.pooling_mode)\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        q_texts, p_texts, n_texts, labels = [], [], [], []\n",
    "\n",
    "        for ex in batch:\n",
    "            q_texts.append(self._prep(ex.texts[0]))\n",
    "            p_texts.append(self._prep(ex.texts[1]))\n",
    "\n",
    "            if len(ex.texts) > 2 and ex.texts[2]:\n",
    "                n_texts.append(self._prep(ex.texts[2]))\n",
    "\n",
    "            labels.append(ex.label)\n",
    "\n",
    "        sent_feat_q = self.model.tokenize(q_texts)          # size B\n",
    "        sent_feat_p = self.model.tokenize(p_texts)          # size B\n",
    "        sent_feat_n = (\n",
    "            self.model.tokenize(n_texts) if n_texts else None\n",
    "        )                                                   # size ≤ B or None\n",
    "\n",
    "        return (sent_feat_q, sent_feat_p, sent_feat_n), torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import LlamaConfig\n",
    "\n",
    "class TinyLLM2Vec(nn.Module):\n",
    "    \"\"\"\n",
    "    Drop‑in replacement for LLM2Vec that is tiny but respects the API:\n",
    "      - .tokenize(list[str]) -> dict[str, Tensor] batch encoding\n",
    "      - .encode(features)    -> Tensor (batch, D)\n",
    "      - .pooling_mode attr   -> str\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name=\"prajjwal1/bert-tiny\", pooling_mode=\"cls\"):\n",
    "        super().__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model     = AutoModel.from_pretrained(model_name)\n",
    "        self.config = self.model.config          # forward attr used by prep‑fn\n",
    "        #self.config = LlamaConfig()\n",
    "        self.config._name_or_path = \"meta-llama/Meta-Llama-3-8B\" # To fake the config\n",
    "        \n",
    "        self.pooling_mode = pooling_mode   # value read by prepare_for_tokenization\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def tokenize(self, texts):\n",
    "        return self.tokenizer(\n",
    "            texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512\n",
    "        )\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode(self, features):\n",
    "        out = self.model(**features).last_hidden_state   # (B, L, H)\n",
    "        if self.pooling_mode == \"cls\":\n",
    "            return out[:, 0]                             # (B, H)\n",
    "        elif self.pooling_mode == \"mean\":\n",
    "            mask = features[\"attention_mask\"].unsqueeze(-1)\n",
    "            return (out * mask).sum(1) / mask.sum(1)     # (B, H)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown pooling mode\")\n",
    "\n",
    "model = TinyLLM2Vec(pooling_mode=\"mean\")      # instead of Llama‑8B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test of prepare_for_tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input query: Locate news articles reporting on political developments in a specific region.; !@#$%^&*()Hvilke indflydelsesrige faktorer bestemmer fremtidens finansielle regulering i Sydøstasien i lyset af voksende middelklasse og stigende digitalisering?\n",
      "Output query: Locate news articles reporting on political developments in a specific region.; !@#$%^&*()Hvilke indflydelsesrige faktorer bestemmer fremtidens finansielle regulering i Sydøstasien i lyset af voksende middelklasse og stigende digitalisering?<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "# Set _name_or_path to define tokenizer model behavior\n",
    "model.config._name_or_path =  \"meta-llama/Meta-Llama-3-8B\"\n",
    "\n",
    "# Inspect the input query and the output query before and after\n",
    "print(f'Input query: {nordic_e5_dataset[0].texts[0]}')\n",
    "print(f'Output query: {prepare_for_tokenization(model, nordic_e5_dataset[0].texts[0], pooling_mode=\"eos_token\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm2vec_da.loss import HardNegativeNLLLoss\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "collator = MixedNegCollator(model)           # the new collator\n",
    "\n",
    "loader   = DataLoader(\n",
    "               dataset=nordic_e5_dataset,\n",
    "               batch_size=32,                 \n",
    "               shuffle=False, # DO NOT SHUFFLE, batching is done in the dataset class\n",
    "               collate_fn=collator\n",
    "           )\n",
    "\n",
    "loss_fn  = HardNegativeNLLLoss(scale=20.0)   # unchanged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of batch: 2\n",
      "# of q_feat: torch.Size([32, 98])\n",
      "# of p_feat: torch.Size([32, 512])\n",
      "# of n_feat: torch.Size([32, 358])\n",
      "# of labels: 32\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(loader))\n",
    "(q_feat, p_feat, n_feat), _ = batch\n",
    "print(f'Length of batch: {len(batch)}\\n# of q_feat: {q_feat[\"input_ids\"].shape}\\n# of p_feat: {p_feat[\"input_ids\"].shape}\\n# of n_feat: {n_feat[\"input_ids\"].shape if n_feat else None}\\n# of labels: {len(_)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward OK, loss = 3.9798991680145264\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(loader))\n",
    "(q_feat, p_feat, n_feat), _ = batch\n",
    "\n",
    "q_reps = model.encode(q_feat)                # (B, D)\n",
    "p_reps = model.encode(p_feat)                # (B, D)\n",
    "n_reps = model.encode(n_feat) if n_feat else None\n",
    "\n",
    "loss = loss_fn(q_reps, p_reps, n_reps)\n",
    "print(\"forward OK, loss =\", loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm2vec_da.training import SupervisedTrainer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm2vec_da import LLM2Vec\n",
    "\n",
    "torch_dtype = (\n",
    "    model_args.torch_dtype\n",
    "    if model_args.torch_dtype in [\"auto\", None]\n",
    "    else getattr(torch, model_args.torch_dtype)\n",
    ")\n",
    "\n",
    "model = LLM2Vec.from_pretrained(\n",
    "    base_model_name_or_path=model_args.model_name_or_path,\n",
    "    enable_bidirectional=model_args.bidirectional,\n",
    "    peft_model_name_or_path=model_args.peft_model_name_or_path,\n",
    "    merge_peft=True,\n",
    "    pooling_mode=model_args.pooling_mode,\n",
    "    max_length=model_args.max_seq_length,\n",
    "    torch_dtype=torch_dtype,\n",
    "    attn_implementation=model_args.attn_implementation,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up PEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm2vec_da.model import initialize_peft\n",
    "\n",
    "\n",
    "peft_model = initialize_peft(\n",
    "    model.model,\n",
    "    lora_r=custom_args.lora_r,\n",
    "    lora_alpha=2 * custom_args.lora_r,\n",
    "    lora_dropout=custom_args.lora_dropout,\n",
    ")\n",
    "\n",
    "# model organization is LLM2VecModel.model -> HF Model, we have to apply PEFT to the inner model\n",
    "model.model = peft_model.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llm2vec.loss.HardNegativeNLLLoss.HardNegativeNLLLoss at 0x7f5876180910>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llm2vec.loss.utils import load_loss\n",
    "train_loss = load_loss(custom_args.loss_class, scale=custom_args.loss_scale)\n",
    "train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading train examples...: 100%|██████████| 96800/96800 [00:06<00:00, 14761.81it/s]\n"
     ]
    }
   ],
   "source": [
    "#from llm2vec_da.training import SupervisedDefaultCollator\n",
    "\n",
    "tokenizer = model.tokenizer\n",
    "data_collator = MixedNegCollator(model)           # the new collator\n",
    "\n",
    "# Load train examples into memory\n",
    "train_examples = [\n",
    "    train_dataset[i]\n",
    "    for i in tqdm(\n",
    "        range(len(train_dataset)),\n",
    "        desc=\"Loading train examples...\",\n",
    "        disable=not accelerator.is_main_process,\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm2vec_da.training import SupervisedTrainer, StopTrainingCallback\n",
    "\n",
    "trainer = SupervisedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_examples,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=model.tokenizer,\n",
    "    loss_function=train_loss,\n",
    ")\n",
    "\n",
    "if custom_args.stop_after_n_steps is not None:\n",
    "    trainer.add_callback(StopTrainingCallback(custom_args.stop_after_n_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dtudeep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
