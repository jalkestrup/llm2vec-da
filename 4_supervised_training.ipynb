{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised training\n",
    "Refer to \n",
    "https://github.com/jalkestrup/llm2vec-dtu/blob/main/experiments/run_supervised.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "from tqdm import tqdm\n",
    "from accelerate import Accelerator, DistributedDataParallelKwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/teamspace/studios/this_studio/llm2vec-da\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "api = HfApi(token=os.getenv(\"HF_TOKEN\"))\n",
    "\n",
    "# Alternatively, login with huggingface_hub GUI\n",
    "#notebook_login()\n",
    "\n",
    "# Handle lighting AI studio path\n",
    "if '/teamspace' in os.getcwd():\n",
    "    os.chdir('/teamspace/studios/this_studio/llm2vec-da')\n",
    "    # Hmm lighting AI studio changed to the below ..?\n",
    "    #os.chdir('/home/zeus/content/llm2vec-da')\n",
    "    print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import HfArgumentParser, TrainingArguments\n",
    "from llm2vec_da.arguments import EmbeddingModelArguments, DataTrainingArguments, CustomArguments\n",
    "\n",
    "supervised_parser = HfArgumentParser(\n",
    "        (EmbeddingModelArguments, DataTrainingArguments, TrainingArguments, CustomArguments)\n",
    "    )\n",
    "\n",
    "model_args, data_args, training_args, custom_args = supervised_parser.parse_json_file(\n",
    "        \"configs/supervised/MetaLlama3-sheared.json\"\n",
    "    )\n",
    "\n",
    "if training_args.ddp_find_unused_parameters:\n",
    "    kwargs = [\n",
    "        DistributedDataParallelKwargs(\n",
    "            dim=0,\n",
    "            broadcast_buffers=True,\n",
    "            bucket_cap_mb=25,\n",
    "            find_unused_parameters=True,\n",
    "            check_reduction=False,\n",
    "            gradient_as_bucket_view=False,\n",
    "        )\n",
    "    ]\n",
    "else:\n",
    "    kwargs = []\n",
    "\n",
    "accelerator = Accelerator(kwargs_handlers=kwargs)\n",
    "transformers.set_seed(training_args.seed)\n",
    "\n",
    "#ABSOLUTELY CRITICAL OR WILL CAUSE OBSCURE NO GRAD ERROR THAT TOOK FREAKING 4 HOURS TO IDENTIFY\n",
    "if training_args.gradient_checkpointing:\n",
    "    training_args.gradient_checkpointing_kwargs = {\"use_reentrant\": False}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List, Union\n",
    "import random\n",
    "import logging\n",
    "import torch\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Abstract class for datasets\n",
    "    \"\"\"\n",
    "    \n",
    "    def load_data(self, file_path: str = None):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def __len__(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "@dataclass\n",
    "class DataSample:\n",
    "    id_: int\n",
    "    query: str\n",
    "    positive: str\n",
    "    negative: str = None\n",
    "    task_name: str = None\n",
    "\n",
    "\n",
    "class TrainSample:\n",
    "    \"\"\"\n",
    "    Structure for one input example with texts, the label and a unique id\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, guid: str = \"\", texts: List[str] = None, label: Union[int, float] = 0\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Creates one TrainSample with the given texts, guid and label\n",
    "\n",
    "\n",
    "        :param guid\n",
    "            id for the example\n",
    "        :param texts\n",
    "            the texts for the example.\n",
    "        :param label\n",
    "            the label for the example\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.texts = texts\n",
    "        self.label = label\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<TrainSample> label: {}, texts: {}\".format(\n",
    "            str(self.label), \"; \".join(self.texts)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class NordicE5Data(Dataset):\n",
    "    \"\"\"\n",
    "    A dataset class for loading and processing data from a Hugging Face dataset to a datasample following E5 datastructure.\n",
    "    \n",
    "    This class handles loading instruction-based samples with queries, positive examples,\n",
    "    and optional negative examples. It processes the data into batches suitable for training.\n",
    "\n",
    "    Args:\n",
    "        hf_dataset: The dataset to load from (local or remote)\n",
    "        [Optional] instruction_column (str): Column name for instructions. Defaults to 'instruction'. Prepends the instruction to the query.\n",
    "        query_column (str): Column name for queries. Defaults to 'query'\n",
    "        pos_column (str): Column name for positive examples. Defaults to 'positive'\n",
    "        [Optional] neg_column (str): Column name for negative examples. Defaults to 'negative'\n",
    "        [Optional] task_column (str): Column name for task labels. Task is used to group the data by task during batching.\n",
    "        split (str): Dataset split to use. Defaults to \"train\"\n",
    "        effective_batch_size (int): Size of batches to create, accounting for parallel processes.\n",
    "        separator (str): Separator string between text segments. Defaults to \"!@#$%^&*()\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hf_dataset,\n",
    "        instruction_column = 'instruction',\n",
    "        query_column = 'query',\n",
    "        pos_column = 'positive',\n",
    "        neg_column = 'negative',\n",
    "        task_column = 'task',\n",
    "        split: str = \"train\",\n",
    "        effective_batch_size: int = 32,\n",
    "        separator: str = \"!@#$%^&*()\", #Note default of LLM2Vec is !@#$%^&*() , changing this would also have to be changed in the llm2vec lib when encoding/decoding\n",
    "    ):\n",
    "        self.instruction_column = instruction_column\n",
    "        self.query_column = query_column\n",
    "        self.pos_column = pos_column\n",
    "        self.neg_column = neg_column\n",
    "        self.task_column = task_column\n",
    "        self.split = split\n",
    "        self.effective_batch_size = effective_batch_size\n",
    "        self.separator = separator\n",
    "        self.data = []\n",
    "        self.load_data(hf_dataset)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def load_data(self, hf_dataset):\n",
    "        # 1) Convert the hf dataset to a list of DataSamples\n",
    "        all_samples = []\n",
    "        for idx, row in tqdm(enumerate(hf_dataset), total=len(hf_dataset), desc='Loading dataset'):\n",
    "            \n",
    "            # If no query and positive example, skip the example\n",
    "            if self.query_column not in row or self.pos_column not in row:\n",
    "                logger.warning(f\"No query or positive example found for example {idx}, skipping\")\n",
    "                continue\n",
    "\n",
    "            # If instruction column is provided, prepend the instruction to the query\n",
    "            if self.instruction_column:\n",
    "                instruction = row[self.instruction_column]\n",
    "                query =  f\"{instruction}; {self.separator}{row[self.query_column]}\"\n",
    "            else:\n",
    "                query =  f\"{row[self.query_column]}\"\n",
    "        \n",
    "            pos   =  f\"{self.separator}{row[self.pos_column]}\"\n",
    "\n",
    "            # If negative column is provided include negative example\n",
    "            neg_raw = row[self.neg_column]\n",
    "            if neg_raw is None or neg_raw.strip().lower() in {\"\", \"none\", \"null\"}:\n",
    "                neg = None\n",
    "            else:\n",
    "                neg   =  f\"{self.separator}{row[self.neg_column]}\"\n",
    "\n",
    "            # If task column is provided include task name as to group batches per task\n",
    "            if row[self.task_column]:\n",
    "                task  =  row[self.task_column]\n",
    "            else:\n",
    "                task = None\n",
    "\n",
    "            all_samples.append(\n",
    "                DataSample(\n",
    "                    id_=idx,\n",
    "                    query=query,\n",
    "                    positive=pos,\n",
    "                    negative=neg,\n",
    "                    task_name=task\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # First, group samples by task\n",
    "        task_samples = {}\n",
    "        for idx, sample in tqdm(enumerate(all_samples), total=len(all_samples), desc='Grouping data by task'):\n",
    "            task = sample.task_name\n",
    "            if task not in task_samples:\n",
    "                task_samples[task] = []\n",
    "            task_samples[task].append(sample)\n",
    "\n",
    "        logger.info(f\"Batching data for effective batch size = {self.effective_batch_size} ...\")\n",
    "        batched_data = []\n",
    "\n",
    "        # Create full batches for each task\n",
    "        for task, samples in tqdm(task_samples.items(), total=len(task_samples), desc='Batching data'):\n",
    "            task_batches = []\n",
    "            for i in range(0, len(samples), self.effective_batch_size):\n",
    "                batch = samples[i : i + self.effective_batch_size]\n",
    "                if len(batch) == self.effective_batch_size:\n",
    "                    task_batches.append(batch)\n",
    "                else:\n",
    "                    logger.info(f\"Skipping partial batch of {len(batch)} samples for task {task}\")\n",
    "            \n",
    "            if task_batches:  # If we got any full batches for this task\n",
    "                batched_data.extend(task_batches)\n",
    "\n",
    "        # Shuffle the batches to mix tasks during training\n",
    "        random.shuffle(batched_data)\n",
    "\n",
    "        # Flatten while maintaining batch boundaries\n",
    "        self.data = [sample for batch in batched_data for sample in batch]\n",
    "        logger.info(f\"Loaded and batched {len(self.data)} samples from {len(task_samples)} tasks\")\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.data[index]\n",
    "        texts = [sample.query, sample.positive]\n",
    "        if sample.negative is not None:          \n",
    "            texts.append(sample.negative)\n",
    "        return TrainSample(texts=texts, label=1.0)\n",
    "        \n",
    "def custom_dataset(hf_dataset,\n",
    "                      effective_batch_size):\n",
    "    \n",
    "    dataset_map = {\n",
    "        \"nordic-embedding-training-data\": NordicE5Data\n",
    "    }\n",
    "\n",
    "    if hf_dataset.info.dataset_name in dataset_map:\n",
    "        return dataset_map[hf_dataset.info.dataset_name](hf_dataset,\n",
    "                                                        effective_batch_size=effective_batch_size)\n",
    "    else:\n",
    "        raise ValueError(f\"Dataset {hf_dataset.info.dataset_name} not found in dataset_map\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'Hvad var de langsigtede konsekvenser for dansk økonomi af den danske forfatningslov af 1849?', 'positive': 'Den danske forfatningslov af 1849 markerede et paradigmeskift i dansk politisk og økonomisk struktur.  Loven etableret et konstitutionelt monarki, begrænsede kongelig magt og indførte en parlamentarisk form for styre. Dette havde vidtrækkende konsekvenser for den danske økonomi.  Den nye forfatning lagde grunden for en mere liberalistisk økonomisk orden, med fokus på fri handel, privat ejendomsret og entreprenørskab.  Den øgede politiske stabilitet og forudsigelighed tiltrak udenlandsk kapital og investeringer, der bidrog til økonomisk vækst.  Samtidig reducerede loven den statslige indblanding i økonomien, hvilket gav plads til privat initiativ og markedskræfter.  Introduktionen af en national valuta og en centralbank styrkede den økonomiske integration med andre europæiske lande.  Mens forfatningsloven af 1849 ikke direkte førte til økonomisk mirakel, lagde den grunden for en periode med vækst og modernisering af den danske økonomi.', 'negative': 'Den danske forfatningslov af 1849 havde stor betydning for udviklingen af dansk national identitet.  Loven vedtog princippet om folkevælde og lige rettigheder for alle borgere, hvilket styrkede følelsen af fælleskab og national tilhørsforhold.  Den nye forfatning gav også anledning til en livlig debat om Danmarks fremtid og rolle i Europa.  Mange grundede politiske partier og foreninger, som arbejdede for at fremme deres visioner for det nye Danmark.  På kulturelt plan inspirerede forfatningsloven til en blomstringsperiode for kunst og litteratur, der reflekterede den nye tid og de nye ideer.', 'language': 'danish', 'task': 'retrieval', 'instruction': 'Locate historical documents mentioning a specific event.', 'prompt': [{'content': 'You have been assigned a retrieval task: Locate historical documents mentioning a specific event.\\n    Your mission is to write one text retrieval example for this task in JSON format. The JSON object must contain the following\\n    keys:\\n    - \"user_query\": a string, a random user search query specified by the retrieval task.\\n    - \"positive_document\": a string, a relevant document for the user query.\\n    - \"hard_negative_document\": a string, a hard negative document that only appears relevant to the query.\\n    Please adhere to the following guidelines:\\n    - The \"user_query\" should be long-tail, 5 to 15 words, clear, and diverse in topic.\\n    - All documents must be created independent of the query. Avoid copying the query verbatim. It’s acceptable if some parts of\\n    the \"positive_document\" are not topically related to the query.\\n    - All documents should be at least 200 words long.\\n    - The \"hard_negative_document\" contains some useful information, but it should be less useful or comprehensive compared\\n    to the \"positive_document\".\\n    - Both the query and documents should be in DANISH.\\n    - Do not provide any explanation in any document on why it is relevant or not relevant to the query.\\n    - Both the query and documents require PhD level education to understand.\\n    Your output must always be a JSON object only, do not explain yourself or output anything else. Be creative!', 'role': 'user'}], 'response': '```json\\n{\\n  \"user_query\": \"Hvad var de langsigtede konsekvenser for dansk økonomi af den danske forfatningslov af 1849?\",\\n  \"positive_document\": \"Den danske forfatningslov af 1849 markerede et paradigmeskift i dansk politisk og økonomisk struktur.  Loven etableret et konstitutionelt monarki, begrænsede kongelig magt og indførte en parlamentarisk form for styre. Dette havde vidtrækkende konsekvenser for den danske økonomi.  Den nye forfatning lagde grunden for en mere liberalistisk økonomisk orden, med fokus på fri handel, privat ejendomsret og entreprenørskab.  Den øgede politiske stabilitet og forudsigelighed tiltrak udenlandsk kapital og investeringer, der bidrog til økonomisk vækst.  Samtidig reducerede loven den statslige indblanding i økonomien, hvilket gav plads til privat initiativ og markedskræfter.  Introduktionen af en national valuta og en centralbank styrkede den økonomiske integration med andre europæiske lande.  Mens forfatningsloven af 1849 ikke direkte førte til økonomisk mirakel, lagde den grunden for en periode med vækst og modernisering af den danske økonomi.\",\\n  \"hard_negative_document\": \"Den danske forfatningslov af 1849 havde stor betydning for udviklingen af dansk national identitet.  Loven vedtog princippet om folkevælde og lige rettigheder for alle borgere, hvilket styrkede følelsen af fælleskab og national tilhørsforhold.  Den nye forfatning gav også anledning til en livlig debat om Danmarks fremtid og rolle i Europa.  Mange grundede politiske partier og foreninger, som arbejdede for at fremme deres visioner for det nye Danmark.  På kulturelt plan inspirerede forfatningsloven til en blomstringsperiode for kunst og litteratur, der reflekterede den nye tid og de nye ideer.\"\\n}\\n```\\n'}\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(data_args.dataset_name,\n",
    "split=\"train[:10%]\")\n",
    "#,columns=['query', 'positive', 'negative', 'instruction', 'task'])\n",
    "\n",
    "# Optionally, save to local file\n",
    "#dataset.save_to_disk(\"nordic-embedding-training-data\")\n",
    "\n",
    "# Optionally, load from local file\n",
    "#from datasets import load_from_disk\n",
    "#ds_transformed = load_from_disk(\"/teamspace/studios/this_studio/synthetic-supervised-dataset-2\")\n",
    "\n",
    "print(dataset[0])\n",
    "\n",
    "# Split the dataset into 95% train and 5% test\n",
    "split_dataset = dataset.train_test_split(test_size=0.05, seed=42)\n",
    "\n",
    "# Define the splits\n",
    "train_dataset = split_dataset['train']\n",
    "valid_dataset = split_dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 91983/91983 [00:11<00:00, 7891.17it/s]\n",
      "Grouping data by task: 100%|██████████| 91983/91983 [00:00<00:00, 2248842.45it/s]\n",
      "INFO:__main__:Batching data for effective batch size = 32 ...\n",
      "Batching data:   0%|          | 0/2 [00:00<?, ?it/s]INFO:__main__:Skipping partial batch of 21 samples for task retrieval\n",
      "INFO:__main__:Skipping partial batch of 26 samples for task classification\n",
      "Batching data: 100%|██████████| 2/2 [00:00<00:00, 382.88it/s]\n",
      "INFO:__main__:Loaded and batched 91936 samples from 2 tasks\n",
      "Loading dataset: 100%|██████████| 4842/4842 [00:00<00:00, 7915.46it/s]\n",
      "Grouping data by task: 100%|██████████| 4842/4842 [00:00<00:00, 2702078.23it/s]\n",
      "INFO:__main__:Batching data for effective batch size = 32 ...\n",
      "Batching data:   0%|          | 0/2 [00:00<?, ?it/s]INFO:__main__:Skipping partial batch of 26 samples for task retrieval\n",
      "INFO:__main__:Skipping partial batch of 16 samples for task classification\n",
      "Batching data: 100%|██████████| 2/2 [00:00<00:00, 2810.25it/s]\n",
      "INFO:__main__:Loaded and batched 4800 samples from 2 tasks\n"
     ]
    }
   ],
   "source": [
    "train_dataset_e5 = custom_dataset(train_dataset, \n",
    "                                 effective_batch_size=training_args.per_device_train_batch_size* accelerator.num_processes)\n",
    "\n",
    "valid_dataset_e5 = custom_dataset(valid_dataset,\n",
    "                                effective_batch_size=training_args.per_device_train_batch_size* accelerator.num_processes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Locate job postings requiring a specific skill set and industry experience.; !@#$%^&*()Jeg søger en stilling som postdoc forsker indenfor kvantitativ genetik med fokus på genomisk selektion hos hvede, hvor jeg kan kombinere mineOoh ekspertise i statistisk modellering med brugen af moderne højtydende computer clusters.',\n",
       " '!@#$%^&*()Vi søger en motiveret og dygtig postdoc forsker til at slutte sig til vores hold, der arbejder med at forstå de genetiske mekanismer bag udvikling af acidotolerante hvedesorter. Projektet involverer anvendelsen af genomisk selektionsanalyse, kvantitativ genetik og avanceret statistisk modellering. Du vil have adgang til state-of-the-art computerclusters og arbejde med store datasæt af sekvensdata. Kendskab til programmeringssprog som R eller Python er et krav. En afhandling indenfor et relevant felt, f.eks. genetik, bioinformatik eller matematik, forventes.',\n",
       " '!@#$%^&*()Vores team af forskere er specialiseret i anvendelse af maskinlæring til at analysere store datasæt af biomedicinske billeder. Vi er på udkig efter en dygtig postdoc med erfaring indenfor billedanalyse og deep learning til at bidrage til vores projekt, der sigter mod at udvikle nye metoder til diagnose af sygdomme baseret på medicinske billeddata. Stærke programmeringsevner og erfaring med Python er en forudsætning.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_e5[0].texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_tokenization(model, text, pooling_mode=\"mean\"):\n",
    "    if model.config._name_or_path == \"meta-llama/Meta-Llama-3-8B-Instruct\":\n",
    "        text = (\n",
    "            \"<|start_header_id|>user<|end_header_id|>\\n\\n\" + text.strip() + \"<|eot_id|>\"\n",
    "        )\n",
    "        return text\n",
    "    if model.config._name_or_path in [\n",
    "        \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "        \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    ]:\n",
    "        text = \"[INST] \" + text.strip() + \" [/INST]\"\n",
    "    if model.config._name_or_path in [\n",
    "        \"google/gemma-2-9b-it\",\n",
    "    ]:\n",
    "        text = \"<bos><start_of_turn>user\\n\" + text.strip() + \"<end_of_turn>\"\n",
    "    if model.config._name_or_path in [\n",
    "        \"Qwen/Qwen2-1.5B-Instruct\",\n",
    "        \"Qwen/Qwen2-7B-Instruct\",\n",
    "    ]:\n",
    "        text = \"<|im_start|>user\\n\" + text.strip() + \"<|im_end|>\"\n",
    "    if pooling_mode == \"eos_token\":\n",
    "        if model.config._name_or_path == \"meta-llama/Meta-Llama-3-8B\":\n",
    "            text = text.strip() + \"<|end_of_text|>\"\n",
    "        elif isinstance(model.config, LlamaConfig) or isinstance(\n",
    "            model.config, MistralConfig\n",
    "        ):\n",
    "            text = text.strip() + \" </s>\"\n",
    "        elif isinstance(model.config, GemmaConfig):\n",
    "            text = text.strip() + \"<eos>\"\n",
    "        elif isinstance(model.config, Qwen2Config):\n",
    "            text = text.strip() + \"<|endoftext|>\"\n",
    "    return text\n",
    "\n",
    "class MixedNegCollator:\n",
    "    #def __init__(self, model: LLM2Vec):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def _prep(self, txt):\n",
    "        return prepare_for_tokenization(self.model, txt,\n",
    "                                        pooling_mode=self.model.pooling_mode)\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        q_texts, p_texts, n_texts, labels = [], [], [], []\n",
    "\n",
    "        for ex in batch:\n",
    "            q_texts.append(self._prep(ex.texts[0]))\n",
    "            p_texts.append(self._prep(ex.texts[1]))\n",
    "\n",
    "            if len(ex.texts) > 2 and ex.texts[2]:\n",
    "                n_texts.append(self._prep(ex.texts[2]))\n",
    "\n",
    "            labels.append(ex.label)\n",
    "\n",
    "        sent_feat_q = self.model.tokenize(q_texts)          # size B\n",
    "        sent_feat_p = self.model.tokenize(p_texts)          # size B\n",
    "        sent_feat_n = (\n",
    "            self.model.tokenize(n_texts) if n_texts else None\n",
    "        )      \n",
    "                                                     # size ≤ B or None\n",
    "\n",
    "        return [sent_feat_q, sent_feat_p, sent_feat_n], torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    LlamaConfig,\n",
    "    MistralConfig,\n",
    "    GemmaConfig,\n",
    "    Qwen2Config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch import nn\n",
    "# from transformers import AutoTokenizer, AutoModel\n",
    "# from transformers import LlamaConfig\n",
    "\n",
    "\n",
    "# class TinyLLM2Vec(nn.Module):\n",
    "#     \"\"\"\n",
    "#     Drop‑in replacement for LLM2Vec that is tiny but respects the API:\n",
    "#       - .tokenize(list[str]) -> dict[str, Tensor] batch encoding\n",
    "#       - .encode(features)    -> Tensor (batch, D)\n",
    "#       - .pooling_mode attr   -> str\n",
    "#     \"\"\"\n",
    "#     def __init__(self, model_name=\"prajjwal1/bert-tiny\", pooling_mode=\"cls\"):\n",
    "#         super().__init__()\n",
    "#         self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#         self.model     = AutoModel.from_pretrained(model_name)\n",
    "#         self.config = self.model.config          # forward attr used by prep‑fn\n",
    "#         #self.config = LlamaConfig()\n",
    "#         self.config._name_or_path = \"meta-llama/Meta-Llama-3-8B\" # To fake the config\n",
    "        \n",
    "#         self.pooling_mode = pooling_mode   # value read by prepare_for_tokenization\n",
    "\n",
    "#     @torch.no_grad()\n",
    "#     def tokenize(self, texts):\n",
    "#         return self.tokenizer(\n",
    "#             texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512\n",
    "#         )\n",
    "\n",
    "#     @torch.no_grad()\n",
    "#     def encode(self, features):\n",
    "#         out = self.model(**features).last_hidden_state   # (B, L, H)\n",
    "#         if self.pooling_mode == \"cls\":\n",
    "#             return out[:, 0]                             # (B, H)\n",
    "#         elif self.pooling_mode == \"mean\":\n",
    "#             mask = features[\"attention_mask\"].unsqueeze(-1)\n",
    "#             return (out * mask).sum(1) / mask.sum(1)     # (B, H)\n",
    "#         else:\n",
    "#             raise ValueError(\"Unknown pooling mode\")\n",
    "\n",
    "# model_tiny = TinyLLM2Vec(pooling_mode=\"mean\")      # instead of Llama‑8B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test of prepare_for_tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set _name_or_path to define tokenizer model behavior\n",
    "# model_tiny.config._name_or_path =  \"meta-llama/Meta-Llama-3-8B\"\n",
    "\n",
    "# # Inspect the input query and the output query before and after\n",
    "# print(f'Input query: {train_dataset_e5[0].texts[0]}')\n",
    "# print(f'Output query: {prepare_for_tokenization(model_tiny, train_dataset_e5[0].texts[0], pooling_mode=\"eos_token\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llm2vec_da.loss import HardNegativeNLLLoss\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# collator = MixedNegCollator(model_tiny)           # the new collator\n",
    "\n",
    "# loader   = DataLoader(\n",
    "#                dataset=train_dataset_e5,\n",
    "#                batch_size=32,                 \n",
    "#                shuffle=False, # DO NOT SHUFFLE, batching is done in the dataset class\n",
    "#                collate_fn=collator\n",
    "#            )\n",
    "\n",
    "# loss_fn  = HardNegativeNLLLoss(scale=20.0)   # unchanged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = next(iter(loader))\n",
    "# (q_feat, p_feat, n_feat), _ = batch\n",
    "\n",
    "# q_reps = model_tiny.encode(q_feat)                # (B, D)\n",
    "# p_reps = model_tiny.encode(p_feat)                # (B, D)\n",
    "# n_reps = model_tiny.encode(n_feat) if n_feat else None\n",
    "\n",
    "# loss = loss_fn(q_reps, p_reps, n_reps)\n",
    "# print(\"forward OK, loss =\", loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    }
   ],
   "source": [
    "from llm2vec_da import LLM2Vec\n",
    "\n",
    "torch_dtype = (\n",
    "    model_args.torch_dtype\n",
    "    if model_args.torch_dtype in [\"auto\", None]\n",
    "    else getattr(torch, model_args.torch_dtype)\n",
    ")\n",
    "\n",
    "#training_args.gradient_checkpointing = False   # turn it off\n",
    "model = LLM2Vec.from_pretrained(\n",
    "    base_model_name_or_path=model_args.model_name_or_path,\n",
    "    enable_bidirectional=model_args.bidirectional,\n",
    "    peft_model_name_or_path=\"McGill-NLP/LLM2Vec-Sheared-LLaMA-mntp\",\n",
    "    merge_peft=True,\n",
    "    pooling_mode=model_args.pooling_mode,\n",
    "    max_length=data_args.max_seq_length,\n",
    "    torch_dtype=torch_dtype,\n",
    "    low_cpu_mem_usage=model_args.low_cpu_mem_usage,\n",
    "    attn_implementation=\"sdpa\", #OBS SET BACK TO FLASH ATTENTION WHEN RUNNING ON A100 GPU!!\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up PEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:peft.tuners.tuners_utils:Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's Lora trainable parameters:\n",
      "trainable params: 14,991,360 || all params: 1,294,878,720 || trainable%: 1.1577\n"
     ]
    }
   ],
   "source": [
    "from llm2vec_da.model import initialize_peft\n",
    "\n",
    "\n",
    "peft_model = initialize_peft(\n",
    "    model.model,\n",
    "    lora_r=custom_args.lora_r,\n",
    "    lora_alpha=2 * custom_args.lora_r,\n",
    "    lora_dropout=custom_args.lora_dropout,\n",
    ")\n",
    "\n",
    "# model organization is LLM2VecModel.model -> HF Model, we have to apply PEFT to the inner model\n",
    "model.model = peft_model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after-peft trainable: ['model.layers.0.self_attn.q_proj.lora_A.default.weight', 'model.layers.0.self_attn.q_proj.lora_B.default.weight', 'model.layers.0.self_attn.k_proj.lora_A.default.weight', 'model.layers.0.self_attn.k_proj.lora_B.default.weight', 'model.layers.0.self_attn.v_proj.lora_A.default.weight', 'model.layers.0.self_attn.v_proj.lora_B.default.weight', 'model.layers.0.self_attn.o_proj.lora_A.default.weight', 'model.layers.0.self_attn.o_proj.lora_B.default.weight', 'model.layers.0.mlp.gate_proj.lora_A.default.weight', 'model.layers.0.mlp.gate_proj.lora_B.default.weight']\n"
     ]
    }
   ],
   "source": [
    "print(\"after-peft trainable:\",\n",
    "      [n for n,p in model.named_parameters() if p.requires_grad][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requires_grad: True <StackBackward0 object at 0x7f481be80c70>\n"
     ]
    }
   ],
   "source": [
    "tok = model.tokenize([\"just a test\"])              # CPU tensors\n",
    "dev = next(model.parameters()).device              # cuda:0 or cpu\n",
    "tok = {k: v.to(dev) for k, v in tok.items()}       # move batch\n",
    "\n",
    "with torch.set_grad_enabled(True):\n",
    "    reps = model(tok)                              # runs __call__ → forward\n",
    "print(\"requires_grad:\", reps.requires_grad, reps.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading train examples...: 100%|██████████| 91936/91936 [00:00<00:00, 139876.47it/s]\n",
      "Loading valid examples...: 100%|██████████| 4800/4800 [00:00<00:00, 442106.79it/s]\n"
     ]
    }
   ],
   "source": [
    "#from llm2vec_da.training import SupervisedDefaultCollator\n",
    "tokenizer = model.tokenizer\n",
    "data_collator = MixedNegCollator(model)           # the new collator\n",
    "\n",
    "# Load train examples into memory\n",
    "train_examples = [\n",
    "    train_dataset_e5[i]\n",
    "    for i in tqdm(\n",
    "        range(len(train_dataset_e5)),\n",
    "        desc=\"Loading train examples...\",\n",
    "        disable=not accelerator.is_main_process,\n",
    "    )\n",
    "]\n",
    "\n",
    "valid_examples = [\n",
    "    valid_dataset_e5[i]\n",
    "    for i in tqdm(\n",
    "        range(len(valid_dataset_e5)),\n",
    "        desc=\"Loading valid examples...\",\n",
    "        disable=not accelerator.is_main_process,\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "from typing import Any, Optional, Tuple, Dict, Union\n",
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "from transformers.trainer_utils import seed_worker\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class SupervisedTrainer(Trainer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *args,\n",
    "        loss_function=None,\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.loss_function = loss_function\n",
    "\n",
    "    def compute_loss(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        inputs: Dict[str, Union[torch.Tensor, Any]],\n",
    "        return_outputs: bool = False,\n",
    "        **kwargs,                      # <-- swallow future extras\n",
    "    ) -> Union[Tuple[torch.Tensor, torch.Tensor], torch.Tensor]:\n",
    "\n",
    "        features, labels = inputs\n",
    "        q_reps = self.model.forward(features[0])\n",
    "        d_reps = self.model.forward(features[1])\n",
    "\n",
    "        d_reps_neg = None\n",
    "        if len(features) > 2 and features[2] is not None:\n",
    "            d_reps_neg = self.model.forward(features[2])\n",
    "\n",
    "        loss = self.loss_function(q_reps, d_reps, d_reps_neg)\n",
    "\n",
    "        if return_outputs:\n",
    "            output = torch.cat(\n",
    "                [model(row)[\"sentence_embedding\"][:, None] for row in features], dim=1\n",
    "            )\n",
    "            return loss, output\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def get_train_dataloader(self) -> DataLoader:\n",
    "        # Copying most of the code from the parent class, changing the sampler to SequentialSampler\n",
    "        if self.train_dataset is None:\n",
    "            raise ValueError(\"Trainer: training requires a train_dataset.\")\n",
    "\n",
    "        train_dataset = self.train_dataset\n",
    "        data_collator = self.data_collator\n",
    "\n",
    "        data_collator = self._get_collator_with_removed_columns(\n",
    "            data_collator, description=\"training\"\n",
    "        )\n",
    "\n",
    "        dataloader_params = {\n",
    "            \"batch_size\": self._train_batch_size,\n",
    "            \"collate_fn\": data_collator,\n",
    "            \"num_workers\": self.args.dataloader_num_workers,\n",
    "            \"pin_memory\": self.args.dataloader_pin_memory,\n",
    "            \"persistent_workers\": self.args.dataloader_persistent_workers,\n",
    "        }\n",
    "\n",
    "        if not isinstance(train_dataset, torch.utils.data.IterableDataset):\n",
    "            # Changing from random sampler to sequential sampler\n",
    "            dataloader_params[\"sampler\"] = SequentialSampler(train_dataset)\n",
    "            dataloader_params[\"drop_last\"] = self.args.dataloader_drop_last\n",
    "            dataloader_params[\"worker_init_fn\"] = seed_worker\n",
    "\n",
    "        return self.accelerator.prepare(DataLoader(train_dataset, **dataloader_params))\n",
    "\n",
    "    def _save(self, output_dir: Optional[str] = None, state_dict=None):\n",
    "        # If we are executing this function, we are the process zero, so we don't check for that.\n",
    "        output_dir = output_dir if output_dir is not None else self.args.output_dir\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        logger.info(f\"Saving model checkpoint to {output_dir}\")\n",
    "\n",
    "        self.model.save(output_dir)\n",
    "\n",
    "        # Good practice: save your training arguments together with the trained model\n",
    "        torch.save(self.args, os.path.join(output_dir, \"training_args.bin\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_830075/3893196380.py:15: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SupervisedTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from llm2vec_da.training import StopTrainingCallback\n",
    "from llm2vec_da.loss.utils import load_loss\n",
    "\n",
    "train_loss = load_loss(custom_args.loss_class, scale=custom_args.loss_scale)\n",
    "\n",
    "samples_skipped = 40 * training_args.per_device_train_batch_size   # 40 steps × 32-sample batches\n",
    "train_subset    = train_examples[samples_skipped:]  \n",
    "\n",
    "trainer = SupervisedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_subset,\n",
    "    eval_dataset=valid_examples,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=model.tokenizer,\n",
    "    loss_function=train_loss,\n",
    ")\n",
    "\n",
    "if custom_args.stop_after_n_steps is not None:\n",
    "    trainer.add_callback(StopTrainingCallback(custom_args.stop_after_n_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"WANDB_DISABLED\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 90,656\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 8,499\n",
      "  Number of trainable parameters = 14,991,360\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "Could not log the number of model parameters in Weights & Biases due to an AttributeError.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='183' max='8499' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 183/8499 32:38 < 25:00:05, 0.09 it/s, Epoch 0.06/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.972700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5.754900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.994500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.018800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.093000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.973400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>5.320300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.782800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.568400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.543000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.827500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.550800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.578900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.450400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.409400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.448600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.417000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.446100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.390000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.398800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.467400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.255100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.434000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>1.995800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.353100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.155200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.288800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.213400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.244100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>1.253600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.203900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>0.296800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.297900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>0.279800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.192700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.265400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.779500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Saving model checkpoint to output/mntp-supervised/Meta-Llama-3-sheared/checkpoint-50\n",
      "Configuration saved in output/mntp-supervised/Meta-Llama-3-sheared/checkpoint-50/config.json\n",
      "Model weights saved in output/mntp-supervised/Meta-Llama-3-sheared/checkpoint-50/model.safetensors\n",
      "tokenizer config file saved in output/mntp-supervised/Meta-Llama-3-sheared/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in output/mntp-supervised/Meta-Llama-3-sheared/checkpoint-50/special_tokens_map.json\n",
      "INFO:__main__:Saving model checkpoint to output/mntp-supervised/Meta-Llama-3-sheared/checkpoint-100\n",
      "Configuration saved in output/mntp-supervised/Meta-Llama-3-sheared/checkpoint-100/config.json\n",
      "Model weights saved in output/mntp-supervised/Meta-Llama-3-sheared/checkpoint-100/model.safetensors\n",
      "tokenizer config file saved in output/mntp-supervised/Meta-Llama-3-sheared/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in output/mntp-supervised/Meta-Llama-3-sheared/checkpoint-100/special_tokens_map.json\n",
      "INFO:__main__:Saving model checkpoint to output/mntp-supervised/Meta-Llama-3-sheared/checkpoint-150\n",
      "Configuration saved in output/mntp-supervised/Meta-Llama-3-sheared/checkpoint-150/config.json\n",
      "Model weights saved in output/mntp-supervised/Meta-Llama-3-sheared/checkpoint-150/model.safetensors\n",
      "tokenizer config file saved in output/mntp-supervised/Meta-Llama-3-sheared/checkpoint-150/tokenizer_config.json\n",
      "Special tokens file saved in output/mntp-supervised/Meta-Llama-3-sheared/checkpoint-150/special_tokens_map.json\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/trainer.py:2164\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2162\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2165\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2169\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/trainer.py:2524\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2517\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2518\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2519\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2520\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2521\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2522\u001b[0m )\n\u001b[1;32m   2523\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2524\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2526\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2527\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2528\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2529\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2530\u001b[0m ):\n\u001b[1;32m   2531\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2532\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/trainer.py:3654\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3651\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3653\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3654\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3656\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3657\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3658\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3660\u001b[0m ):\n",
      "Cell \u001b[0;32mIn[28], line 32\u001b[0m, in \u001b[0;36mSupervisedTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m d_reps_neg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(features) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m features[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 32\u001b[0m     d_reps_neg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_function(q_reps, d_reps, d_reps_neg)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_outputs:\n",
      "File \u001b[0;32m~/llm2vec-da/llm2vec_da/model_modifications/llm2vec_class.py:237\u001b[0m, in \u001b[0;36mLLM2Vec.forward\u001b[0;34m(self, sentence_feature)\u001b[0m\n\u001b[1;32m    234\u001b[0m reps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msentence_feature)\n\u001b[1;32m    235\u001b[0m sentence_feature[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membed_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m embed_mask\n\u001b[0;32m--> 237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_pooling\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence_feature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreps\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlast_hidden_state\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/llm2vec-da/llm2vec_da/model_modifications/llm2vec_class.py:248\u001b[0m, in \u001b[0;36mLLM2Vec.get_pooling\u001b[0;34m(self, features, last_hidden_states)\u001b[0m\n\u001b[1;32m    245\u001b[0m seq_lengths \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooling_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack(\n\u001b[0;32m--> 248\u001b[0m         [\n\u001b[1;32m    249\u001b[0m             last_hidden_states[i, \u001b[38;5;241m-\u001b[39mlength:, :]\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    250\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m i, length \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(seq_lengths)\n\u001b[1;32m    251\u001b[0m         ],\n\u001b[1;32m    252\u001b[0m         dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    253\u001b[0m     )\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooling_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweighted_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    255\u001b[0m     bs, l, _ \u001b[38;5;241m=\u001b[39m last_hidden_states\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/llm2vec-da/llm2vec_da/model_modifications/llm2vec_class.py:249\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    245\u001b[0m seq_lengths \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooling_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack(\n\u001b[1;32m    248\u001b[0m         [\n\u001b[0;32m--> 249\u001b[0m             \u001b[43mlast_hidden_states\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mlength\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m i, length \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(seq_lengths)\n\u001b[1;32m    251\u001b[0m         ],\n\u001b[1;32m    252\u001b[0m         dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    253\u001b[0m     )\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooling_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweighted_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    255\u001b[0m     bs, l, _ \u001b[38;5;241m=\u001b[39m last_hidden_states\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dtudeep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
