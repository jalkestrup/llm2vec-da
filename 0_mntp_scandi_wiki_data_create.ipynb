{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dd498a587c94a3992751ad336dd11c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "from datasets import load_dataset, concatenate_datasets, DatasetDict\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf356fd4282a41f0833cb88be54195af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0000.parquet:   0%|          | 0.00/50.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0efaddd47b24266a95c298d02b87fe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/55418 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c659170d83964625bf403709c0737e48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0000.parquet:   0%|          | 0.00/299M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3552fc44425e4fb182e71ceddaca2794",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0001.parquet:   0%|          | 0.00/272M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "637a150849bc4efbbc5c791b757049d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/596593 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70921abc56244a4e8629d800d3d3d8f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0000.parquet:   0%|          | 0.00/130M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16236953dfb94325911c24a1409aeb5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/162776 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5bd3fdf731549a89e8eacc167891ca1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0000.parquet:   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03575a3118a045cd9953601a6f08b9c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0001.parquet:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "195d5d5b8a204c089446c5331e0a954b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0002.parquet:   0%|          | 0.00/145M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8244888436f4ce3820a84c585f2bc06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0003.parquet:   0%|          | 0.00/190M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca38dadd39df416a8e1b95543e3557f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0004.parquet:   0%|          | 0.00/37.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce4d9641ad444a0cb115edf218c2383e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/2469978 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faf498e8a11d4aca84555fe1bdc2227d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/362582 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create an empty list to hold datasets\n",
    "all_datasets = []\n",
    "\n",
    "# Load and sample the Danish dataset\n",
    "da_dataset = load_dataset(\"alexandrainst/scandi-wiki\", 'da', split='train')\n",
    "da_sampled = da_dataset.shuffle().select(range(150000))\n",
    "all_datasets.append(da_sampled)\n",
    "\n",
    "# List of other language codes\n",
    "other_langs = ['fo', 'is', 'nb', 'nn', 'sv']\n",
    "\n",
    "# Sample 50,000 instances from each of the other languages\n",
    "for lang in other_langs:\n",
    "    lang_dataset = load_dataset(\"alexandrainst/scandi-wiki\", lang, split='train')\n",
    "    if len(lang_dataset) < 50000:\n",
    "        lang_sampled = lang_dataset\n",
    "    else:\n",
    "        lang_sampled = lang_dataset.shuffle().select(range(50000))\n",
    "    all_datasets.append(lang_sampled)\n",
    "\n",
    "# Combine all datasets into one\n",
    "combined_dataset = concatenate_datasets(all_datasets)\n",
    "combined_dataset = combined_dataset.shuffle()\n",
    "combined_dataset.save_to_disk(\"data/combined_scandi_wiki\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "247b8fa087e44978b96db91a86c2fea5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45b8f03651d64f00b283196843572f10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/363 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ee6a849ea8741b783079f3553692a77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/395 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/jealk/scandi-wiki-combined/commit/f8bb1532690d3d546d7dd208ccc792d67e72596d', commit_message='Shuffled dataset', commit_description='', oid='f8bb1532690d3d546d7dd208ccc792d67e72596d', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/jealk/scandi-wiki-combined', endpoint='https://huggingface.co', repo_type='dataset', repo_id='jealk/scandi-wiki-combined'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Push the directory with the multiple configurations to the Hub\n",
    "combined_dataset.push_to_hub(repo_id=\"jealk/scandi-wiki-combined\", config_name=\"default\", set_default=True, commit_message=\"Shuffled dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SIMCSE, Split to sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a9f8c2fccb7498b87bc9cdf5d80ad45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/362582 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Drop the wikipedia_id column and version_id column from all splits\n",
    "def drop_columns(ds):\n",
    "    \"\"\"Function to drop columns from a Hugging Face DatasetDict.\"\"\"\n",
    "    # Define the transformation to apply to each instance\n",
    "    def apply_drop_fn(batch):\n",
    "        batch.pop('id', None)\n",
    "        batch.pop('url', None)\n",
    "        batch.pop('title', None)\n",
    "        return batch    \n",
    "    \n",
    "    # Use `map` function to apply `apply_drop_fn` to the entire dataset (train, test, validation)\n",
    "    ds = ds.map(apply_drop_fn, batched=False)\n",
    "\n",
    "    return ds\n",
    "    \n",
    "# Load and sample the Danish dataset\n",
    "ds = load_dataset(\"jealk/scandi-wiki-combined\")\n",
    "ds = drop_columns(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d5193bc9c894823b98ab1408f959227",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/362582 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dbc640ae14442c3a1fca1e2fe77a802",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/4164833 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "from datasets import DatasetDict\n",
    "\n",
    "def split_into_sentences(text_batch):\n",
    "    \"\"\"\n",
    "    Splits each text entry in the input batch of texts into sentences using punctuation followed by capital letters\n",
    "    (., !, and ?) to recognize sentence boundaries. \n",
    "    Ensures that each split sentence becomes a new row in the dataset.\n",
    "    \"\"\"\n",
    "    # Regex to split sentences on .,!,? only when followed by whitespace and a capital letter (or special Danish capital letters)\n",
    "    split_regex = r'(?<=[.!?])(?=\\s+[A-ZÆØÅ])'\n",
    "\n",
    "    # List to store all sentences across the batch\n",
    "    all_sentences = []\n",
    "\n",
    "    # Process each text in the batch\n",
    "    for text in text_batch:\n",
    "        # Perform the smart splitting using the regex\n",
    "        sentences = re.split(split_regex, text.strip())\n",
    "\n",
    "        # Filter out empty results and remove extra whitespace from sentences, then extend the result list\n",
    "        all_sentences.extend([sentence.strip() for sentence in sentences if sentence.strip()])\n",
    "\n",
    "    # Return the flattened list of all sentences for this batch\n",
    "    return all_sentences\n",
    "\n",
    "def create_sentences_dataset(ds):\n",
    "    \"\"\"\n",
    "    Given a Hugging Face dataset, it splits paragraphs into sentences and returns a new dataset\n",
    "    where each sentence is a new row.\n",
    "    \"\"\"\n",
    "    # Apply the map function to split into sentences\n",
    "    ds_sentences = ds.map(\n",
    "        lambda batch: {\"text\": split_into_sentences(batch[\"text\"])},\n",
    "        batched=True,  # Process on batches\n",
    "        batch_size=1000,  # Adjust based on memory/capability, 1000 is a good value for efficiency\n",
    "        num_proc=4,  # Use multiple processes for performance (adjust based on machine)\n",
    "    )\n",
    "\n",
    "    # Flatten the dataset’s structure (make sure every sentence is an individual row)\n",
    "    ds_sentences = ds_sentences.flatten_indices()\n",
    "\n",
    "    return ds_sentences\n",
    "\n",
    "# Assuming 'ds' is the original dataset containing paragraphs\n",
    "ds_sentences = DatasetDict({\n",
    "    \"train\": create_sentences_dataset(ds['train']),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297647 sentences was removed, of the total 4164833 sentences.\n"
     ]
    }
   ],
   "source": [
    "sentences_before_filtering = sum(len(ds_sentences[split]) for split in ds_sentences.keys())\n",
    "\n",
    "def filter_short_and_long_sentences(example):\n",
    "    \"\"\"\n",
    "    Filters out sentences that are shorter than 5 words or longer than 100 words.\n",
    "    \"\"\"\n",
    "    word_count = len(example['text'].split())  # Calculate word count\n",
    "    return 5 <= word_count <= 100  # Only keep sentences with 5 <= word_count <= 100\n",
    "\n",
    "ds_sentences_filtered = ds_sentences.filter(filter_short_and_long_sentences)\n",
    "\n",
    "sentences_after_filtering = sum(len(ds_sentences_filtered[split]) for split in ds_sentences_filtered.keys())\n",
    "\n",
    "print(f\"{sentences_before_filtering - sentences_after_filtering} sentences was removed, of the total {sentences_before_filtering} sentences.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_sentences_filtered = ds_sentences_filtered['train'].shuffle().select(range(1000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdda50ada79542c2897b481cbe2e0e0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1000000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save the new sentence-level dataset to disk\n",
    "ds_sentences_filtered.save_to_disk(\"data/scandi-wiki-combined-sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6110cd7a9e84d33a9908308cf13da7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2f134e806e64ecb9dea6d53aafec67b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1000 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/jealk/scandi-wiki-combined/commit/a6f2e5dbfb3737248d629385c137a6056c160a9e', commit_message='Filtered sentences to be between 5 and 100 words long, 1M samples', commit_description='', oid='a6f2e5dbfb3737248d629385c137a6056c160a9e', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/jealk/scandi-wiki-combined', endpoint='https://huggingface.co', repo_type='dataset', repo_id='jealk/scandi-wiki-combined'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Push the directory with the multiple configurations to the Hub\n",
    "ds_sentences_filtered.push_to_hub(repo_id=\"jealk/scandi-wiki-combined\", config_name=\"sentences\", set_default=False, commit_message=\"Filtered sentences to be between 5 and 100 words long, 1M samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"wiki1m_for_simcse_scandi.txt\"\n",
    "train_texts = ds_sentences_filtered['text']\n",
    "\n",
    "# Write the dataset contents into the txt file, one sentence per line\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    for text in train_texts:\n",
    "        f.write(text + '\\n')  # Write each text line and ensure Danish characters are preserved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gemma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
