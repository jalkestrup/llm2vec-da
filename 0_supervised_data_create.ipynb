{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dk_datasets =[\n",
    "    {\n",
    "        \"dataset_name\": \"wiki_queries_gemma\",\n",
    "        \"dataset_hf_path\": \"DDSC/da-wikipedia-queries-gemma-processed\",\n",
    "        \"query\": \"anchor\",\n",
    "        \"pos\": \"positive\",\n",
    "        \"loss\": \"multiple_negatives_ranking\",\n",
    "        \"label\": \"\"\n",
    "    },\n",
    "    {\n",
    "        \"dataset_name\": \"hestenet\",\n",
    "        \"dataset_hf_path\": \"kardosdrur/hestenet-qa\",\n",
    "        \"query\": \"question\",\n",
    "        \"pos\": \"answer\",\n",
    "        \"loss\": \"multiple_negatives_ranking\",\n",
    "        \"label\": \"\"\n",
    "    },\n",
    "    {\n",
    "        \"dataset_name\": \"folketinget\",\n",
    "        \"dataset_hf_path\": \"kardosdrur/folketinget-discussions\",\n",
    "        \"query\": \"comment\",\n",
    "        \"pos\": \"response\",\n",
    "        \"loss\": \"multiple_negatives_ranking\",\n",
    "        \"label\": \"\"\n",
    "    },\n",
    "    {\n",
    "        \"dataset_name\": \"wiki_qa\",\n",
    "        \"dataset_hf_path\": \"kardosdrur/dawiki_qa_zephyr\",\n",
    "        \"query\": \"question\",\n",
    "        \"pos\": \"answer\",\n",
    "        \"loss\": \"multiple_negatives_ranking\",\n",
    "        \"label\": \"\"\n",
    "    },\n",
    "    {\n",
    "        \"dataset_name\": \"news\",\n",
    "        \"dataset_hf_path\": \"jealk/danews_title_content_512\",\n",
    "        \"config_name\": \"256-token\",\n",
    "        \"query\": \"title\",\n",
    "        \"pos\": \"content\",\n",
    "        \"loss\": \"multiple_negatives_ranking\",\n",
    "        \"label\": \"\"\n",
    "    },\n",
    "    {\n",
    "        \"dataset_name\": \"opensubtitles_da_no\",\n",
    "        \"dataset_hf_path\": \"kardosdrur/opensubtitles-no-da\",\n",
    "        \"query\": \"no\",\n",
    "        \"pos\": \"da\",\n",
    "        \"loss\": \"multiple_negatives_ranking\",\n",
    "        \"label\": \"overlap\"\n",
    "    },\n",
    "    {\n",
    "        \"dataset_name\": \"europarl\",\n",
    "        \"dataset_hf_path\": \"kardosdrur/europarl-scandinavian\",\n",
    "        \"query\": \"sv\",\n",
    "        \"pos\": \"da\",\n",
    "        \"loss\": \"multiple_negatives_ranking\",\n",
    "        \"label\": \"\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets, DatasetDict\n",
    "from datasets import Dataset as HFDataset\n",
    "\n",
    "def create_combined_dataset(json_list):\n",
    "    combined_train_data = []\n",
    "    combined_test_data = []\n",
    "\n",
    "    for entry in json_list:\n",
    "        try:\n",
    "            # Load the dataset and check for train/test splits\n",
    "            if 'config_name' in entry:\n",
    "                dataset_dict = load_dataset(entry['dataset_hf_path'], name=entry['config_name'])\n",
    "            else:\n",
    "                dataset_dict = load_dataset(entry['dataset_hf_path'])\n",
    "\n",
    "            if 'train' in dataset_dict and 'test' in dataset_dict:\n",
    "                train_dataset = dataset_dict['train']\n",
    "                test_dataset = dataset_dict['test']\n",
    "            elif 'train' in dataset_dict:\n",
    "                train_dataset = dataset_dict['train']\n",
    "                # Create a test split if not present\n",
    "                test_size = max(1, int(0.05 * len(train_dataset)))\n",
    "                train_size = len(train_dataset) - test_size\n",
    "                train_dataset, test_dataset = train_dataset.train_test_split(test_size=test_size).values()\n",
    "            else:\n",
    "                # Use first available split to create both train and test splits\n",
    "                first_split = next(iter(dataset_dict))\n",
    "                dataset = dataset_dict[first_split]\n",
    "                test_size = max(1, int(0.05 * len(dataset)))\n",
    "                train_size = len(dataset) - test_size\n",
    "                train_dataset, test_dataset = dataset.train_test_split(test_size=test_size).values()\n",
    "\n",
    "        except ValueError:\n",
    "            raise Exception(f\"Error loading dataset: {entry['dataset_hf_path']}\")\n",
    "\n",
    "        # Select only relevant columns and limit to 10,000 samples per dataset\n",
    "        def process_data(dataset):\n",
    "            # Determine if we should subsample\n",
    "            if entry['dataset_name'] in [\"europarl\", \"opensubtitles_da_no\"]:\n",
    "                dataset = dataset.select(range(min(10000, len(dataset))))\n",
    "            if entry['dataset_name'] == \"news\":\n",
    "                dataset = dataset.select(range(min(30000, len(dataset))))\n",
    "            \n",
    "            return dataset.map(\n",
    "                lambda example: {\n",
    "                    'query': example[entry['query']],\n",
    "                    'pos': example[entry['pos']],\n",
    "                    'dataset_name': entry['dataset_name'],\n",
    "                    'label': max(0.0, min(1.0, float(example[entry['label']]))) if entry['label'] and example[entry['label']] else 1.0\n",
    "                },\n",
    "                remove_columns=dataset.column_names\n",
    "            )\n",
    "\n",
    "        # Process train and test datasets\n",
    "        train_dataset = process_data(train_dataset)\n",
    "        test_dataset = process_data(test_dataset)\n",
    "\n",
    "        # Append processed data to lists\n",
    "        combined_train_data.append(train_dataset)\n",
    "        combined_test_data.append(test_dataset)\n",
    "\n",
    "    # Concatenate all train and test datasets into one\n",
    "    combined_train_dataset = concatenate_datasets(combined_train_data)\n",
    "    combined_test_dataset = concatenate_datasets(combined_test_data)\n",
    "\n",
    "    return DatasetDict({\"train\": combined_train_dataset, \"test\": combined_test_dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fde9fc009af14d95876d72ad413d92bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/28766 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aab8edd7d4d149c6baf3207e8d84b3ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1514 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9c9a736b6234a63a08de259b43ba974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/30000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c928cae739bf4dc699e2c1eead0dccad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/30000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "combined_dataset = create_combined_dataset(dk_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "426e88b0162246b2bb8b0f8d4596705c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b04e3c7942104d41bf22f07d4a696b75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/94 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc66fe9aeb384caca5db0284c0b1a2f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9adf1c2adb36433bbfa4c1d8d035245a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/56 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "797453d8382d481c936a84669ff0ad3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/498 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/jealk/supervised-da/commit/7a832ba37cc3cb21c18d5eb8b61d9500e6f8f213', commit_message='Filtered da-news to only include 256 token context, and truncated at 30.000 samples', commit_description='', oid='7a832ba37cc3cb21c18d5eb8b61d9500e6f8f213', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/jealk/supervised-da', endpoint='https://huggingface.co', repo_type='dataset', repo_id='jealk/supervised-da'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Push the directory with the multiple configurations to the Hub\n",
    "combined_dataset.push_to_hub(repo_id=\"jealk/supervised-da\", config_name=\"default\", set_default=True, commit_message=\"Filtered da-news to only include 256 token context, and truncated at 30.000 samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local cache for llama2vec default training script.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae8fd26e4a554ec5bd70004245ae66de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/489 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd70c108d9b646ea9cfe6f20724a9e12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/51.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f09a4ef26534eab8249024f121e2f38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/38.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e868bca5adb5458e8164cdae45058ecd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/93200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd2c42ae3e514c5d9e5074ec32f27832",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/55124 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "combined_dataset = load_dataset(\"jealk/supervised-da\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving each dataset as a seperate JSONL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets saved in cache/dk-data as JSONL files.\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import os\n",
    "import json\n",
    "\n",
    "def save_combined_dataset_as_jsonl(combined_dataset, save_directory):\n",
    "    os.makedirs(save_directory, exist_ok=True)\n",
    "    grouped_samples = {}\n",
    "\n",
    "    for sample in combined_dataset:\n",
    "        dataset_name = sample['dataset_name']\n",
    "        \n",
    "        if dataset_name not in grouped_samples:\n",
    "            grouped_samples[dataset_name] = []\n",
    "        \n",
    "        # Prepare the sample\n",
    "        json_sample = {\n",
    "            \"query\": sample['query'],\n",
    "            \"positive\": sample['pos'],\n",
    "            \"negative\": \"\"  # Placeholder: you might have logic to set this\n",
    "                            # if there are negative examples as well\n",
    "        }\n",
    "        \n",
    "        grouped_samples[dataset_name].append(json_sample)\n",
    "    \n",
    "    for dataset_name, samples in grouped_samples.items():\n",
    "        jsonl_filepath = os.path.join(save_directory, f\"{dataset_name}.jsonl\")\n",
    "        \n",
    "        with open(jsonl_filepath, 'w', encoding='utf-8') as f:\n",
    "            for sample in samples:\n",
    "                f.write(json.dumps(sample) + \"\\n\")\n",
    "                \n",
    "    print(f\"Datasets saved in {save_directory} as JSONL files.\")\n",
    "\n",
    "\n",
    "save_directory = \"cache/dk-data\"\n",
    "save_combined_dataset_as_jsonl(combined_dataset['train'], save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the dataset as a combined JSONL\n",
    "**NOTE**: Shuffling the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset saved in cache/dk-data/supervised_dk_combined.jsonl\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Shufle the dataset\n",
    "combined_dataset['train'] = combined_dataset['train'].shuffle()\n",
    "\n",
    "# Define the save path\n",
    "save_directory = \"cache/dk-data\"\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "jsonl_filepath = os.path.join(save_directory, \"supervised_dk_combined.jsonl\")\n",
    "\n",
    "# Save the shuffled samples as a JSONL file\n",
    "with open(jsonl_filepath, 'w', encoding='utf-8') as f:\n",
    "    for sample in combined_dataset['train']:\n",
    "        json_sample = {\n",
    "            \"query\": sample['query'],\n",
    "            \"positive\": sample['pos'],\n",
    "            \"negative\": \"\"  # Placeholder: customize based on your logic for negative samples\n",
    "        }\n",
    "        f.write(json.dumps(json_sample) + \"\\n\")\n",
    "\n",
    "print(f\"Combined dataset saved in {jsonl_filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering the news dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import tiktoken\n",
    "encoding = tiktoken.encoding_for_model('gpt-4o')\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"kardosdrur/danews_title_content\")\n",
    "\n",
    "# Define a function to filter based on the length of 'content'\n",
    "def filter_by_content_length(example):\n",
    "    return len(example['content']) < 5000\n",
    "\n",
    "# Apply the filter to the train split or any split of your interest\n",
    "filtered_dataset = dataset['train'].filter(filter_by_content_length)\n",
    "#Plot distribution of token lengths\n",
    "# Filter away any examples that are longer than 512 tokens\n",
    "filtered_dataset = filtered_dataset.filter(lambda x: len(encoding.encode(x['content'])) < 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Push the directory with the multiple configurations to the Hub\n",
    "filtered_dataset.push_to_hub(repo_id=\"jealk/danews_title_content_512\", config_name=\"256-token\", set_default=False, commit_message=\"Version filtered down to max 256 tokens\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gemma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
