{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba2f9fd8729a4964b24b9ed7acf898f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "from datasets import load_dataset, concatenate_datasets, DatasetDict\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f91fe0844ffe416aba9996e2b65a429c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/412582 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create an empty list to hold datasets\n",
    "all_datasets = []\n",
    "\n",
    "# Load and sample the Danish dataset\n",
    "da_dataset = load_dataset(\"alexandrainst/scandi-wiki\", 'da', split='train')\n",
    "da_sampled = da_dataset.shuffle().select(range(200000))\n",
    "all_datasets.append(da_sampled)\n",
    "\n",
    "# List of other language codes\n",
    "other_langs = ['fo', 'is', 'nb', 'nn', 'sv']\n",
    "\n",
    "# Sample 50,000 instances from each of the other languages\n",
    "for lang in other_langs:\n",
    "    lang_dataset = load_dataset(\"alexandrainst/scandi-wiki\", lang, split='train')\n",
    "    if len(lang_dataset) < 50000:\n",
    "        lang_sampled = lang_dataset\n",
    "    else:\n",
    "        lang_sampled = lang_dataset.shuffle().select(range(50000))\n",
    "    all_datasets.append(lang_sampled)\n",
    "\n",
    "# Combine all datasets into one\n",
    "combined_dataset = concatenate_datasets(all_datasets)\n",
    "combined_dataset = combined_dataset.shuffle()\n",
    "combined_dataset.save_to_disk(\"data/combined_scandi_wiki\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "702074567bc84c46a8cf7a0bc6745e7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a965cad48bf14ae89d5bedb2f10c1a13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/413 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3dccd7332014a63abbda41b5cb25de6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/720 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/jealk/scandi-wiki-combined/commit/08cc77f0f49d2fb47b4c477455d34c9697efe50b', commit_message='Increased number of da samples to 200k', commit_description='', oid='08cc77f0f49d2fb47b4c477455d34c9697efe50b', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/jealk/scandi-wiki-combined', endpoint='https://huggingface.co', repo_type='dataset', repo_id='jealk/scandi-wiki-combined'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Push the directory with the multiple configurations to the Hub\n",
    "combined_dataset.push_to_hub(repo_id=\"jealk/scandi-wiki-combined\", config_name=\"default\", set_default=True, commit_message=\"Increased number of da samples to 200k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SIMCSE, Split to sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a4078c366ca42ef91e280e1771cd953",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/720 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe772d122c324a92aa9481cef6440108",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/366M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d156a4d13604330b36940f50f4cddb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/362582 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ae6a288992f43a9803c8c02bee8b07f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/362582 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Drop the wikipedia_id column and version_id column from all splits\n",
    "def drop_columns(ds):\n",
    "    \"\"\"Function to drop columns from a Hugging Face DatasetDict.\"\"\"\n",
    "    # Define the transformation to apply to each instance\n",
    "    def apply_drop_fn(batch):\n",
    "        batch.pop('id', None)\n",
    "        batch.pop('url', None)\n",
    "        batch.pop('title', None)\n",
    "        return batch    \n",
    "    \n",
    "    # Use `map` function to apply `apply_drop_fn` to the entire dataset (train, test, validation)\n",
    "    ds = ds.map(apply_drop_fn, batched=False)\n",
    "\n",
    "    return ds\n",
    "    \n",
    "# Load and sample the Danish dataset\n",
    "ds = load_dataset(\"jealk/scandi-wiki-combined\")\n",
    "ds = drop_columns(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "784fd4c2b6eb4c50b889f24e2d2a3e18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/362582 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "647cad36ac614b1388dbbbc328882173",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/4164833 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "from datasets import DatasetDict\n",
    "\n",
    "def split_into_sentences(text_batch):\n",
    "    \"\"\"\n",
    "    Splits each text entry in the input batch of texts into sentences using punctuation followed by capital letters\n",
    "    (., !, and ?) to recognize sentence boundaries. \n",
    "    Ensures that each split sentence becomes a new row in the dataset.\n",
    "    \"\"\"\n",
    "    # Regex to split sentences on .,!,? only when followed by whitespace and a capital letter (or special Danish capital letters)\n",
    "    split_regex = r'(?<=[.!?])(?=\\s+[A-ZÆØÅ])'\n",
    "\n",
    "    # List to store all sentences across the batch\n",
    "    all_sentences = []\n",
    "\n",
    "    # Process each text in the batch\n",
    "    for text in text_batch:\n",
    "        # Perform the smart splitting using the regex\n",
    "        sentences = re.split(split_regex, text.strip())\n",
    "\n",
    "        # Filter out empty results and remove extra whitespace from sentences, then extend the result list\n",
    "        all_sentences.extend([sentence.strip() for sentence in sentences if sentence.strip()])\n",
    "\n",
    "    # Return the flattened list of all sentences for this batch\n",
    "    return all_sentences\n",
    "\n",
    "def create_sentences_dataset(ds):\n",
    "    \"\"\"\n",
    "    Given a Hugging Face dataset, it splits paragraphs into sentences and returns a new dataset\n",
    "    where each sentence is a new row.\n",
    "    \"\"\"\n",
    "    # Apply the map function to split into sentences\n",
    "    ds_sentences = ds.map(\n",
    "        lambda batch: {\"text\": split_into_sentences(batch[\"text\"])},\n",
    "        batched=True,  # Process on batches\n",
    "        batch_size=1000,  # Adjust based on memory/capability, 1000 is a good value for efficiency\n",
    "        num_proc=4,  # Use multiple processes for performance (adjust based on machine)\n",
    "    )\n",
    "\n",
    "    # Flatten the dataset’s structure (make sure every sentence is an individual row)\n",
    "    ds_sentences = ds_sentences.flatten_indices()\n",
    "\n",
    "    return ds_sentences\n",
    "\n",
    "# Assuming 'ds' is the original dataset containing paragraphs\n",
    "ds_sentences = DatasetDict({\n",
    "    \"train\": create_sentences_dataset(ds['train']),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b65f932963e940b993c54b3bbfdd091e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/4164833 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297647 sentences was removed, of the total 4164833 sentences.\n"
     ]
    }
   ],
   "source": [
    "sentences_before_filtering = sum(len(ds_sentences[split]) for split in ds_sentences.keys())\n",
    "\n",
    "def filter_short_and_long_sentences(example):\n",
    "    \"\"\"\n",
    "    Filters out sentences that are shorter than 5 words or longer than 100 words.\n",
    "    \"\"\"\n",
    "    word_count = len(example['text'].split())  # Calculate word count\n",
    "    return 5 <= word_count <= 100  # Only keep sentences with 5 <= word_count <= 100\n",
    "\n",
    "ds_sentences_filtered = ds_sentences.filter(filter_short_and_long_sentences)\n",
    "\n",
    "sentences_after_filtering = sum(len(ds_sentences_filtered[split]) for split in ds_sentences_filtered.keys())\n",
    "\n",
    "print(f\"{sentences_before_filtering - sentences_after_filtering} sentences was removed, of the total {sentences_before_filtering} sentences.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle sentences\n",
    "ds_sentences_filtered = ds_sentences_filtered['train'].shuffle().select(range(1000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdda50ada79542c2897b481cbe2e0e0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1000000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save the new sentence-level dataset to disk\n",
    "ds_sentences_filtered.save_to_disk(\"data/scandi-wiki-combined-sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6110cd7a9e84d33a9908308cf13da7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2f134e806e64ecb9dea6d53aafec67b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1000 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/jealk/scandi-wiki-combined/commit/a6f2e5dbfb3737248d629385c137a6056c160a9e', commit_message='Filtered sentences to be between 5 and 100 words long, 1M samples', commit_description='', oid='a6f2e5dbfb3737248d629385c137a6056c160a9e', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/jealk/scandi-wiki-combined', endpoint='https://huggingface.co', repo_type='dataset', repo_id='jealk/scandi-wiki-combined'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Push the directory with the multiple configurations to the Hub\n",
    "ds_sentences_filtered.push_to_hub(repo_id=\"jealk/scandi-wiki-combined\", config_name=\"sentences\", set_default=False, commit_message=\"Filtered sentences to be between 5 and 100 words long, 1M samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local txt file\n",
    "Needed for default LLM2Vec data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "131501d3f8d140ff8b160f4a605f4054",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/101M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a1956bd2588404e92f858c3b83c46ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1000000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "from datasets import load_dataset, concatenate_datasets, DatasetDict\n",
    "\n",
    "scandi_dataset = load_dataset(\"jealk/scandi-wiki-combined\", 'sentences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"data/wiki1m_for_simcse_scandi.txt\"\n",
    "train_texts = scandi_dataset['train']['text']\n",
    "\n",
    "# Write the dataset contents into the txt file, one sentence per line\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    for text in train_texts:\n",
    "        f.write(text + '\\n')  # Write each text line and ensure Danish characters are preserved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
