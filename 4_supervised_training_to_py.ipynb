{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp 4_supervised_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised training\n",
    "Refer to \n",
    "https://github.com/jalkestrup/llm2vec-dtu/blob/main/experiments/run_supervised.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "from transformers import HfArgumentParser, TrainingArguments\n",
    "from tqdm import tqdm\n",
    "from accelerate import Accelerator, DistributedDataParallelKwargs\n",
    "from huggingface_hub import HfApi\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Union\n",
    "import torch\n",
    "\n",
    "from llm2vec_da import LLM2Vec\n",
    "from llm2vec_da.model import initialize_peft\n",
    "from llm2vec_da.data_utils import custom_dataset\n",
    "from llm2vec_da.loss.utils import load_loss\n",
    "from llm2vec_da.training import MixedNegCollator, SupervisedTrainer, StopTrainingCallback\n",
    "\n",
    "\n",
    "from llm2vec_da.arguments import EmbeddingModelArguments, DataTrainingArguments, CustomArguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/teamspace/studios/this_studio/llm2vec-da\n"
     ]
    }
   ],
   "source": [
    "#|export\n",
    "load_dotenv()\n",
    "\n",
    "api = HfApi(token=os.getenv(\"HF_TOKEN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, login with huggingface_hub GUI\n",
    "#notebook_login()\n",
    "\n",
    "# Handle lighting AI studio path\n",
    "if '/teamspace' in os.getcwd():\n",
    "    os.chdir('/teamspace/studios/this_studio/llm2vec-da')\n",
    "    # Hmm lighting AI studio changed to the below ..?\n",
    "    #os.chdir('/home/zeus/content/llm2vec-da')\n",
    "    print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "supervised_parser = HfArgumentParser(\n",
    "        (EmbeddingModelArguments, DataTrainingArguments, TrainingArguments, CustomArguments)\n",
    "    )\n",
    "\n",
    "model_args, data_args, training_args, custom_args = supervised_parser.parse_json_file(\n",
    "        \"configs/supervised/MetaLlama3-sheared.json\"\n",
    "    )\n",
    "\n",
    "if training_args.ddp_find_unused_parameters:\n",
    "    kwargs = [\n",
    "        DistributedDataParallelKwargs(\n",
    "            dim=0,\n",
    "            broadcast_buffers=True,\n",
    "            bucket_cap_mb=25,\n",
    "            find_unused_parameters=True,\n",
    "            check_reduction=False,\n",
    "            gradient_as_bucket_view=False,\n",
    "        )\n",
    "    ]\n",
    "else:\n",
    "    kwargs = []\n",
    "\n",
    "accelerator = Accelerator(kwargs_handlers=kwargs)\n",
    "transformers.set_seed(training_args.seed)\n",
    "\n",
    "#ABSOLUTELY CRITICAL OR WILL CAUSE OBSCURE NO GRAD ERROR THAT TOOK FREAKING 4 HOURS TO IDENTIFY\n",
    "if training_args.gradient_checkpointing:\n",
    "    training_args.gradient_checkpointing_kwargs = {\"use_reentrant\": False}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'Hvad var de langsigtede konsekvenser for dansk økonomi af den danske forfatningslov af 1849?', 'positive': 'Den danske forfatningslov af 1849 markerede et paradigmeskift i dansk politisk og økonomisk struktur.  Loven etableret et konstitutionelt monarki, begrænsede kongelig magt og indførte en parlamentarisk form for styre. Dette havde vidtrækkende konsekvenser for den danske økonomi.  Den nye forfatning lagde grunden for en mere liberalistisk økonomisk orden, med fokus på fri handel, privat ejendomsret og entreprenørskab.  Den øgede politiske stabilitet og forudsigelighed tiltrak udenlandsk kapital og investeringer, der bidrog til økonomisk vækst.  Samtidig reducerede loven den statslige indblanding i økonomien, hvilket gav plads til privat initiativ og markedskræfter.  Introduktionen af en national valuta og en centralbank styrkede den økonomiske integration med andre europæiske lande.  Mens forfatningsloven af 1849 ikke direkte førte til økonomisk mirakel, lagde den grunden for en periode med vækst og modernisering af den danske økonomi.', 'negative': 'Den danske forfatningslov af 1849 havde stor betydning for udviklingen af dansk national identitet.  Loven vedtog princippet om folkevælde og lige rettigheder for alle borgere, hvilket styrkede følelsen af fælleskab og national tilhørsforhold.  Den nye forfatning gav også anledning til en livlig debat om Danmarks fremtid og rolle i Europa.  Mange grundede politiske partier og foreninger, som arbejdede for at fremme deres visioner for det nye Danmark.  På kulturelt plan inspirerede forfatningsloven til en blomstringsperiode for kunst og litteratur, der reflekterede den nye tid og de nye ideer.', 'language': 'danish', 'task': 'retrieval', 'instruction': 'Locate historical documents mentioning a specific event.', 'prompt': [{'content': 'You have been assigned a retrieval task: Locate historical documents mentioning a specific event.\\n    Your mission is to write one text retrieval example for this task in JSON format. The JSON object must contain the following\\n    keys:\\n    - \"user_query\": a string, a random user search query specified by the retrieval task.\\n    - \"positive_document\": a string, a relevant document for the user query.\\n    - \"hard_negative_document\": a string, a hard negative document that only appears relevant to the query.\\n    Please adhere to the following guidelines:\\n    - The \"user_query\" should be long-tail, 5 to 15 words, clear, and diverse in topic.\\n    - All documents must be created independent of the query. Avoid copying the query verbatim. It’s acceptable if some parts of\\n    the \"positive_document\" are not topically related to the query.\\n    - All documents should be at least 200 words long.\\n    - The \"hard_negative_document\" contains some useful information, but it should be less useful or comprehensive compared\\n    to the \"positive_document\".\\n    - Both the query and documents should be in DANISH.\\n    - Do not provide any explanation in any document on why it is relevant or not relevant to the query.\\n    - Both the query and documents require PhD level education to understand.\\n    Your output must always be a JSON object only, do not explain yourself or output anything else. Be creative!', 'role': 'user'}], 'response': '```json\\n{\\n  \"user_query\": \"Hvad var de langsigtede konsekvenser for dansk økonomi af den danske forfatningslov af 1849?\",\\n  \"positive_document\": \"Den danske forfatningslov af 1849 markerede et paradigmeskift i dansk politisk og økonomisk struktur.  Loven etableret et konstitutionelt monarki, begrænsede kongelig magt og indførte en parlamentarisk form for styre. Dette havde vidtrækkende konsekvenser for den danske økonomi.  Den nye forfatning lagde grunden for en mere liberalistisk økonomisk orden, med fokus på fri handel, privat ejendomsret og entreprenørskab.  Den øgede politiske stabilitet og forudsigelighed tiltrak udenlandsk kapital og investeringer, der bidrog til økonomisk vækst.  Samtidig reducerede loven den statslige indblanding i økonomien, hvilket gav plads til privat initiativ og markedskræfter.  Introduktionen af en national valuta og en centralbank styrkede den økonomiske integration med andre europæiske lande.  Mens forfatningsloven af 1849 ikke direkte førte til økonomisk mirakel, lagde den grunden for en periode med vækst og modernisering af den danske økonomi.\",\\n  \"hard_negative_document\": \"Den danske forfatningslov af 1849 havde stor betydning for udviklingen af dansk national identitet.  Loven vedtog princippet om folkevælde og lige rettigheder for alle borgere, hvilket styrkede følelsen af fælleskab og national tilhørsforhold.  Den nye forfatning gav også anledning til en livlig debat om Danmarks fremtid og rolle i Europa.  Mange grundede politiske partier og foreninger, som arbejdede for at fremme deres visioner for det nye Danmark.  På kulturelt plan inspirerede forfatningsloven til en blomstringsperiode for kunst og litteratur, der reflekterede den nye tid og de nye ideer.\"\\n}\\n```\\n'}\n"
     ]
    }
   ],
   "source": [
    "#|export\n",
    "\n",
    "dataset = load_dataset(data_args.dataset_name,\n",
    "split=\"train[:10%]\")\n",
    "#,columns=['query', 'positive', 'negative', 'instruction', 'task'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdataset\u001b[49m[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "print(dataset[0])\n",
    "# Optionally, save to local file\n",
    "#dataset.save_to_disk(\"nordic-embedding-training-data\")\n",
    "\n",
    "# Optionally, load from local file\n",
    "#from datasets import load_from_disk\n",
    "#ds_transformed = load_from_disk(\"/teamspace/studios/this_studio/synthetic-supervised-dataset-2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "# Split the dataset into 95% train and 5% test\n",
    "split_dataset = dataset.train_test_split(test_size=0.05, seed=42)\n",
    "\n",
    "# Define the splits\n",
    "train_dataset = split_dataset['train']\n",
    "valid_dataset = split_dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 91983/91983 [00:11<00:00, 7891.17it/s]\n",
      "Grouping data by task: 100%|██████████| 91983/91983 [00:00<00:00, 2248842.45it/s]\n",
      "INFO:__main__:Batching data for effective batch size = 32 ...\n",
      "Batching data:   0%|          | 0/2 [00:00<?, ?it/s]INFO:__main__:Skipping partial batch of 21 samples for task retrieval\n",
      "INFO:__main__:Skipping partial batch of 26 samples for task classification\n",
      "Batching data: 100%|██████████| 2/2 [00:00<00:00, 382.88it/s]\n",
      "INFO:__main__:Loaded and batched 91936 samples from 2 tasks\n",
      "Loading dataset: 100%|██████████| 4842/4842 [00:00<00:00, 7915.46it/s]\n",
      "Grouping data by task: 100%|██████████| 4842/4842 [00:00<00:00, 2702078.23it/s]\n",
      "INFO:__main__:Batching data for effective batch size = 32 ...\n",
      "Batching data:   0%|          | 0/2 [00:00<?, ?it/s]INFO:__main__:Skipping partial batch of 26 samples for task retrieval\n",
      "INFO:__main__:Skipping partial batch of 16 samples for task classification\n",
      "Batching data: 100%|██████████| 2/2 [00:00<00:00, 2810.25it/s]\n",
      "INFO:__main__:Loaded and batched 4800 samples from 2 tasks\n"
     ]
    }
   ],
   "source": [
    "#|export\n",
    "\n",
    "train_dataset_e5 = custom_dataset(train_dataset, \n",
    "                                 effective_batch_size=training_args.per_device_train_batch_size* accelerator.num_processes)\n",
    "\n",
    "valid_dataset_e5 = custom_dataset(valid_dataset,\n",
    "                                effective_batch_size=training_args.per_device_train_batch_size* accelerator.num_processes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Locate job postings requiring a specific skill set and industry experience.; !@#$%^&*()Jeg søger en stilling som postdoc forsker indenfor kvantitativ genetik med fokus på genomisk selektion hos hvede, hvor jeg kan kombinere mineOoh ekspertise i statistisk modellering med brugen af moderne højtydende computer clusters.',\n",
       " '!@#$%^&*()Vi søger en motiveret og dygtig postdoc forsker til at slutte sig til vores hold, der arbejder med at forstå de genetiske mekanismer bag udvikling af acidotolerante hvedesorter. Projektet involverer anvendelsen af genomisk selektionsanalyse, kvantitativ genetik og avanceret statistisk modellering. Du vil have adgang til state-of-the-art computerclusters og arbejde med store datasæt af sekvensdata. Kendskab til programmeringssprog som R eller Python er et krav. En afhandling indenfor et relevant felt, f.eks. genetik, bioinformatik eller matematik, forventes.',\n",
       " '!@#$%^&*()Vores team af forskere er specialiseret i anvendelse af maskinlæring til at analysere store datasæt af biomedicinske billeder. Vi er på udkig efter en dygtig postdoc med erfaring indenfor billedanalyse og deep learning til at bidrage til vores projekt, der sigter mod at udvikle nye metoder til diagnose af sygdomme baseret på medicinske billeddata. Stærke programmeringsevner og erfaring med Python er en forudsætning.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_e5[0].texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch import nn\n",
    "# from transformers import AutoTokenizer, AutoModel\n",
    "# from transformers import LlamaConfig\n",
    "\n",
    "\n",
    "# class TinyLLM2Vec(nn.Module):\n",
    "#     \"\"\"\n",
    "#     Drop‑in replacement for LLM2Vec that is tiny but respects the API:\n",
    "#       - .tokenize(list[str]) -> dict[str, Tensor] batch encoding\n",
    "#       - .encode(features)    -> Tensor (batch, D)\n",
    "#       - .pooling_mode attr   -> str\n",
    "#     \"\"\"\n",
    "#     def __init__(self, model_name=\"prajjwal1/bert-tiny\", pooling_mode=\"cls\"):\n",
    "#         super().__init__()\n",
    "#         self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#         self.model     = AutoModel.from_pretrained(model_name)\n",
    "#         self.config = self.model.config          # forward attr used by prep‑fn\n",
    "#         #self.config = LlamaConfig()\n",
    "#         self.config._name_or_path = \"meta-llama/Meta-Llama-3-8B\" # To fake the config\n",
    "        \n",
    "#         self.pooling_mode = pooling_mode   # value read by prepare_for_tokenization\n",
    "\n",
    "#     @torch.no_grad()\n",
    "#     def tokenize(self, texts):\n",
    "#         return self.tokenizer(\n",
    "#             texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512\n",
    "#         )\n",
    "\n",
    "#     @torch.no_grad()\n",
    "#     def encode(self, features):\n",
    "#         out = self.model(**features).last_hidden_state   # (B, L, H)\n",
    "#         if self.pooling_mode == \"cls\":\n",
    "#             return out[:, 0]                             # (B, H)\n",
    "#         elif self.pooling_mode == \"mean\":\n",
    "#             mask = features[\"attention_mask\"].unsqueeze(-1)\n",
    "#             return (out * mask).sum(1) / mask.sum(1)     # (B, H)\n",
    "#         else:\n",
    "#             raise ValueError(\"Unknown pooling mode\")\n",
    "\n",
    "# model_tiny = TinyLLM2Vec(pooling_mode=\"mean\")      # instead of Llama‑8B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test of prepare_for_tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set _name_or_path to define tokenizer model behavior\n",
    "# model_tiny.config._name_or_path =  \"meta-llama/Meta-Llama-3-8B\"\n",
    "\n",
    "# # Inspect the input query and the output query before and after\n",
    "# print(f'Input query: {train_dataset_e5[0].texts[0]}')\n",
    "# print(f'Output query: {prepare_for_tokenization(model_tiny, train_dataset_e5[0].texts[0], pooling_mode=\"eos_token\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llm2vec_da.loss import HardNegativeNLLLoss\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# collator = MixedNegCollator(model_tiny)           # the new collator\n",
    "\n",
    "# loader   = DataLoader(\n",
    "#                dataset=train_dataset_e5,\n",
    "#                batch_size=32,                 \n",
    "#                shuffle=False, # DO NOT SHUFFLE, batching is done in the dataset class\n",
    "#                collate_fn=collator\n",
    "#            )\n",
    "\n",
    "# loss_fn  = HardNegativeNLLLoss(scale=20.0)   # unchanged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = next(iter(loader))\n",
    "# (q_feat, p_feat, n_feat), _ = batch\n",
    "\n",
    "# q_reps = model_tiny.encode(q_feat)                # (B, D)\n",
    "# p_reps = model_tiny.encode(p_feat)                # (B, D)\n",
    "# n_reps = model_tiny.encode(n_feat) if n_feat else None\n",
    "\n",
    "# loss = loss_fn(q_reps, p_reps, n_reps)\n",
    "# print(\"forward OK, loss =\", loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    }
   ],
   "source": [
    "#|export\n",
    "torch_dtype = (\n",
    "    model_args.torch_dtype\n",
    "    if model_args.torch_dtype in [\"auto\", None]\n",
    "    else getattr(torch, model_args.torch_dtype)\n",
    ")\n",
    "\n",
    "#training_args.gradient_checkpointing = False   # turn it off\n",
    "model = LLM2Vec.from_pretrained(\n",
    "    base_model_name_or_path=model_args.model_name_or_path,\n",
    "    enable_bidirectional=model_args.bidirectional,\n",
    "    peft_model_name_or_path=\"McGill-NLP/LLM2Vec-Sheared-LLaMA-mntp\",\n",
    "    merge_peft=True,\n",
    "    pooling_mode=model_args.pooling_mode,\n",
    "    max_length=data_args.max_seq_length,\n",
    "    torch_dtype=torch_dtype,\n",
    "    low_cpu_mem_usage=model_args.low_cpu_mem_usage,\n",
    "    attn_implementation=\"sdpa\", #OBS SET BACK TO FLASH ATTENTION WHEN RUNNING ON A100 GPU!!\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up PEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:peft.tuners.tuners_utils:Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's Lora trainable parameters:\n",
      "trainable params: 14,991,360 || all params: 1,294,878,720 || trainable%: 1.1577\n"
     ]
    }
   ],
   "source": [
    "#|export\n",
    "peft_model = initialize_peft(\n",
    "    model.model,\n",
    "    lora_r=custom_args.lora_r,\n",
    "    lora_alpha=2 * custom_args.lora_r,\n",
    "    lora_dropout=custom_args.lora_dropout,\n",
    ")\n",
    "\n",
    "# model organization is LLM2VecModel.model -> HF Model, we have to apply PEFT to the inner model\n",
    "model.model = peft_model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after-peft trainable: ['model.layers.0.self_attn.q_proj.lora_A.default.weight', 'model.layers.0.self_attn.q_proj.lora_B.default.weight', 'model.layers.0.self_attn.k_proj.lora_A.default.weight', 'model.layers.0.self_attn.k_proj.lora_B.default.weight', 'model.layers.0.self_attn.v_proj.lora_A.default.weight', 'model.layers.0.self_attn.v_proj.lora_B.default.weight', 'model.layers.0.self_attn.o_proj.lora_A.default.weight', 'model.layers.0.self_attn.o_proj.lora_B.default.weight', 'model.layers.0.mlp.gate_proj.lora_A.default.weight', 'model.layers.0.mlp.gate_proj.lora_B.default.weight']\n"
     ]
    }
   ],
   "source": [
    "print(\"after-peft trainable:\",\n",
    "      [n for n,p in model.named_parameters() if p.requires_grad][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requires_grad: True <StackBackward0 object at 0x7f481be80c70>\n"
     ]
    }
   ],
   "source": [
    "tok = model.tokenize([\"just a test\"])              # CPU tensors\n",
    "dev = next(model.parameters()).device              # cuda:0 or cpu\n",
    "tok = {k: v.to(dev) for k, v in tok.items()}       # move batch\n",
    "\n",
    "with torch.set_grad_enabled(True):\n",
    "    reps = model(tok)                              # runs __call__ → forward\n",
    "print(\"requires_grad:\", reps.requires_grad, reps.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading train examples...: 100%|██████████| 91936/91936 [00:00<00:00, 139876.47it/s]\n",
      "Loading valid examples...: 100%|██████████| 4800/4800 [00:00<00:00, 442106.79it/s]\n"
     ]
    }
   ],
   "source": [
    "#|export\n",
    "\n",
    "tokenizer = model.tokenizer\n",
    "data_collator = MixedNegCollator(model)           # the new collator\n",
    "\n",
    "# Load train examples into memory\n",
    "train_examples = [\n",
    "    train_dataset_e5[i]\n",
    "    for i in tqdm(\n",
    "        range(len(train_dataset_e5)),\n",
    "        desc=\"Loading train examples...\",\n",
    "        disable=not accelerator.is_main_process,\n",
    "    )\n",
    "]\n",
    "\n",
    "valid_examples = [\n",
    "    valid_dataset_e5[i]\n",
    "    for i in tqdm(\n",
    "        range(len(valid_dataset_e5)),\n",
    "        desc=\"Loading valid examples...\",\n",
    "        disable=not accelerator.is_main_process,\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_830075/3893196380.py:15: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SupervisedTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#|export\n",
    "train_loss = load_loss(custom_args.loss_class, scale=custom_args.loss_scale)\n",
    "\n",
    "trainer = SupervisedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_examples,\n",
    "    eval_dataset=valid_examples,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=model.tokenizer,\n",
    "    loss_function=train_loss,\n",
    ")\n",
    "\n",
    "if custom_args.stop_after_n_steps is not None:\n",
    "    trainer.add_callback(StopTrainingCallback(custom_args.stop_after_n_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"WANDB_DISABLED\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 90,656\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 8,499\n",
      "  Number of trainable parameters = 14,991,360\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "Could not log the number of model parameters in Weights & Biases due to an AttributeError.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='183' max='8499' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 183/8499 32:38 < 25:00:05, 0.09 it/s, Epoch 0.06/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.972700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5.754900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.994500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.018800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.093000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.973400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>5.320300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.782800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.568400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.543000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.827500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.550800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.578900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.450400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.409400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.448600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.417000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.446100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.390000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.398800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.467400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.255100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.434000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>1.995800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.353100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.155200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.288800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.213400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.244100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>1.253600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.203900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>0.296800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.297900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>0.279800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.192700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.265400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.779500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Saving model checkpoint to output/mntp-supervised/Meta-Llama-3-sheared/checkpoint-50\n",
      "Configuration saved in output/mntp-supervised/Meta-Llama-3-sheared/checkpoint-50/config.json\n",
      "Model weights saved in output/mntp-supervised/Meta-Llama-3-sheared/checkpoint-50/model.safetensors\n",
      "tokenizer config file saved in output/mntp-supervised/Meta-Llama-3-sheared/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in output/mntp-supervised/Meta-Llama-3-sheared/checkpoint-50/special_tokens_map.json\n",
      "INFO:__main__:Saving model checkpoint to output/mntp-supervised/Meta-Llama-3-sheared/checkpoint-100\n",
      "Configuration saved in output/mntp-supervised/Meta-Llama-3-sheared/checkpoint-100/config.json\n",
      "Model weights saved in output/mntp-supervised/Meta-Llama-3-sheared/checkpoint-100/model.safetensors\n",
      "tokenizer config file saved in output/mntp-supervised/Meta-Llama-3-sheared/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in output/mntp-supervised/Meta-Llama-3-sheared/checkpoint-100/special_tokens_map.json\n",
      "INFO:__main__:Saving model checkpoint to output/mntp-supervised/Meta-Llama-3-sheared/checkpoint-150\n",
      "Configuration saved in output/mntp-supervised/Meta-Llama-3-sheared/checkpoint-150/config.json\n",
      "Model weights saved in output/mntp-supervised/Meta-Llama-3-sheared/checkpoint-150/model.safetensors\n",
      "tokenizer config file saved in output/mntp-supervised/Meta-Llama-3-sheared/checkpoint-150/tokenizer_config.json\n",
      "Special tokens file saved in output/mntp-supervised/Meta-Llama-3-sheared/checkpoint-150/special_tokens_map.json\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/trainer.py:2164\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2162\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2165\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2169\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/trainer.py:2524\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2517\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2518\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2519\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2520\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2521\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2522\u001b[0m )\n\u001b[1;32m   2523\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2524\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2526\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2527\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2528\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2529\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2530\u001b[0m ):\n\u001b[1;32m   2531\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2532\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/trainer.py:3654\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3651\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3653\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3654\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3656\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3657\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3658\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3660\u001b[0m ):\n",
      "Cell \u001b[0;32mIn[28], line 32\u001b[0m, in \u001b[0;36mSupervisedTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m d_reps_neg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(features) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m features[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 32\u001b[0m     d_reps_neg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_function(q_reps, d_reps, d_reps_neg)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_outputs:\n",
      "File \u001b[0;32m~/llm2vec-da/llm2vec_da/model_modifications/llm2vec_class.py:237\u001b[0m, in \u001b[0;36mLLM2Vec.forward\u001b[0;34m(self, sentence_feature)\u001b[0m\n\u001b[1;32m    234\u001b[0m reps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msentence_feature)\n\u001b[1;32m    235\u001b[0m sentence_feature[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membed_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m embed_mask\n\u001b[0;32m--> 237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_pooling\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence_feature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreps\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlast_hidden_state\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/llm2vec-da/llm2vec_da/model_modifications/llm2vec_class.py:248\u001b[0m, in \u001b[0;36mLLM2Vec.get_pooling\u001b[0;34m(self, features, last_hidden_states)\u001b[0m\n\u001b[1;32m    245\u001b[0m seq_lengths \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooling_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack(\n\u001b[0;32m--> 248\u001b[0m         [\n\u001b[1;32m    249\u001b[0m             last_hidden_states[i, \u001b[38;5;241m-\u001b[39mlength:, :]\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    250\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m i, length \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(seq_lengths)\n\u001b[1;32m    251\u001b[0m         ],\n\u001b[1;32m    252\u001b[0m         dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    253\u001b[0m     )\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooling_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweighted_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    255\u001b[0m     bs, l, _ \u001b[38;5;241m=\u001b[39m last_hidden_states\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/llm2vec-da/llm2vec_da/model_modifications/llm2vec_class.py:249\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    245\u001b[0m seq_lengths \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooling_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack(\n\u001b[1;32m    248\u001b[0m         [\n\u001b[0;32m--> 249\u001b[0m             \u001b[43mlast_hidden_states\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mlength\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m i, length \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(seq_lengths)\n\u001b[1;32m    251\u001b[0m         ],\n\u001b[1;32m    252\u001b[0m         dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    253\u001b[0m     )\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooling_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweighted_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    255\u001b[0m     bs, l, _ \u001b[38;5;241m=\u001b[39m last_hidden_states\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#|export\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "def main():\n",
    "    trainer.train()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.export import nb_export\n",
    "nb_export('4_supervised_training_to_py.ipynb', '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dtudeep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
