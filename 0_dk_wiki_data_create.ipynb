{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1b1da3dde4b4ca999a23106a67c2025",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"alexandrainst/wiki40b-da\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_paragraphs(text):\n",
    "    \"\"\"Clean and process text by removing formatting and keeping text after _START_PARAGRAPH_.\"\"\"\n",
    "    # Split at _START_PARAGRAPH_\n",
    "    paragraphs = text.split('_START_PARAGRAPH_')[1:]  # Skip the first part before _START_PARAGRAPH_\n",
    "\n",
    "    # Join all paragraphs and replace unwanted formatting markers\n",
    "    processed_text = ' '.join(para.replace('\\n', ' ').replace('_NEWLINE_', ' ').replace('_START_SECTION_', '').strip() for para in paragraphs)\n",
    "    \n",
    "    return processed_text\n",
    "\n",
    "# Apply the transformation to all splits of the dataset\n",
    "def clean_dataset(ds):\n",
    "    \"\"\"Function to clean the 'text' column for a Hugging Face DatasetDict.\"\"\"\n",
    "    # Define the transformation to apply to each instance\n",
    "    def apply_clean_fn(batch):\n",
    "        batch['text'] = clean_paragraphs(batch['text'])\n",
    "        return batch\n",
    "    \n",
    "    # Use `map` function to apply `apply_clean_fn` to the entire dataset (train, test, validation)\n",
    "    ds = ds.map(apply_clean_fn, batched=False)\n",
    "\n",
    "    return ds\n",
    "\n",
    "# Calling the clean_dataset function to clean the text in all splits (train/validation/test)\n",
    "ds_clean = clean_dataset(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_clean['validation']['text'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the wikipedia_id column and version_id column from all splits\n",
    "def drop_columns(ds):\n",
    "    \"\"\"Function to drop columns from a Hugging Face DatasetDict.\"\"\"\n",
    "    # Define the transformation to apply to each instance\n",
    "    def apply_drop_fn(batch):\n",
    "        batch.pop('wikidata_id', None)\n",
    "        batch.pop('version_id', None)\n",
    "        return batch\n",
    "    \n",
    "    # Use `map` function to apply `apply_drop_fn` to the entire dataset (train, test, validation)\n",
    "    ds = ds.map(apply_drop_fn, batched=False)\n",
    "\n",
    "    return ds\n",
    "\n",
    "# Calling the drop_columns function to drop the columns from all splits (train/validation/test)\n",
    "ds_clean = drop_columns(ds_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you ran all cleaning already, and your script is ready\n",
    "ds_clean.save_to_disk(\"data/wiki40b-da\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push the directory with the multiple configurations to the Hub\n",
    "ds_clean.push_to_hub(repo_id=\"jealk/wiki40b-da-clean\", config_name=\"default\", set_default=True, commit_message=\"Prev commit was original data: Now: Removed text formatting and article titles, dropped ID columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SIMCSE, Split to sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datasets import DatasetDict\n",
    "\n",
    "def split_into_sentences(text_batch):\n",
    "    \"\"\"\n",
    "    Splits each text entry in the input batch of texts into sentences using punctuation followed by capital letters\n",
    "    (., !, and ?) to recognize sentence boundaries. \n",
    "    Ensures that each split sentence becomes a new row in the dataset.\n",
    "    \"\"\"\n",
    "    # Regex to split sentences on .,!,? only when followed by whitespace and a capital letter (or special Danish capital letters)\n",
    "    split_regex = r'(?<=[.!?])(?=\\s+[A-ZÆØÅ])'\n",
    "\n",
    "    # List to store all sentences across the batch\n",
    "    all_sentences = []\n",
    "\n",
    "    # Process each text in the batch\n",
    "    for text in text_batch:\n",
    "        # Perform the smart splitting using the regex\n",
    "        sentences = re.split(split_regex, text.strip())\n",
    "\n",
    "        # Filter out empty results and remove extra whitespace from sentences, then extend the result list\n",
    "        all_sentences.extend([sentence.strip() for sentence in sentences if sentence.strip()])\n",
    "\n",
    "    # Return the flattened list of all sentences for this batch\n",
    "    return all_sentences\n",
    "\n",
    "def create_sentences_dataset(ds):\n",
    "    \"\"\"\n",
    "    Given a Hugging Face dataset, it splits paragraphs into sentences and returns a new dataset\n",
    "    where each sentence is a new row.\n",
    "    \"\"\"\n",
    "    # Apply the map function to split into sentences\n",
    "    ds_sentences = ds.map(\n",
    "        lambda batch: {\"text\": split_into_sentences(batch[\"text\"])},\n",
    "        batched=True,  # Process on batches\n",
    "        batch_size=1000,  # Adjust based on memory/capability, 1000 is a good value for efficiency\n",
    "        num_proc=4,  # Use multiple processes for performance (adjust based on machine)\n",
    "    )\n",
    "\n",
    "    # Flatten the dataset’s structure (make sure every sentence is an individual row)\n",
    "    ds_sentences = ds_sentences.flatten_indices()\n",
    "\n",
    "    return ds_sentences\n",
    "\n",
    "# Assuming 'ds' is the original dataset containing paragraphs\n",
    "ds_sentences = DatasetDict({\n",
    "    \"train\": create_sentences_dataset(ds_clean[\"train\"]),\n",
    "    \"validation\": create_sentences_dataset(ds_clean[\"validation\"]),\n",
    "    \"test\": create_sentences_dataset(ds_clean[\"test\"])\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_before_filtering = sum(len(ds_sentences[split]) for split in ds_sentences.keys())\n",
    "\n",
    "def filter_short_and_long_sentences(example):\n",
    "    \"\"\"\n",
    "    Filters out sentences that are shorter than 5 words or longer than 100 words.\n",
    "    \"\"\"\n",
    "    word_count = len(example['text'].split())  # Calculate word count\n",
    "    return 5 <= word_count <= 100  # Only keep sentences with 5 <= word_count <= 100\n",
    "\n",
    "ds_sentences_filtered = ds_sentences.filter(filter_short_and_long_sentences)\n",
    "\n",
    "sentences_after_filtering = sum(len(ds_sentences_filtered[split]) for split in ds_sentences_filtered.keys())\n",
    "\n",
    "print(f\"{sentences_before_filtering - sentences_after_filtering} sentences was removed, of the total {sentences_before_filtering} sentences.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the new sentence-level dataset to disk\n",
    "ds_sentences_filtered.save_to_disk(\"data/wiki40b-da-sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push the directory with the multiple configurations to the Hub\n",
    "ds_sentences_filtered.push_to_hub(repo_id=\"jealk/wiki40b-da-clean\", config_name=\"sentences\", set_default=False, commit_message=\"Filtered sentences to be between 5 and 100 words long\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gemma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
