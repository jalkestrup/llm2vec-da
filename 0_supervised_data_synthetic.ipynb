{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean and convert synthetic data\n",
    "Notebook to clean and convert https://huggingface.co/collections/ThatsGroes/nordic-embedding-training-data-678f53542163a7eaf5d2194e to a unified and clean dataset for embedding finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import HfApi\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "api = HfApi(token=os.getenv(\"HF_TOKEN\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8494c7a605444ef8b2dfd0f6a29a451f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/92783 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dcea826c37641508406c23fa1938ca2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/92783 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6749809df8294c2b9a6e1ac56ff2c3c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/97745 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "637af693f62241729fb13eac8fcdfd26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/97745 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71844b4e471e42aa8d2549bc4823de7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/98865 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26444442d98b45079e8380af6c719721",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/98865 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb7094112f3847b5acb87035c8ac5b55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/94734 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e49fd44d985466b9e01617cb5ef8619",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/94734 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cb1e545302f4094a7968ac76d0a746d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/99382 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4285c47c0b2249fbad96897b96dc12dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/99382 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e78768c2a094cf2b7c657f1990fee2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/48937 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1646582f597e47d596fbd1318859f1a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/48937 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9613d8c2fc474b7885e5fe2b89cd3d2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/49362 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ff987a2f16c44c496f5450df89fb3ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/49362 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e566b12650c84d9a854f60f79813134e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/46877 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d52a6d5ac0e94f009e7ff39509ac1f94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/46877 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "263dc75892ae45cdac4c64cf6b361e14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/48895 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf630be4de5d44d484f52314dabee58b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/48895 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07fb5e5b13934f22a99ce118b2e29946",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/49497 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b132aae6bd1e4fcd9513a86f732ae367",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/49497 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9f4c140718842e0bfdd0469a90db043",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/46668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d941c1421a974bf490e38fc68e526211",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/46668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a505b0debe141c5829cdef669b0d471",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/49695 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8954453baaa24f41be6438063d027fbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/49695 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8531fe4e8f304df58d780cb2cc62bf4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/47458 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c8aa2d01b7c4558abdd4e167b184bf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/47458 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f5318c654a74c93a77543ad1b53c381",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/47660 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f1c1e9f47014b4298ce32e59d2a0e9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/47660 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e1d82258b1f480bb5a7209eda268558",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/49691 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e1f9103bf8548c7883058d2d12f9948",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/49691 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "import json_repair\n",
    "\n",
    "def extract_similarity_scores(text: str):\n",
    "    \"\"\"\n",
    "    Look for lines like:\n",
    "      - \"The similarity score between S1 and S2 should be 4\"\n",
    "      - \"The similarity score between S1 and S3 should be 3.5\"\n",
    "    Returns a list of [score_S1S2, score_S1S3] as floats, or None if not found.\n",
    "    \"\"\"\n",
    "    # Regex to find \"S1 and S2 should be XXX\"\n",
    "    pattern_s2 = re.compile(r\"s1\\s+and\\s+s2\\s+should\\s+be\\s+(\\d+(\\.\\d+)?)\", re.IGNORECASE)\n",
    "    # Regex to find \"S1 and S3 should be XXX\"\n",
    "    pattern_s3 = re.compile(r\"s1\\s+and\\s+s3\\s+should\\s+be\\s+(\\d+(\\.\\d+)?)\", re.IGNORECASE)\n",
    "    \n",
    "    match_s2 = pattern_s2.search(text)\n",
    "    match_s3 = pattern_s3.search(text)\n",
    "    \n",
    "    if match_s2 and match_s3:\n",
    "        score_s2 = float(match_s2.group(1))\n",
    "        score_s3 = float(match_s3.group(1))\n",
    "        return [score_s2, score_s3]\n",
    "    \n",
    "    return None\n",
    "\n",
    "def clean_extracted_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Cleans the input text by:\n",
    "      - Removing literal \"\\\\n\" substrings and standalone backslashes.\n",
    "      - Replacing newline characters with a space.\n",
    "      - Removing all leading non-alphabetic characters.\n",
    "      - Collapsing multiple spaces into one.\n",
    "      - Removing any trailing characters after the last alphabetic character,\n",
    "        unless they are one of: period (.), question mark (?), exclamation mark (!), or closing parenthesis ())\n",
    "      - Handling empty or invalid input by returning an empty string.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "        \n",
    "    # Remove literal \"\\n\" substrings and any remaining standalone backslashes.\n",
    "    text = text.replace(\"\\\\n\", \"\")\n",
    "    text = text.replace(\"\\\\\", \"\")\n",
    "    # Replace newline characters with a space.\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    # Remove all leading characters until the first alphabetic character.\n",
    "    text = re.sub(r'^[^A-Za-z]+', '', text)\n",
    "    # If the result is empty after removing leading non-alphabetic characters, return empty string.\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    # Collapse multiple spaces into a single space.\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    \n",
    "    # Remove trailing characters beyond the last alphabetic character,\n",
    "    # while preserving a trailing allowed punctuation if present.\n",
    "    m = re.match(r'^(.*[A-Za-z])([.?!)]*)[^A-Za-z]*$', text)\n",
    "    if m:\n",
    "        text = m.group(1) + m.group(2)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def extract_assigned_instruction(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts the substring following:\n",
    "      - \"You have been assigned a (text classification|retrieval|text matching) task\"\n",
    "    and ending right before \"Your mission...\".\n",
    "    Cleans the extracted text.\n",
    "    \"\"\"\n",
    "    pattern = re.compile(\n",
    "        r\"You have been assigned a (?:text classification|retrieval|text matching) task\\s*[:]*\\s*(.*?)\\s*(?=Your mission)\",\n",
    "        flags=re.IGNORECASE | re.DOTALL\n",
    "    )\n",
    "    \n",
    "    match = pattern.search(text)\n",
    "    if not match:\n",
    "        return None\n",
    "    \n",
    "    extracted = match.group(1)\n",
    "    extracted = clean_extracted_text(extracted)\n",
    "    return extracted\n",
    "\n",
    "def has_cjk_characters(text):\n",
    "    # Unicode ranges for CJK characters:\n",
    "    # Chinese, Hiragana, Katakana, CJK Unified Ideographs Extension A & B, and CJK Compatibility Ideographs.\n",
    "    cjk_pattern = r'[\\u4e00-\\u9fff\\u3040-\\u309f\\u30a0-\\u30ff\\u3400-\\u4dbf\\U00020000-\\U0002A6DF\\uf900-\\ufaff]'\n",
    "    return bool(re.search(cjk_pattern, text))\n",
    "\n",
    "def is_valid_json(example):\n",
    "    # If \"response\" isn't even a string, fail immediately\n",
    "    if not isinstance(example[\"response\"], str):\n",
    "        return False\n",
    "    \n",
    "    #cleaned = example[\"response\"].strip().removeprefix(\"```json\").removesuffix(\"```\").strip()\n",
    "    json_sample = json_repair.repair_json(example[\"response\"], return_objects=True)\n",
    "    if json_sample is None or json_sample == {}:\n",
    "        print(f\"Error parsing example: {example['response']}\")\n",
    "        return False\n",
    "\n",
    "    # Check for CJK\n",
    "    if json_sample is None or json_sample == {} or json_sample == \"\":\n",
    "        return False\n",
    "    try:\n",
    "        values = list(json_sample.values())\n",
    "    except:\n",
    "        print(f\"Error list conversion on json_sample: {json_sample}\")\n",
    "        return False\n",
    "\n",
    "    # Ensure length >= 2\n",
    "    if len(values) < 2:\n",
    "        return False\n",
    "\n",
    "    for value in values:\n",
    "        if not isinstance(value, str):\n",
    "            return False\n",
    "        if has_cjk_characters(value):\n",
    "            #print(f\"CJK characters found in {value}\")\n",
    "            return False\n",
    "\n",
    "    # If it passes all checks, itâ€™s valid\n",
    "    return True\n",
    "\n",
    "def transform_function(json_sample, dataset_name, prompt):\n",
    "    # we assume is_valid_json has already guaranteed it's valid\n",
    "    values = list(json_sample.values())\n",
    "    #If dataset name contains text-mat do not include neg\n",
    "\n",
    "    new_data = {}\n",
    "\n",
    "    task_match = re.search(r'synthetic-from-(.+?)-tasks-(?:danish|swedish|norwegian)', dataset_name)\n",
    "    new_data[\"task\"] = task_match.group(1) if task_match else None\n",
    "\n",
    "    #Fix typos in the task name\n",
    "    if new_data[\"task\"] == \"text-mathing-short\":\n",
    "        new_data[\"task\"] = \"text-matching-short\"\n",
    "\n",
    "    #Extract the instruction, for unit-triple tasks, extract the similarity scores, for all other tasks, extract the assigned instruction\n",
    "    if new_data[\"task\"] == \"unit-triple\":\n",
    "        new_data[\"instruction\"] = \"Retrieve semantically similar text\"\n",
    "        new_data[\"triple_label\"] = extract_similarity_scores(prompt)\n",
    "    else:\n",
    "        new_data[\"instruction\"] = extract_assigned_instruction(prompt)\n",
    "\n",
    "    new_data[\"query\"] = values[0]\n",
    "    new_data[\"positive\"] = values[1]\n",
    "    new_data[\"language\"] = dataset_name.split(\"-\")[-1]\n",
    "    if new_data['task'] == 'retrieval' or new_data['task'] == 'unit-triple':\n",
    "        if len(values) > 2 and values[2] is not None:\n",
    "            new_data[\"negative\"] = values[2]\n",
    "        else:\n",
    "            new_data[\"negative\"] = \"\"\n",
    "            \n",
    "    return new_data\n",
    "\n",
    "\n",
    "# List of Hugging Face repository URLs.\n",
    "hf_repos = [\n",
    "    \"ThatsGroes/synthetic-from-retrieval-tasks-danish\",\n",
    "    \"ThatsGroes/synthetic-from-classification-tasks-danish\",\n",
    "    \"ThatsGroes/synthetic-from-unit-triple-tasks-danish\",\n",
    "    \"ThatsGroes/synthetic-from-text-matching-long-tasks-danish\",\n",
    "    \"ThatsGroes/synthetic-from-text-matching-short-tasks-danish\",\n",
    "    \"ThatsGroes/synthetic-from-classification-tasks-swedish\",\n",
    "    \"ThatsGroes/synthetic-from-unit-triple-tasks-swedish\",\n",
    "    \"ThatsGroes/synthetic-from-retrieval-tasks-norwegian\",  \n",
    "    \"ThatsGroes/synthetic-from-classification-tasks-norwegian\",\n",
    "    \"ThatsGroes/synthetic-from-unit-triple-tasks-norwegian\",\n",
    "    \"ThatsGroes/synthetic-from-retrieval-tasks-swedish\",\n",
    "    \"ThatsGroes/synthetic-from-text-mathing-short-tasks-swedish\",\n",
    "    \"ThatsGroes/synthetic-from-text-matching-long-tasks-swedish\",\n",
    "    \"ThatsGroes/synthetic-from-text-matching-long-tasks-norwegian\",\n",
    "    \"ThatsGroes/synthetic-from-text-mathing-short-tasks-norwegian\"\n",
    "]\n",
    "\n",
    "hf_retrieval_tasks = [\n",
    "    \"ThatsGroes/synthetic-from-retrieval-tasks-danish\",\n",
    "    \"ThatsGroes/synthetic-from-retrieval-tasks-swedish\",\n",
    "    \"ThatsGroes/synthetic-from-retrieval-tasks-norwegian\"\n",
    "]\n",
    "\n",
    "hf_classification_tasks = [\n",
    "    \"ThatsGroes/synthetic-from-classification-tasks-danish\",\n",
    "    \"ThatsGroes/synthetic-from-classification-tasks-swedish\",\n",
    "    \"ThatsGroes/synthetic-from-classification-tasks-norwegian\"\n",
    "]\n",
    "\n",
    "\n",
    "hf_unit_triple_tasks = [\n",
    "    \"ThatsGroes/synthetic-from-unit-triple-tasks-danish\",\n",
    "    \"ThatsGroes/synthetic-from-unit-triple-tasks-swedish\",\n",
    "    \"ThatsGroes/synthetic-from-unit-triple-tasks-norwegian\"\n",
    "]\n",
    "\n",
    "hf_text_matching_tasks = [\n",
    "    \"ThatsGroes/synthetic-from-text-mathing-short-tasks-swedish\",\n",
    "    \"ThatsGroes/synthetic-from-text-mathing-short-tasks-danish\",\n",
    "    \"ThatsGroes/synthetic-from-text-matching-long-tasks-swedish\",\n",
    "    \"ThatsGroes/synthetic-from-text-matching-long-tasks-norwegian\"\n",
    "]\n",
    "\n",
    "\n",
    "transformed_datasets = []\n",
    "\n",
    "# Loop over each repository URL.\n",
    "for repo in hf_repos:\n",
    "    # Extract the dataset name from the URL (e.g., \"synthetic-from-retrieval-tasks-danish\")\n",
    "    dataset_name = repo.split(\"/\")[-1]\n",
    "    \n",
    "    # Load the dataset (adjust the split name if necessary)\n",
    "    ds = load_dataset(repo, split=\"train\")\n",
    "    \n",
    "    #Check that the response and prompt columns are strings\n",
    "    ds = ds.filter(lambda x: isinstance(x[\"response\"], str) and isinstance(x[\"prompt\"], list))\n",
    "    #Apply the filter is_valid_json to the response column\n",
    "    ds = ds.filter(is_valid_json)\n",
    "\n",
    "    def map_fn(example):\n",
    "        json_sample = json_repair.repair_json(example[\"response\"], return_objects=True)\n",
    "        return transform_function(json_sample, dataset_name, str(example[\"prompt\"]))\n",
    "    \n",
    "    #Drop the model column\n",
    "    ds = ds.remove_columns([\"model\"])\n",
    "\n",
    "    ds_transformed = ds.map(map_fn)\n",
    "\n",
    "    #Can you reorder the columns to be query, pos, neg, language, task, instruction, prompt, response.\n",
    "    #Currently response and prompt are the first two columns\n",
    "    if \"negative\" in ds_transformed.column_names:\n",
    "        column_order = [\n",
    "            \"query\", \"positive\", \"negative\", \"language\", \"task\", \n",
    "            \"instruction\", 'triple_label,' \"prompt\", \"response\"\n",
    "        ]\n",
    "    else:\n",
    "        column_order = [\n",
    "            \"query\", \"positive\", \"language\", \"task\", \n",
    "            \"instruction\", \"prompt\", \"response\"\n",
    "        ]\n",
    "\n",
    "    ds_transformed = ds_transformed.select_columns(column_order)\n",
    "\n",
    "    transformed_datasets.append(ds_transformed)\n",
    "    # Save the transformed dataset to disk\n",
    "    ds_transformed.save_to_disk(f\"synthetic-supervised-dataset-{dataset_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['query', 'positive', 'negative', 'language', 'task', 'instruction', 'prompt', 'response'],\n",
       "    num_rows: 968249\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optionally, concatenate all transformed datasets into one final dataset.\n",
    "final_dataset = concatenate_datasets(transformed_datasets)\n",
    "final_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "079b2ab3ea4c4e3e9abe9186bc0f51a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/5 shards):   0%|          | 0/920589 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_dataset.save_to_disk(\"synthetic-supervised-dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b76c5e952cd74a638e4e751897bd17ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/6 shards):   0%|          | 0/968249 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# save to disk\n",
    "final_dataset.save_to_disk(\"synthetic-supervised-dataset-2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ec9b4e4a6ab48ffb29668f75f103216",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7396a5eb9f5400aaee88b5c7271370f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/162 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92ceeab334f1446db1d6bbefa4d6ffae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/162 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1fa6c3a16e547cab9184a1646c36390",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/162 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef35a7c618ec4f74aa6ae083d4f6846f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/162 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89622ecc639b4c219eab913f42432693",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/162 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01549b1e28c14b73ba68df3f92683454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/162 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7ca00851a3949c0909fd2a22a4f95c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/610 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/DDSC/nordic-embedding-training-data/commit/5fda060ad9b0e446ba3240e515084ac8ef65fef0', commit_message='Upload dataset', commit_description='', oid='5fda060ad9b0e446ba3240e515084ac8ef65fef0', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/DDSC/nordic-embedding-training-data', endpoint='https://huggingface.co', repo_type='dataset', repo_id='DDSC/nordic-embedding-training-data'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "import os\n",
    "\n",
    "api = HfApi(token=os.getenv(\"HF_TOKEN\"))\n",
    "final_dataset.push_to_hub(\"DDSC/nordic-embedding-training-data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
