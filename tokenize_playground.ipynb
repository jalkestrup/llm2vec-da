{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize for MNTP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lenovo/miniforge3/envs/l2v/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset and specify the configuration with name=\"sentences\"\n",
    "dataset = load_dataset(\"jealk/wiki40b-da-clean\", name=\"default\")\n",
    "\n",
    "train_texts = dataset['train']['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model from config is: AI-Sweden-Models/Llama-3-8B-instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lenovo/miniforge3/envs/l2v/lib/python3.12/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "\n",
    "with open('llama-swe-it-mntp.json') as f:\n",
    "    config = json.load(f)\n",
    "print(f'Model from config is: {config[\"model_name_or_path\"]}')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config['model_name_or_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentence: Tekstiler havde mange forskellige formål i oldtidens Ægypten, og blev brugt af alle mennesker på tværs af de sociale lag\n",
      "is tokenized as: ['Tek', 'st', 'iler', 'Ġhav', 'de', 'Ġmange', 'Ġforsk', 'ellig', 'e', 'Ġform', 'Ã¥l', 'Ġi', 'Ġold', 'tid', 'ens', 'ĠÃ', 'Ĩ', 'gypt', 'en', ',', 'Ġog', 'Ġblev', 'Ġbr', 'ug', 't', 'Ġaf', 'Ġalle', 'Ġmennes', 'ker', 'ĠpÃ¥', 'Ġtv', 'Ã¦', 'rs', 'Ġaf', 'Ġde', 'Ġsociale', 'Ġlag']\n",
      "Has token ids: {'input_ids': [128000, 56815, 267, 5888, 31081, 451, 60534, 84452, 6842, 68, 1376, 87604, 602, 2362, 25453, 729, 1717, 228, 13849, 268, 11, 7500, 73099, 1437, 773, 83, 8136, 12584, 96218, 7197, 9292, 11333, 9371, 5544, 8136, 409, 75107, 22171], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "And decoded as: <|begin_of_text|>Tekstiler havde mange forskellige formål i oldtidens Ægypten, og blev brugt af alle mennesker på tværs af de sociale lag\n"
     ]
    }
   ],
   "source": [
    "first_sentence = train_texts[0].split(\".\")[0]\n",
    "tokken_ids = tokenizer(first_sentence)\n",
    "decoded_sentence = tokenizer.decode(tokken_ids['input_ids'])\n",
    "print(f'The sentence: {first_sentence}\\nis tokenized as: {tokenizer.tokenize(first_sentence)}\\nHas token ids: {tokken_ids}\\nAnd decoded as: {decoded_sentence}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to local file (expected by LLM2Vec SimCSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset and specify the configuration with name=\"sentences\"\n",
    "dataset = load_dataset(\"jealk/wiki40b-da-clean\", name=\"sentences\")\n",
    "train_texts = dataset['train']['text']\n",
    "\n",
    "output_file = \"wiki1m_for_simcse_dk.txt\"\n",
    "\n",
    "# Write the dataset contents into the txt file, one sentence per line\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    for text in train_texts:\n",
    "        f.write(text + '\\n')  # Write each text line and ensure Danish characters are preserved"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "l2v",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
