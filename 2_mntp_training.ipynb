{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import transformers\n",
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/teamspace/studios/this_studio/llm2vec-da\n"
     ]
    }
   ],
   "source": [
    "# Handle lighting AI studio\n",
    "if '/teamspace' in os.getcwd():\n",
    "    os.chdir('/teamspace/studios/this_studio/llm2vec-da')\n",
    "    print(os.getcwd())\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import HfApi\n",
    "load_dotenv()\n",
    "\n",
    "api = HfApi(token=os.getenv(\"HF_TOKEN\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from llm2vec_da.arguments import ModelArguments, MNTPDataTrainingArguments, CustomArguments\n",
    "from transformers import HfArgumentParser, TrainingArguments\n",
    "\n",
    "parser = HfArgumentParser(\n",
    "    (ModelArguments, MNTPDataTrainingArguments, TrainingArguments, CustomArguments)\n",
    ")\n",
    "\n",
    "model_args, data_args, training_args, custom_args = parser.parse_json_file(\"configs/mntp/MetaLlama3-sheared.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_kwargs = {\n",
    "    \"cache_dir\": model_args.cache_dir,\n",
    "    \"revision\": model_args.model_revision,\n",
    "    \"token\": model_args.token,\n",
    "    \"trust_remote_code\": model_args.trust_remote_code,\n",
    "}\n",
    "if training_args.gradient_checkpointing:\n",
    "    training_args.gradient_checkpointing_kwargs = {\"use_reentrant\": False}\n",
    "# Set seed before initializing model.\n",
    "\n",
    "transformers.set_seed(training_args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model class detected by LLM2Vec clas:\n",
      "<class 'llm2vec_da.model_modifications.bidirectional_llama.LlamaBiForMNTP'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "from llm2vec_da.model import get_model_class\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_args.model_name_or_path, **config_kwargs\n",
    ")\n",
    "\n",
    "#Verifying that LLM2Vec is detecting the correct model class\n",
    "model_class = get_model_class(config)\n",
    "print(f'Model class detected by LLM2Vec clas:\\n{model_class}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ! REMEMBER TO CHANGE ATTN_IMPLEMENTATION BACK TO FLASH !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'princeton-nlp/Sheared-LLaMA-1.3B'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_args.model_name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_dtype = (\n",
    "    model_args.torch_dtype\n",
    "    if model_args.torch_dtype in [\"auto\", None]\n",
    "    else getattr(torch, model_args.torch_dtype)\n",
    ")\n",
    "model = model_class.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    device_map=\"auto\",\n",
    "    config=config,\n",
    "    cache_dir=model_args.cache_dir,\n",
    "    revision=model_args.model_revision,\n",
    "    token=model_args.token,\n",
    "    trust_remote_code=model_args.trust_remote_code,\n",
    "    torch_dtype=torch_dtype,\n",
    "    low_cpu_mem_usage=model_args.low_cpu_mem_usage,\n",
    "    attn_implementation=\"sdpa\", #OBS SET BACK TO FLASH ATTENTION WHEN RUNNING ON A100 GPU!!\n",
    ")\n",
    "#    device_map=\"auto\",\n",
    "#model_args.attn_implementation,\n",
    "#model.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting model to see the modified Attention layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaBiForMNTP(\n",
       "  (model): LlamaBiModel(\n",
       "    (embed_tokens): Embedding(32000, 2048, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x ModifiedLlamaDecoderLayer(\n",
       "        (self_attn): ModifiedLlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
       "          (down_proj): Linear(in_features=5504, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModifiedLlamaSdpaAttention(\n",
       "  (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "  (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "  (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "  (rotary_emb): LlamaRotaryEmbedding()\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0].self_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up PEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's Lora trainable parameters:\n",
      "trainable params: 14,991,360 || all params: 1,294,878,720 || trainable%: 1.1577\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "from typing import List, Optional\n",
    "\n",
    "def initialize_peft(\n",
    "    model,\n",
    "    lora_r: int = 8,\n",
    "    lora_alpha: int = 16,\n",
    "    lora_dropout: float = 0.05,\n",
    "    lora_modules: Optional[List[str]] = None,\n",
    "):\n",
    "    if lora_modules is None and model.config.__class__.__name__ in [\n",
    "        \"LlamaConfig\",\n",
    "        \"MistralConfig\",\n",
    "    ]:\n",
    "        lora_modules = [\n",
    "            \"q_proj\",\n",
    "            \"v_proj\",\n",
    "            \"k_proj\",\n",
    "            \"o_proj\",\n",
    "            \"gate_proj\",\n",
    "            \"up_proj\",\n",
    "            \"down_proj\",\n",
    "        ]\n",
    "    elif lora_modules is None:\n",
    "        raise ValueError(\"lora_modules must be specified for this model.\")\n",
    "\n",
    "    config = LoraConfig(\n",
    "        r=lora_r,\n",
    "        lora_alpha=lora_alpha,\n",
    "        target_modules=lora_modules,\n",
    "        lora_dropout=lora_dropout,\n",
    "        bias=\"none\",\n",
    "        task_type=None,\n",
    "    )\n",
    "    # model organization is MODEL_TYPEBiForMNTP.model -> MODEL_TYPELBiModel, we have to apply PEFT to the inner model\n",
    "    peft_model = get_peft_model(model, config)\n",
    "    print(f\"Model's Lora trainable parameters:\")\n",
    "    peft_model.print_trainable_parameters()\n",
    "    return peft_model\n",
    "\n",
    "#Similar to the below, just copied out for readability\n",
    "#from llm2vec_da.model import initialize_peft\n",
    "\n",
    "peft_model = initialize_peft(\n",
    "    model.model,\n",
    "    lora_r=custom_args.lora_r,\n",
    "    lora_alpha=2 * custom_args.lora_r,\n",
    "    lora_dropout=custom_args.lora_dropout,\n",
    ")\n",
    "\n",
    "model.model = peft_model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after-peft trainable: ['model.layers.0.self_attn.q_proj.lora_A.default.weight', 'model.layers.0.self_attn.q_proj.lora_B.default.weight', 'model.layers.0.self_attn.k_proj.lora_A.default.weight', 'model.layers.0.self_attn.k_proj.lora_B.default.weight', 'model.layers.0.self_attn.v_proj.lora_A.default.weight', 'model.layers.0.self_attn.v_proj.lora_B.default.weight', 'model.layers.0.self_attn.o_proj.lora_A.default.weight', 'model.layers.0.self_attn.o_proj.lora_B.default.weight', 'model.layers.0.mlp.gate_proj.lora_A.default.weight', 'model.layers.0.mlp.gate_proj.lora_B.default.weight']\n"
     ]
    }
   ],
   "source": [
    "print(\"after-peft trainable:\",\n",
    "      [n for n,p in model.named_parameters() if p.requires_grad][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaBiModel(\n",
       "  (embed_tokens): Embedding(32000, 2048, padding_idx=0)\n",
       "  (layers): ModuleList(\n",
       "    (0-23): 24 x ModifiedLlamaDecoderLayer(\n",
       "      (self_attn): ModifiedLlamaSdpaAttention(\n",
       "        (q_proj): lora.Linear(\n",
       "          (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (lora_dropout): ModuleDict(\n",
       "            (default): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "          (lora_A): ModuleDict(\n",
       "            (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "          )\n",
       "          (lora_B): ModuleDict(\n",
       "            (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "          )\n",
       "          (lora_embedding_A): ParameterDict()\n",
       "          (lora_embedding_B): ParameterDict()\n",
       "          (lora_magnitude_vector): ModuleDict()\n",
       "        )\n",
       "        (k_proj): lora.Linear(\n",
       "          (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (lora_dropout): ModuleDict(\n",
       "            (default): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "          (lora_A): ModuleDict(\n",
       "            (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "          )\n",
       "          (lora_B): ModuleDict(\n",
       "            (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "          )\n",
       "          (lora_embedding_A): ParameterDict()\n",
       "          (lora_embedding_B): ParameterDict()\n",
       "          (lora_magnitude_vector): ModuleDict()\n",
       "        )\n",
       "        (v_proj): lora.Linear(\n",
       "          (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (lora_dropout): ModuleDict(\n",
       "            (default): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "          (lora_A): ModuleDict(\n",
       "            (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "          )\n",
       "          (lora_B): ModuleDict(\n",
       "            (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "          )\n",
       "          (lora_embedding_A): ParameterDict()\n",
       "          (lora_embedding_B): ParameterDict()\n",
       "          (lora_magnitude_vector): ModuleDict()\n",
       "        )\n",
       "        (o_proj): lora.Linear(\n",
       "          (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (lora_dropout): ModuleDict(\n",
       "            (default): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "          (lora_A): ModuleDict(\n",
       "            (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "          )\n",
       "          (lora_B): ModuleDict(\n",
       "            (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "          )\n",
       "          (lora_embedding_A): ParameterDict()\n",
       "          (lora_embedding_B): ParameterDict()\n",
       "          (lora_magnitude_vector): ModuleDict()\n",
       "        )\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): lora.Linear(\n",
       "          (base_layer): Linear(in_features=2048, out_features=5504, bias=False)\n",
       "          (lora_dropout): ModuleDict(\n",
       "            (default): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "          (lora_A): ModuleDict(\n",
       "            (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "          )\n",
       "          (lora_B): ModuleDict(\n",
       "            (default): Linear(in_features=16, out_features=5504, bias=False)\n",
       "          )\n",
       "          (lora_embedding_A): ParameterDict()\n",
       "          (lora_embedding_B): ParameterDict()\n",
       "          (lora_magnitude_vector): ModuleDict()\n",
       "        )\n",
       "        (up_proj): lora.Linear(\n",
       "          (base_layer): Linear(in_features=2048, out_features=5504, bias=False)\n",
       "          (lora_dropout): ModuleDict(\n",
       "            (default): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "          (lora_A): ModuleDict(\n",
       "            (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "          )\n",
       "          (lora_B): ModuleDict(\n",
       "            (default): Linear(in_features=16, out_features=5504, bias=False)\n",
       "          )\n",
       "          (lora_embedding_A): ParameterDict()\n",
       "          (lora_embedding_B): ParameterDict()\n",
       "          (lora_magnitude_vector): ModuleDict()\n",
       "        )\n",
       "        (down_proj): lora.Linear(\n",
       "          (base_layer): Linear(in_features=5504, out_features=2048, bias=False)\n",
       "          (lora_dropout): ModuleDict(\n",
       "            (default): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "          (lora_A): ModuleDict(\n",
       "            (default): Linear(in_features=5504, out_features=16, bias=False)\n",
       "          )\n",
       "          (lora_B): ModuleDict(\n",
       "            (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "          )\n",
       "          (lora_embedding_A): ParameterDict()\n",
       "          (lora_embedding_B): ParameterDict()\n",
       "          (lora_magnitude_vector): ModuleDict()\n",
       "        )\n",
       "        (act_fn): SiLU()\n",
       "      )\n",
       "      (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    )\n",
       "  )\n",
       "  (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "  (rotary_emb): LlamaRotaryEmbedding()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up data collation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer_kwargs = {\n",
    "    #\"cache_dir\": model_args.cache_dir,\n",
    "    \"use_fast\": model_args.use_fast_tokenizer,\n",
    "    \"revision\": model_args.model_revision,\n",
    "    \"token\": model_args.token,\n",
    "    \"trust_remote_code\": model_args.trust_remote_code,\n",
    "}\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_args.model_name_or_path, **tokenizer_kwargs\n",
    ")\n",
    "#tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting mask token to _\n"
     ]
    }
   ],
   "source": [
    "if tokenizer.mask_token is None:\n",
    "    if custom_args.mask_token_type == \"blank\":\n",
    "        print(\"Setting mask token to _\")\n",
    "        tokenizer.mask_token = \"_\"\n",
    "    elif custom_args.mask_token_type == \"eos\":\n",
    "        print(\"Setting mask token to eos\")\n",
    "        tokenizer.mask_token = tokenizer.eos_token\n",
    "    elif custom_args.mask_token_type == \"mask\":\n",
    "        print(\"Setting mask token to <mask>\")\n",
    "        tokenizer.add_tokens([\"<mask>\"])\n",
    "        tokenizer.mask_token = \"<mask>\"\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"mask_token_type {custom_args.mask_token_type} is not supported.\"\n",
    "        )\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm2vec_da.training import DataCollatorForLanguageModelingWithFullMasking\n",
    "\n",
    "data_collator = DataCollatorForLanguageModelingWithFullMasking(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm_probability=data_args.mlm_probability\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Verifying that the data collator works**\n",
    "\n",
    "\n",
    "As seen below, parts of the input is now masked with the mask token (vocab 62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator.tokenizer.vocab['_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator( (torch.randint(0, 10, (1, 10)), ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dataset\n",
    "#### **REMEMBER TO CHANGE TO CORRECT DATASET**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model specific (tokenizer) dataset\n",
    "tokenized_datasets = datasets.load_from_disk(\"data/mntp_wiki_dk_512_sheared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "if data_args.max_train_samples is not None:\n",
    "    max_train_samples = min(len(train_dataset), data_args.max_train_samples)\n",
    "    train_dataset = train_dataset.select(range(max_train_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = tokenized_datasets[\"validation\"]\n",
    "if data_args.max_eval_samples is not None:\n",
    "    max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)\n",
    "    eval_dataset = eval_dataset.select(range(max_eval_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import is_torch_tpu_available\n",
    "from llm2vec_da.training import MNTPTrainer, StopTrainingCallback\n",
    "from llm2vec_da.metrics import MetricEvaluator, preprocess_logits_for_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MetricEvaluator(model_args.cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb\n",
    "\n",
    "# Ensure W&B picks up the correct settings\n",
    "os.environ[\"WANDB_PROJECT\"] = custom_args.wandb_project\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = custom_args.wandb_log_model\n",
    "if custom_args.wandb_run_group:\n",
    "    os.environ[\"WANDB_RUN_GROUP\"] = custom_args.wandb_run_group\n",
    "if custom_args.wandb_watch:\n",
    "    os.environ[\"WANDB_WATCH\"] = custom_args.wandb_watch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/utils/import_utils.py:627: FutureWarning: `is_torch_tpu_available` is deprecated and will be removed in 4.41.0. Please use the `is_torch_xla_available` instead.\n",
      "  warnings.warn(\n",
      "/teamspace/studios/this_studio/llm2vec-da/llm2vec_da/training.py:64: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `MNTPTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trainer = MNTPTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset if training_args.do_train else None,\n",
    "    eval_dataset=eval_dataset if training_args.do_eval else None,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=evaluator if training_args.do_eval and not is_torch_tpu_available()\n",
    "                              else None,\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics\n",
    "    if training_args.do_eval and not is_torch_tpu_available()\n",
    "    else None,\n",
    "\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "\n",
    "trainer.add_callback(StopTrainingCallback(custom_args.stop_after_n_steps))\n",
    "\n",
    "#trainer.callback_handler.remove_callback(transformers.integrations.integration_utils.WandbCallback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "Bear in mind that ~50GB of GPU memory is required to run the below. Training was run on a A100 GPU with 80GB GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 140,252\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 13,149\n",
      "  Number of trainable parameters = 80,527,360\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjalkestrup\u001b[0m (\u001b[33mjealk\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/teamspace/studios/this_studio/llm2vec-da/wandb/run-20250427_165548-2fgmtevb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jealk/LLM2Vec/runs/2fgmtevb' target=\"_blank\">MetaLlama3-sheared</a></strong> to <a href='https://wandb.ai/jealk/LLM2Vec' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jealk/LLM2Vec' target=\"_blank\">https://wandb.ai/jealk/LLM2Vec</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jealk/LLM2Vec/runs/2fgmtevb' target=\"_blank\">https://wandb.ai/jealk/LLM2Vec/runs/2fgmtevb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='13149' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   20/13149 05:23 < 65:32:42, 0.06 it/s, Epoch 0/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>6.335700</td>\n",
       "      <td>6.097910</td>\n",
       "      <td>0.017115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>5.604500</td>\n",
       "      <td>5.407476</td>\n",
       "      <td>0.024594</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 32\n",
      "Configuration saved in output/mntp/Meta-Llama-3-sheared-1B-Instruct/checkpoint-10/config.json\n",
      "Model weights saved in output/mntp/Meta-Llama-3-sheared-1B-Instruct/checkpoint-10/model.safetensors\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "tokenizer config file saved in output/mntp/Meta-Llama-3-sheared-1B-Instruct/checkpoint-10/tokenizer_config.json\n",
      "Special tokens file saved in output/mntp/Meta-Llama-3-sheared-1B-Instruct/checkpoint-10/special_tokens_map.json\n",
      "tokenizer config file saved in output/mntp/Meta-Llama-3-sheared-1B-Instruct/tokenizer_config.json\n",
      "Special tokens file saved in output/mntp/Meta-Llama-3-sheared-1B-Instruct/special_tokens_map.json\n",
      "Logging checkpoint artifacts in checkpoint-10. ...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./output/mntp/Meta-Llama-3-sheared-1B-Instruct/checkpoint-10)... Done. 11.3s\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 32\n",
      "Configuration saved in output/mntp/Meta-Llama-3-sheared-1B-Instruct/checkpoint-20/config.json\n",
      "Model weights saved in output/mntp/Meta-Llama-3-sheared-1B-Instruct/checkpoint-20/model.safetensors\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "tokenizer config file saved in output/mntp/Meta-Llama-3-sheared-1B-Instruct/checkpoint-20/tokenizer_config.json\n",
      "Special tokens file saved in output/mntp/Meta-Llama-3-sheared-1B-Instruct/checkpoint-20/special_tokens_map.json\n",
      "tokenizer config file saved in output/mntp/Meta-Llama-3-sheared-1B-Instruct/tokenizer_config.json\n",
      "Special tokens file saved in output/mntp/Meta-Llama-3-sheared-1B-Instruct/special_tokens_map.json\n",
      "Logging checkpoint artifacts in checkpoint-20. ...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./output/mntp/Meta-Llama-3-sheared-1B-Instruct/checkpoint-20)... Done. 13.4s\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to /tmp/tmp8wu0n2g9\n",
      "Configuration saved in /tmp/tmp8wu0n2g9/config.json\n",
      "Configuration saved in /tmp/tmp8wu0n2g9/generation_config.json\n",
      "Model weights saved in /tmp/tmp8wu0n2g9/model.safetensors\n",
      "Saving model checkpoint to output/mntp/Meta-Llama-3-sheared-1B-Instruct\n",
      "Configuration saved in output/mntp/Meta-Llama-3-sheared-1B-Instruct/config.json\n",
      "Configuration saved in output/mntp/Meta-Llama-3-sheared-1B-Instruct/generation_config.json\n",
      "Model weights saved in output/mntp/Meta-Llama-3-sheared-1B-Instruct/model.safetensors\n",
      "Logging model artifacts. ...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁█</td></tr><tr><td>eval/loss</td><td>█▁</td></tr><tr><td>eval/runtime</td><td>▁█</td></tr><tr><td>eval/samples_per_second</td><td>█▁</td></tr><tr><td>eval/steps_per_second</td><td>█▁</td></tr><tr><td>train/epoch</td><td>▁▂▄▄▆███</td></tr><tr><td>train/global_step</td><td>▁▂▄▄▆███</td></tr><tr><td>train/grad_norm</td><td>█▃▂▁▁</td></tr><tr><td>train/learning_rate</td><td>█▇▅▃▁</td></tr><tr><td>train/loss</td><td>█▅▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.02459</td></tr><tr><td>eval/loss</td><td>5.40748</td></tr><tr><td>eval/runtime</td><td>1.7102</td></tr><tr><td>eval/samples_per_second</td><td>11.695</td></tr><tr><td>eval/steps_per_second</td><td>0.585</td></tr><tr><td>total_flos</td><td>2545835153817600.0</td></tr><tr><td>train/epoch</td><td>0.00456</td></tr><tr><td>train/global_step</td><td>20</td></tr><tr><td>train/grad_norm</td><td>3.70826</td></tr><tr><td>train/learning_rate</td><td>5e-05</td></tr><tr><td>train/loss</td><td>5.6045</td></tr><tr><td>train_loss</td><td>6.2132</td></tr><tr><td>train_runtime</td><td>253.1355</td></tr><tr><td>train_samples_per_second</td><td>1662.177</td></tr><tr><td>train_steps_per_second</td><td>51.945</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MetaLlama3-sheared</strong> at: <a href='https://wandb.ai/jealk/LLM2Vec/runs/2fgmtevb' target=\"_blank\">https://wandb.ai/jealk/LLM2Vec/runs/2fgmtevb</a><br> View project at: <a href='https://wandb.ai/jealk/LLM2Vec' target=\"_blank\">https://wandb.ai/jealk/LLM2Vec</a><br>Synced 5 W&B file(s), 0 media file(s), 20 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250427_165548-2fgmtevb/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "train_result = trainer.train() \n",
    "\n",
    "# [optional] finish the wandb run, necessary in notebooks\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(ModelArguments.model_name_or_path+\"_mntp_trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()  # Saves the tokenizer too for easy upload\n",
    "metrics = train_result.metrics\n",
    "\n",
    "max_train_samples = (\n",
    "    data_args.max_train_samples\n",
    "    if data_args.max_train_samples is not None\n",
    "    else len(train_dataset)\n",
    ")\n",
    "metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n",
    "\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "trainer.save_state()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
